{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Lumped Basin Workflow (Bow River at Banff)\n",
    "\n",
    "This notebook walks through a complete workflow for a lumped basin model using the Bow River at Banff as an example. We'll execute each step individually to understand what's happening at each stage.\n",
    "\n",
    "## Overview of This Tutorial\n",
    "\n",
    "We'll work through the simplest case in catchment modeling: a lumped basin model. This treats the entire watershed as a single unit, making it an ideal starting point for understanding the CONFLUENCE workflow.\n",
    "\n",
    "We'll run through:\n",
    "1. Project setup and configuration\n",
    "2. Domain definition (watershed delineation)\n",
    "3. Data acquisition (forcings and attributes)\n",
    "4. Model preprocessing\n",
    "5. Model execution\n",
    "6. Results visualization\n",
    "\n",
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize CONFLUENCE\n",
    "First, let's set up our directories and load the configuration. CONFLUENCE uses a centralized configuration file that controls all aspects of the modeling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load and update configuration\n",
    "config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "\n",
    "# Read config file and update paths\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Setup - Organizing the Modeling Workflow\n",
    "The first step in any CONFLUENCE workflow is to establish a well-organized project structure. This might seem trivial, but it's crucial for:\n",
    "\n",
    "- Maintaining consistency across different experiments\n",
    "- Ensuring all components can find required files\n",
    "- Enabling reproducibility\n",
    "- Facilitating collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")\n",
    "\n",
    "print(\"\\nDirectory purposes:\")\n",
    "print(\"  üìÅ shapefiles: Domain geometry (watershed, pour points, river network)\")\n",
    "print(\"  üìÅ attributes: Static characteristics (elevation, soil, land cover)\")\n",
    "print(\"  üìÅ forcing: Meteorological inputs (precipitation, temperature)\")\n",
    "print(\"  üìÅ simulations: Model outputs\")\n",
    "print(\"  üìÅ evaluation: Performance metrics and comparisons\")\n",
    "print(\"  üìÅ plots: Visualizations\")\n",
    "print(\"  üìÅ optimisation: Calibration results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition and Analysis - A data acquisition \n",
    "Before we can delineate the watershed, we need elevation data. CONFLUENCE also acquires soil and land cover data at this stage for later use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Geospatial Domain Definition and Analysis\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Analysis ===\")\n",
    "\n",
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes (DEM, soil, land cover)...\")\n",
    "confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geospatial Domain Definition and Analysis - Delineation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define domain\n",
    "print(f\"\\nDelineating watershed using method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition and Analysis - Discretisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discretize domain\n",
    "print(f\"\\nCreating HRUs using method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Delineated Domain\n",
    "Let's see what our watershed looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the watershed\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "if basin_path.exists():\n",
    "    basin_files = list(basin_path.glob('*.shp'))\n",
    "    \n",
    "    if basin_files:\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Load watershed and pour point\n",
    "        basin_gdf = gpd.read_file(basin_files[0])\n",
    "        pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "        \n",
    "        # Reproject for visualization\n",
    "        basin_web = basin_gdf.to_crs(epsg=3857)\n",
    "        pour_web = pour_point_gdf.to_crs(epsg=3857)\n",
    "        \n",
    "        # Plot watershed\n",
    "        basin_web.plot(ax=ax, facecolor='lightblue', edgecolor='navy', \n",
    "                       linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add pour point\n",
    "        pour_web.plot(ax=ax, color='red', markersize=200, marker='o', \n",
    "                      edgecolor='white', linewidth=2, zorder=5)\n",
    "                \n",
    "        # Set extent\n",
    "        minx, miny, maxx, maxy = basin_web.total_bounds\n",
    "        pad = 5000\n",
    "        ax.set_xlim(minx - pad, maxx + pad)\n",
    "        ax.set_ylim(miny - pad, maxy + pad)\n",
    "        \n",
    "        ax.set_title('Bow River Watershed at Banff \\n All water from this area flows to the pour point', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Agnostic Data Pre-Processing - Observed data\n",
    "For a lumped model, the entire watershed becomes a single Hydrologic Response Unit (HRU). This simplification assumes uniform characteristics across the watershed - obviously an approximation, but useful for many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Model Agnostic Data Pre-Processing\n",
    "print(\"=== Step 3: Model Agnostic Data Pre-Processing ===\")\n",
    "\n",
    "# Process observed data\n",
    "print(\"Processing observed streamflow data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize observed streamflow data\n",
    "obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "if obs_path.exists():\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(obs_df['datetime'], obs_df['discharge_cms'], \n",
    "            linewidth=1.5, color='blue', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    ax.set_title(f'Observed Streamflow - Bow River at Banff (WSC Station: {confluence.config[\"STATION_ID\"]})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    ax.text(0.02, 0.95, f'Mean: {obs_df[\"discharge_cms\"].mean():.1f} m¬≥/s\\\\nMax: {obs_df[\"discharge_cms\"].max():.1f} m¬≥/s', \n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Agnostic Data Pre-Processing - Forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data: {confluence.config['FORCING_DATASET']}\")\n",
    "confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Agnostic Data Pre-Processing - Remapping and zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model-Specific - Preprocessing\n",
    "Now we prepare inputs specific to our chosen hydrological model (SUMMA in this case). Each model has its own requirements for input format and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Specific Processing and Initialization\n",
    "print(\"=== Step 4: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "# Preprocess models\n",
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "confluence.managers['model'].preprocess_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model-Specific - Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} model...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nModel run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 Visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Visualize Observed vs. Simulated Streamflow\n",
    "print(\"=== Step 14: Comparing Observed vs. Simulated Streamflow ===\")\n",
    "import numpy as np \n",
    "\n",
    "# 1. Load the observed streamflow data\n",
    "obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "if not obs_path.exists():\n",
    "    print(f\"Warning: Observed streamflow data not found at {obs_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    alt_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"**/observations/streamflow/preprocessed/*_streamflow_processed.csv\"))\n",
    "    if alt_paths:\n",
    "        obs_path = alt_paths[0]\n",
    "        print(f\"Found alternative streamflow data at: {obs_path}\")\n",
    "    else:\n",
    "        print(\"No observed streamflow data found. Only simulated data will be displayed.\")\n",
    "\n",
    "# 2. Load the simulated streamflow data from SUMMA output\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_timestep.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "        f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Extract averageRoutedRunoff\n",
    "print(\"Extracting 'averageRoutedRunoff' variable...\")\n",
    "if 'averageRoutedRunoff' in ds:\n",
    "    # Extract and convert to DataFrame\n",
    "    sim_runoff = ds['averageRoutedRunoff'].to_dataframe().reset_index()\n",
    "    \n",
    "    # Get catchment area from the river basin shapefile to convert from m/s to m¬≥/s\n",
    "    basin_shapefile = config_dict.get('RIVER_BASINS_NAME', 'default')\n",
    "    if basin_shapefile == 'default':\n",
    "        basin_shapefile = f\"{config_dict['DOMAIN_NAME']}_riverBasins_{config_dict.get('DOMAIN_DEFINITION_METHOD', 'lumped')}.shp\"\n",
    "    \n",
    "    basin_path = project_dir / \"shapefiles\" / \"river_basins\" / basin_shapefile\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading catchment shapefile from: {basin_path}\")\n",
    "        basin_gdf = gpd.read_file(basin_path)\n",
    "        area_col = config_dict.get('RIVER_BASIN_SHP_AREA', 'GRU_area')\n",
    "        \n",
    "        # Area should be in m¬≤\n",
    "        if area_col in basin_gdf.columns:\n",
    "            area_m2 = basin_gdf[area_col].sum()\n",
    "            print(f\"Catchment area: {area_m2:.2f} m¬≤ ({area_m2/1e6:.2f} km¬≤)\")\n",
    "            \n",
    "            # Convert from m/s to m¬≥/s by multiplying by area in m¬≤\n",
    "            # Assuming first GRU for lumped basin simulation if multiple GRUs exist\n",
    "            if 'gru' in sim_runoff.columns:\n",
    "                sim_runoff = sim_runoff[sim_runoff['gru'] == 1][['time', 'averageRoutedRunoff']]\n",
    "            else:\n",
    "                sim_runoff = sim_runoff[['time', 'averageRoutedRunoff']]\n",
    "            \n",
    "            # Convert units: m/s -> m¬≥/s\n",
    "            sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff'] * area_m2\n",
    "            print(f\"Converted runoff from m/s to m¬≥/s (multiplied by basin area)\")\n",
    "        else:\n",
    "            print(f\"Warning: Area column '{area_col}' not found in catchment shapefile\")\n",
    "            sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff']  # Use raw values as fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting basin area: {str(e)}. Using raw values.\")\n",
    "        sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff']  # Use raw values as fallback\n",
    "    \n",
    "    # Set index to time for easier processing\n",
    "    sim_runoff.set_index('time', inplace=True)\n",
    "    sim_df = sim_runoff[['discharge_cms']]\n",
    "else:\n",
    "    print(\"Warning: 'averageRoutedRunoff' variable not found in the SUMMA output\")\n",
    "    print(\"Available variables:\", list(ds.data_vars))\n",
    "    raise ValueError(\"Required 'averageRoutedRunoff' variable not found in SUMMA output\")\n",
    "\n",
    "# Load observed data\n",
    "obs_df = None\n",
    "if obs_path.exists():\n",
    "    print(f\"Loading observed streamflow data from: {obs_path}\")\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    obs_df.set_index('datetime', inplace=True)\n",
    "    print(f\"Observed data period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "    print(f\"Observed streamflow range: {obs_df['discharge_cms'].min():.2f} to {obs_df['discharge_cms'].max():.2f} m¬≥/s\")\n",
    "\n",
    "# Show simulated data info\n",
    "print(f\"Simulated data period: {sim_df.index.min()} to {sim_df.index.max()}\")\n",
    "print(f\"Simulated streamflow range: {sim_df['discharge_cms'].min():.2f} to {sim_df['discharge_cms'].max():.2f} m¬≥/s\")\n",
    "\n",
    "# Find common date range if observed data exists\n",
    "if obs_df is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_daily = obs_df.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    sim_daily = sim_df.resample('D').mean()  # Daily mean if sub-daily sim data\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_daily.index.min(), sim_daily.index.min())\n",
    "    end_date = min(obs_daily.index.max(), sim_daily.index.max())\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "\n",
    "    # Advance the start date to skip the initial spinup\n",
    "    start_date = start_date + pd.Timedelta(days=30)\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_daily.loc[start_date:end_date]\n",
    "    sim_period = sim_daily.loc[start_date:end_date]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    # Calculate root mean square error (RMSE)\n",
    "    rmse = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).mean()**0.5\n",
    "    \n",
    "    # Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "    mean_obs = obs_period['discharge_cms'].mean()\n",
    "    numerator = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).sum()\n",
    "    denominator = ((obs_period['discharge_cms'] - mean_obs)**2).sum()\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    # Calculate Percent Bias (PBIAS)\n",
    "    pbias = 100 * (sim_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "    \n",
    "    # Calculate Kling-Gupta Efficiency (KGE)\n",
    "    r = obs_period['discharge_cms'].corr(sim_period['discharge_cms'])  # Correlation\n",
    "    alpha = sim_period['discharge_cms'].std() / obs_period['discharge_cms'].std()  # Relative variability\n",
    "    beta = sim_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()  # Bias ratio\n",
    "    kge = 1 - ((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)**0.5\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} m¬≥/s\")\n",
    "    print(f\"  - NSE: {nse:.2f}\")\n",
    "    print(f\"  - PBIAS: {pbias:.2f}%\")\n",
    "    print(f\"  - KGE: {kge:.2f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Time Series Plot - Full Period\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(obs_period.index, obs_period['discharge_cms'], 'b-', label='Observed', linewidth=1.5, alpha=0.7)\n",
    "    plt.plot(sim_period.index, sim_period['discharge_cms'], 'r-', label='Simulated', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Observed vs. Simulated Streamflow - {config_dict[\"DOMAIN_NAME\"].replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add performance metrics as text box\n",
    "    plt.text(0.02, 0.95, \n",
    "             f\"RMSE: {rmse:.2f} m¬≥/s\\nNSE: {nse:.2f}\\nPBIAS: {pbias:.2f}%\\nKGE: {kge:.2f}\",\n",
    "             transform=plt.gca().transAxes, \n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # 2. Scatter Plot with 1:1 line\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(obs_period['discharge_cms'], sim_period['discharge_cms'], alpha=0.5, color='blue')\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    max_val = max(obs_period['discharge_cms'].max(), sim_period['discharge_cms'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    plt.title('Observed vs. Simulated Comparison', fontsize=14)\n",
    "    plt.xlabel('Observed Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.ylabel('Simulated Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 3. Annual cycle plot - by month\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate monthly means\n",
    "    obs_monthly = obs_period.groupby(obs_period.index.month)['discharge_cms'].mean()\n",
    "    sim_monthly = sim_period.groupby(sim_period.index.month)['discharge_cms'].mean()\n",
    "    \n",
    "    # Get month names for x-axis\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(range(1, 13), obs_monthly.reindex(range(1, 13)), 'b-o', label='Observed', linewidth=2)\n",
    "    plt.plot(range(1, 13), sim_monthly.reindex(range(1, 13)), 'r-o', label='Simulated', linewidth=2)\n",
    "    \n",
    "    plt.title('Annual Cycle (Monthly Average)', fontsize=14)\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Average Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(1, 13), month_names)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create Flow Duration Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort values in descending order\n",
    "    obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "    sim_sorted = sim_period['discharge_cms'].sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate exceedance probabilities\n",
    "    obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "    sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted)\n",
    "    \n",
    "    # Plot Flow Duration Curves\n",
    "    plt.semilogy(obs_ranks * 100, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "    plt.semilogy(sim_ranks * 100, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "    \n",
    "    plt.title('Flow Duration Curve', fontsize=14)\n",
    "    plt.xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "    plt.ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.grid(True, which='both', alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add low, medium and high flow regions\n",
    "    plt.axvspan(0, 20, alpha=0.2, color='blue', label='High Flows')\n",
    "    plt.axvspan(20, 70, alpha=0.2, color='green', label='Medium Flows')\n",
    "    plt.axvspan(70, 100, alpha=0.2, color='red', label='Low Flows')\n",
    "    \n",
    "    # Add text labels for flow regions\n",
    "    plt.text(10, max(obs_sorted.max(), sim_sorted.max()) * 0.8, 'High Flows', fontsize=10, ha='center')\n",
    "    plt.text(45, max(obs_sorted.max(), sim_sorted.max()) * 0.1, 'Medium Flows', fontsize=10, ha='center')\n",
    "    plt.text(85, max(obs_sorted.max(), sim_sorted.max()) * 0.02, 'Low Flows', fontsize=10, ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # If no observed data, just plot simulated\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(sim_df.index, sim_df['discharge_cms'], '-', label='Simulated Streamflow', color='blue', linewidth=1.5)\n",
    "    plt.title(f\"Simulated Streamflow - {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nStreamflow visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Agnostic Data Pre-Processing - Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarking\n",
    "print(\"\\nRunning benchmarking analysis...\")\n",
    "benchmark_results = confluence.managers['analysis'].run_benchmarking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Optional Steps - Optimization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 & 6: Optional Steps (Optimization and Analysis)\n",
    "print(\"=== Step 5 & 6: Optional Steps ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - Run Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete workflow in one step\n",
    "# (Uncomment to use this instead of the step-by-step approach)\n",
    "\n",
    "# confluence.run_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Understanding the CONFLUENCE Workflow\n",
    "Congratulations! You've completed a full lumped basin modeling workflow with CONFLUENCE. \n",
    "\n",
    "## Next Steps You Could Try:\n",
    "\n",
    "### Experiment with different models (change HYDROLOGICAL_MODEL)\n",
    "- Try distributed modeling (change SPATIAL_MODE to 'Distributed')\n",
    "- Calibrate the model (use the optimization module)\n",
    "- Analyze model sensitivity to different parameters\n",
    "- Compare multiple model structures (decision analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
