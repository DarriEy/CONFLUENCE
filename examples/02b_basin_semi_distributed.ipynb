{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Distributed Basin Workflow with Delineation\n",
    "\n",
    "This notebook demonstrates the distributed modeling approach using the delineation method. We'll use the same Bow River at Banff location but create a distributed model with multiple GRUs (Grouped Response Units).\n",
    "\n",
    "## Key Differences from Lumped Model\n",
    "\n",
    "- **Domain Method**: `delineate` instead of `lumped`\n",
    "- **Stream Threshold**: 5000 (creates more sub-basins)\n",
    "- **Multiple GRUs**: Each sub-basin becomes a GRU\n",
    "- **Routing**: mizuRoute connects the GRUs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand watershed delineation with stream networks\n",
    "2. Create a distributed model with multiple GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize CONFLUENCE\n",
    "First, let's set up our directories and load the configuration. We'll modify the configuration from Tutorial 1 to create a distributed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update core paths\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Modify for distributed delineation\n",
    "config_dict['DOMAIN_NAME'] = 'Bow_at_Banff_distributed'\n",
    "config_dict['EXPERIMENT_ID'] = 'distributed_tutorial'\n",
    "config_dict['EXPERIMENT_TIME_START'] = '2011-01-01 01:00'\n",
    "config_dict['EXPERIMENT_TIME_END'] = '2022-01-01 01:00'\n",
    "config_dict['DOMAIN_DEFINITION_METHOD'] = 'delineate'  # Changed from 'lumped'\n",
    "config_dict['STREAM_THRESHOLD'] = 5000  # Higher threshold for fewer sub-basins\n",
    "config_dict['DOMAIN_DISCRETIZATION'] = 'GRUs'  # Keep as GRUs\n",
    "config_dict['SPATIAL_MODE'] = 'Distributed'  # Changed from 'Lumped'\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_distributed.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Domain Method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"Stream Threshold: {confluence.config['STREAM_THRESHOLD']}\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup - Organizing the Modeling Workflow\n",
    "\n",
    " We establish the project structure, similar to what we did in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Project Initialization\n",
    "\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition - Data Acquisition and Preparation\n",
    "\n",
    "We'll reuse some of the geospatial data from the lumped model tutorial, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can reuse data from the lumped model\n",
    "lumped_dem_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped_tutorial' / 'attributes' / 'elevation' / 'dem'\n",
    "lumped_forcing_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped_tutorial' / 'forcing' / 'raw_data'\n",
    "can_reuse = lumped_dem_path.exists()\n",
    "can_reuse_forcing = lumped_forcing_path.exists()\n",
    "\n",
    "if can_reuse or can_reuse_forcing:\n",
    "    import shutil\n",
    "    \n",
    "    # Create a function to copy files with name substitution\n",
    "    def copy_with_name_substitution(src_path, dst_path, old_str='_lumped', new_str='_distributed'):\n",
    "        if not src_path.exists():\n",
    "            return False\n",
    "            \n",
    "        # Create destination directory if it doesn't exist\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if src_path.is_dir():\n",
    "            # Copy entire directory\n",
    "            if not dst_path.exists():\n",
    "                dst_path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "            # Copy all files with name substitution\n",
    "            for src_file in src_path.glob('**/*'):\n",
    "                if src_file.is_file():\n",
    "                    # Create relative path\n",
    "                    rel_path = src_file.relative_to(src_path)\n",
    "                    # Create new filename with substitution\n",
    "                    new_name = src_file.name.replace(old_str, new_str)\n",
    "                    # Create destination path\n",
    "                    dst_file = dst_path / rel_path.parent / new_name\n",
    "                    # Create parent directories if they don't exist\n",
    "                    dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    # Copy the file\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "            return True\n",
    "        elif src_path.is_file():\n",
    "            # Copy single file with name substitution\n",
    "            new_name = dst_path.name.replace(old_str, new_str)\n",
    "            dst_file = dst_path.parent / new_name\n",
    "            dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src_path, dst_file)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    print(\"Found existing geospatial data from lumped model. Copying and renaming files...\")\n",
    "    \n",
    "    # Copy and rename DEM and other attribute data\n",
    "    if can_reuse:\n",
    "        # Define paths\n",
    "        src_attr_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped_tutorial' / 'attributes'\n",
    "        dst_attr_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed' / 'attributes'\n",
    "        \n",
    "        # Copy attributes with name substitution\n",
    "        copied = copy_with_name_substitution(src_attr_path, dst_attr_path, '_lumped_tutorial', '_distributed')\n",
    "        if copied:\n",
    "            print(\"‚úì Copied and renamed attribute files from lumped model\")\n",
    "    \n",
    "    # Copy and rename forcing data\n",
    "    if can_reuse_forcing:\n",
    "        # Define paths\n",
    "        src_forcing_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped_tutorial' / 'forcing' / 'raw_data'\n",
    "        dst_forcing_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed' / 'forcing' / 'raw_data'\n",
    "         \n",
    "        # Copy forcing data with name substitution\n",
    "        copied = copy_with_name_substitution(src_forcing_path, dst_forcing_path, '_lumped_tutorial', '_distributed')\n",
    "        if copied:\n",
    "            print(\"‚úì Copied and renamed forcing data from lumped model\")\n",
    "            \n",
    "    print(\"The distributed model will use these copied files as a starting point.\")\n",
    "else:\n",
    "    print(\"No existing data found from the lumped model. Will acquire all data from scratch.\")\n",
    "\n",
    "    # Step 2: Geospatial Domain Definition - Data Acquisition\n",
    "    print(\"\\n=== Step 2: Geospatial Domain Definition - Data Acquisition ===\")\n",
    "    \n",
    "    # Acquire attributes\n",
    "    print(\"Acquiring geospatial attributes (DEM, soil, land cover)...\")\n",
    "    confluence.managers['data'].acquire_attributes()\n",
    "\n",
    "    # Acquire forcings\n",
    "    print(f\"\\nAcquiring forcing data: {confluence.config['FORCING_DATASET']}\")\n",
    "    confluence.managers['data'].acquire_forcings()\n",
    "    \n",
    "print(\"\\n‚úì Geospatial attributes acquired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition - Delineation with Stream Network\n",
    "\n",
    "This is where the main difference occurs - we'll create multiple sub-basins connected by a stream network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Geospatial Domain Definition - Delineation\n",
    "print(\"=== Step 3: Geospatial Domain Definition - Delineation ===\")\n",
    "\n",
    "# Define domain\n",
    "print(f\"Delineating distributed watershed...\")\n",
    "print(f\"Method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"Stream threshold: {confluence.config['STREAM_THRESHOLD']}\")\n",
    "print(\"\\nThis will create multiple sub-basins connected by a stream network.\")\n",
    "\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "# Check outputs\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "if basin_path.exists():\n",
    "    basin_files = list(basin_path.glob('*.shp'))\n",
    "    print(f\"\\n‚úì Created basin shapefiles: {len(basin_files)}\")\n",
    "    \n",
    "if network_path.exists():\n",
    "    network_files = list(network_path.glob('*.shp'))\n",
    "    print(f\"‚úì Created river network shapefiles: {len(network_files)}\")\n",
    "    \n",
    "    # Load and check number of basins\n",
    "    if basin_files:\n",
    "        gdf = gpd.read_file(basin_files[0])\n",
    "        print(f\"\\nNumber of sub-basins (GRUs): {len(gdf)}\")\n",
    "        print(f\"Total area: {gdf.geometry.area.sum() / 1e6:.2f} km¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Distributed Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the delineated domain with stream network\n",
    "basin_files = list((project_dir / 'shapefiles' / 'river_basins').glob('*.shp'))\n",
    "network_files = list((project_dir / 'shapefiles' / 'river_network').glob('*.shp'))\n",
    "    \n",
    "if basin_files and network_files:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Load data\n",
    "    basins = gpd.read_file(basin_files[0])\n",
    "    rivers = gpd.read_file(network_files[0])\n",
    "    \n",
    "    # Plot basins with different colors\n",
    "    basins.plot(ax=ax, column='GRU_ID', cmap='viridis', \n",
    "               alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Plot river network\n",
    "    rivers.plot(ax=ax, color='blue', linewidth=2)\n",
    "    \n",
    "    # Add pour point\n",
    "    pour_point = gpd.read_file(pour_point_path)\n",
    "    pour_point.plot(ax=ax, color='red', markersize=150, marker='o', zorder=5)\n",
    "    \n",
    "    ax.set_title(f'Distributed Domain: {len(basins)} Sub-basins', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Add colorbar for GRU IDs\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                               norm=plt.Normalize(vmin=basins['GRU_ID'].min(), \n",
    "                                                 vmax=basins['GRU_ID'].max()))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('GRU ID', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition - Discretization\n",
    "\n",
    "Now we'll create Hydrologic Response Units (HRUs) based on the Grouped Response Units (GRUs) we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Geospatial Domain Definition - Discretization\n",
    "\n",
    "print(f\"Creating HRUs based on GRUs...\")\n",
    "print(f\"Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(\"For this tutorial: 1 GRU = 1 HRU (simplest case)\")\n",
    "\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check the created HRU shapefile\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "if catchment_path.exists():\n",
    "    hru_files = list(catchment_path.glob('*.shp'))\n",
    "    print(f\"\\n‚úì Created HRU shapefiles: {len(hru_files)}\")\n",
    "    \n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        print(f\"\\nHRU Statistics:\")\n",
    "        print(f\"Number of HRUs: {len(hru_gdf)}\")\n",
    "        print(f\"Number of GRUs: {hru_gdf['GRU_ID'].nunique()}\")\n",
    "        print(f\"Total area: {hru_gdf.geometry.area.sum() / 1e6:.2f} km¬≤\")\n",
    "        \n",
    "        # Show HRU distribution\n",
    "        hru_counts = hru_gdf.groupby('GRU_ID').size()\n",
    "        print(f\"\\nHRUs per GRU:\")\n",
    "        for gru_id, count in hru_counts.items():\n",
    "            print(f\"  GRU {gru_id}: {count} HRUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Agnostic Data Processing - Observed Data\n",
    "\n",
    "The observed streamflow data will be the same for both the lumped and distributed models since they use the same pour point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Agnostic Data Processing - Observed Data\n",
    "print(\"Processing observed streamflow data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Agnostic Data Processing - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Agnostic Data Processing - Preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\n‚úì Model-agnostic preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Specific Processing - Preprocessing\n",
    "\n",
    "Now we prepare inputs specific to our chosen hydrological model (SUMMA in this case), set up for a distributed configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Specific Processing and Initialization\n",
    "\n",
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "print(f\"Note: For distributed mode with {confluence.config['HYDROLOGICAL_MODEL']}, this includes generating:\")\n",
    "print(f\"  - Model parameter files for each GRU\")\n",
    "print(f\"  - Routing configuration for river network\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\n‚úì Model-specific preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Distributed Model\n",
    "\n",
    "Now we execute the SUMMA model in distributed mode with routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Distributed Model\n",
    "print(\"Note: This will take longer than the lumped model due to multiple units.\")\n",
    "\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\n‚úì Model execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Distributed Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Observed vs. Simulated Streamflow for Distributed Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Load and plot simulation results\n",
    "sim_path = project_dir / 'simulations' / confluence.config['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "sim_files = list(sim_path.glob('*.nc'))\n",
    "\n",
    "# Load simulation data\n",
    "print(f\"Loading simulation data from: {sim_files[0]}\")\n",
    "sim_data = xr.open_dataset(sim_files[0])\n",
    "\n",
    "# Load observation data\n",
    "obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "\n",
    "if not obs_path.exists():\n",
    "    print(f\"Warning: Observation data not found at expected path: {obs_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    alt_obs_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "        f\"domain_{config_dict['DOMAIN_NAME']}/observations/streamflow/preprocessed/*_streamflow_processed.csv\"))\n",
    "    \n",
    "    if alt_obs_paths:\n",
    "        obs_path = alt_obs_paths[0]\n",
    "        print(f\"Found alternative observation data at: {obs_path}\")\n",
    "    else:\n",
    "        print(\"No observation data found. Only simulations will be displayed.\")\n",
    "\n",
    "if obs_path.exists():\n",
    "    print(f\"Loading observation data from: {obs_path}\")\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    obs_df.set_index('datetime', inplace=True)\n",
    "    print(f\"Observation period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "else:\n",
    "    obs_df = None\n",
    "    \n",
    "# Find the segment ID for the outlet\n",
    "reach_id = int(confluence.config.get('SIM_REACH_ID', 0))\n",
    "print(f\"Using reach ID for outlet: {reach_id}\")\n",
    "\n",
    "reach_indices = np.where(sim_data.reachID.values == reach_id)[0]\n",
    "\n",
    "# Extract flow at the outlet segment\n",
    "if 'seg' in sim_data.dims:\n",
    "    sim_flow = sim_data.IRFroutedRunoff.sel(seg=reach_idx).to_series()\n",
    "else:\n",
    "    sim_flow = sim_data.IRFroutedRunoff.isel(reachID=reach_idx).to_series()\n",
    "\n",
    "sim_df = pd.DataFrame(sim_flow)\n",
    "sim_df.columns = ['discharge_cms']\n",
    "\n",
    "# Determine common time period if observations exist\n",
    "if obs_df is not None:\n",
    "    # Align to daily timestep for comparison\n",
    "    obs_daily = obs_df.resample('D').mean()\n",
    "    sim_daily = sim_df.resample('D').mean()\n",
    "    \n",
    "    # Find overlapping time period\n",
    "    start_date = max(obs_daily.index.min(), sim_daily.index.min())\n",
    "    end_date = min(obs_daily.index.max(), sim_daily.index.max())\n",
    "    \n",
    "    # Advance start date by 1 month to skip initial spinup\n",
    "    start_date = start_date + pd.DateOffset(months=1)\n",
    "    \n",
    "    print(f\"Common data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_daily.loc[start_date:end_date]\n",
    "    sim_period = sim_daily.loc[start_date:end_date]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).mean())\n",
    "    \n",
    "    # Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "    mean_obs = obs_period['discharge_cms'].mean()\n",
    "    numerator = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).sum()\n",
    "    denominator = ((obs_period['discharge_cms'] - mean_obs)**2).sum()\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    # Calculate Percent Bias (PBIAS)\n",
    "    pbias = 100 * (sim_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "    \n",
    "    # Calculate Kling-Gupta Efficiency (KGE)\n",
    "    r = obs_period['discharge_cms'].corr(sim_period['discharge_cms'])  # Correlation\n",
    "    alpha = sim_period['discharge_cms'].std() / obs_period['discharge_cms'].std()  # Relative variability\n",
    "    beta = sim_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()  # Bias ratio\n",
    "    kge = 1 - ((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)**0.5\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} m¬≥/s\")\n",
    "    print(f\"  - NSE: {nse:.2f}\")\n",
    "    print(f\"  - PBIAS: {pbias:.2f}%\")\n",
    "    print(f\"  - KGE: {kge:.2f}\")\n",
    "    \n",
    "    # Create figure with two subplots for time series and flow duration curve\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 16))\n",
    "    fig.suptitle(f\"Distributed Model Results - {confluence.config['DOMAIN_NAME'].replace('_', ' ').title()}\", \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot time series\n",
    "    ax1.plot(obs_period.index, obs_period['discharge_cms'], \n",
    "             'b-', label='Observed', linewidth=1.5, alpha=0.7)\n",
    "    ax1.plot(sim_period.index, sim_period['discharge_cms'], \n",
    "             'r-', label='Simulated (Distributed)', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "    ax1.set_xlabel('Date', fontsize=12)\n",
    "    ax1.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    ax1.set_title('Streamflow Comparison', fontsize=14)\n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "    ax1.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    \n",
    "    # Add metrics as text\n",
    "    ax1.text(0.02, 0.95, \n",
    "             f\"RMSE: {rmse:.2f} m¬≥/s\\nNSE: {nse:.2f}\\nPBIAS: {pbias:.2f}%\\nKGE: {kge:.2f}\",\n",
    "             transform=ax1.transAxes, \n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Plot flow duration curve\n",
    "    # Sort values in descending order\n",
    "    obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "    sim_sorted = sim_period['discharge_cms'].sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate exceedance probabilities\n",
    "    obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "    sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted)\n",
    "    \n",
    "    # Plot Flow Duration Curves\n",
    "    ax2.loglog(obs_ranks * 100, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "    ax2.loglog(sim_ranks * 100, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "    ax2.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    ax2.set_title('Flow Duration Curve', fontsize=14)\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, which='both', linestyle=':', alpha=0.6)\n",
    "    ax2.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Add flow regime regions\n",
    "    ax2.axvspan(0, 20, alpha=0.2, color='blue', label='High Flows')\n",
    "    ax2.axvspan(20, 70, alpha=0.2, color='green', label='Medium Flows')\n",
    "    ax2.axvspan(70, 100, alpha=0.2, color='red', label='Low Flows')\n",
    "    \n",
    "    # Save the plot to file\n",
    "    plot_folder = project_dir / \"plots\" / \"results\"\n",
    "    plot_folder.mkdir(parents=True, exist_ok=True)\n",
    "    plot_filename = plot_folder / f\"{confluence.config['EXPERIMENT_ID']}_streamflow_comparison.png\"\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to: {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    # If no observations, just plot simulation\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(sim_df.index, sim_df['discharge_cms'], \n",
    "            color='red', linewidth=1.5, label='Simulated (Distributed)')\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    ax.set_title(f'Distributed Model Results - {confluence.config[\"DOMAIN_NAME\"].replace(\"_\", \" \").title()}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "sim_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Optimization and Analysis (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete workflow in one step\n",
    "# (Uncomment to use this instead of the step-by-step approach)\n",
    "\n",
    "# confluence.run_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
