{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 3: Lumped Basin Workflow (Bow River at Banff)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial shows how to scale up from point-scale modeling to basin-scale streamflow simulation using CONFLUENCE. Building on our previous tutorials with SNOTEL and FLUXNET data, we now demonstrate how to model an entire watershed as a single unit to generate streamflow at the basin outlet.\n",
    "\n",
    "### What is Lumped Basin Modeling?\n",
    "\n",
    "Lumped basin modeling treats the entire watershed as one homogeneous unit, averaging all the spatial variability across the catchment. While this is a simplification, it's a valuable approach because:\n",
    "\n",
    "- **Simplicity**: Easier to understand and implement than distributed models\n",
    "- **Computational efficiency**: Fast execution makes it ideal for calibration and uncertainty analysis  \n",
    "- **Baseline performance**: Establishes whether a model can capture the basic watershed response before adding spatial complexity\n",
    "- **Parameter identification**: Simpler structure makes it easier to understand which parameters control model behavior\n",
    "\n",
    "### Case Study: Bow River at Banff\n",
    "\n",
    "We'll use the Bow River at Banff as our example watershed:\n",
    "\n",
    "- **Location**: Canadian Rockies, Alberta, Canada\n",
    "- **Drainage area**: ~2,210 kmÂ²\n",
    "- **Elevation**: Ranges from 1,384 m at the outlet to over 3,400 m in the headwaters\n",
    "- **Climate**: Snow-dominated mountain system with pronounced seasonal cycles\n",
    "- **Gauging station**: Water Survey of Canada station 05BB001 with long-term observations\n",
    "\n",
    "This watershed presents interesting modeling challenges:\n",
    "- Strong elevation gradients affecting temperature and precipitation\n",
    "- Complex snow dynamics across elevation zones\n",
    "- Seasonal storage in snowpack and glaciers\n",
    "- Pronounced spring freshet from snowmelt\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "This tutorial will teach you how to:\n",
    "\n",
    "1. **Set up a basin-scale project** with CONFLUENCE's automated workflow\n",
    "2. **Delineate watersheds** automatically from digital elevation models\n",
    "3. **Aggregate spatial data** to create catchment-averaged characteristics\n",
    "4. **Process meteorological forcing** data for basin-scale modeling\n",
    "5. **Configure and run SUMMA** for lumped basin simulation\n",
    "6. **Evaluate model performance** using standard hydrological metrics\n",
    "7. **Interpret results** and understand model limitations\n",
    "\n",
    "### Tutorial Overview\n",
    "\n",
    "We'll walk through the complete CONFLUENCE workflow step by step:\n",
    "\n",
    "1. **Project Setup**: Create the organized directory structure\n",
    "2. **Watershed Delineation**: Automatically identify the watershed boundary\n",
    "3. **Data Acquisition**: Get elevation, soil, and land cover data\n",
    "4. **Forcing Data**: Process meteorological inputs\n",
    "5. **Model Configuration**: Set up SUMMA for the lumped basin\n",
    "6. **Model Execution**: Run the simulation\n",
    "7. **Results Analysis**: Compare simulated and observed streamflow\n",
    "\n",
    "By the end of this tutorial, you'll understand how CONFLUENCE handles the transition from point-scale to basin-scale modeling and be ready to explore more complex distributed modeling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize CONFLUENCE\n",
    "Now let's set up our directories and load the configuration. CONFLUENCE uses a centralized configuration file that controls all aspects of the modeling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # â† User should modify this path\n",
    "\n",
    "# Load and update configuration\n",
    "config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "\n",
    "# Read config file and update paths\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Update name and experiment id\n",
    "config_dict['DOMAIN_NAME'] = 'Bow_at_Banff_lumped_tutorial'\n",
    "config_dict['EXPERIMENT_ID'] = 'tutorial_run'\n",
    "config_dict['EXPERIMENT_TIME_START'] = '2004-01-01 01:00'\n",
    "config_dict['EXPERIMENT_TIME_END'] = '2022-12-31 23:00'\n",
    "\n",
    "\n",
    "config_dict['HYDROLOGICAL_MODEL'] = 'FUSE'\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Setup - Organizing the Modeling Workflow\n",
    "The first step in any CONFLUENCE workflow is to establish a well-organized project structure. This might seem trivial, but it's crucial for:\n",
    "\n",
    "- Maintaining consistency across different experiments\n",
    "- Ensuring all components can find required files\n",
    "- Enabling reproducibility\n",
    "- Facilitating collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Project Initialization\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  ðŸ“ {item.name}\")\n",
    "\n",
    "print(\"\\nDirectory purposes:\")\n",
    "print(\"  ðŸ“ shapefiles: Domain geometry (watershed, pour points, river network)\")\n",
    "print(\"  ðŸ“ attributes: Static characteristics (elevation, soil, land cover)\")\n",
    "print(\"  ðŸ“ forcing: Meteorological inputs (precipitation, temperature)\")\n",
    "print(\"  ðŸ“ simulations: Model outputs\")\n",
    "print(\"  ðŸ“ evaluation: Performance metrics and comparisons\")\n",
    "print(\"  ðŸ“ plots: Visualizations\")\n",
    "print(\"  ðŸ“ optimisation: Calibration results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition and Analysis - Data acquisition \n",
    "Before we can delineate the watershed, we need elevation data. CONFLUENCE also acquires soil and land cover data at this stage for later use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes (DEM, soil, land cover)...\")\n",
    "confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition and Analysis - Delineation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define domain\n",
    "print(f\"\\nDelineating watershed using method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Domain Definition and Analysis - Discretisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discretize domain\n",
    "print(f\"\\nCreating HRUs using method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Delineated Domain\n",
    "Let's see what our watershed looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the watershed\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "if basin_path.exists():\n",
    "    basin_files = list(basin_path.glob('*.shp'))\n",
    "    \n",
    "    if basin_files:\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Load watershed and pour point\n",
    "        basin_gdf = gpd.read_file(basin_files[0])\n",
    "        pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "        \n",
    "        # Reproject for visualization\n",
    "        basin_web = basin_gdf.to_crs(epsg=3857)\n",
    "        pour_web = pour_point_gdf.to_crs(epsg=3857)\n",
    "        \n",
    "        # Plot watershed\n",
    "        basin_web.plot(ax=ax, facecolor='lightblue', edgecolor='navy', \n",
    "                       linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add pour point\n",
    "        pour_web.plot(ax=ax, color='red', markersize=200, marker='o', \n",
    "                      edgecolor='white', linewidth=2, zorder=5)\n",
    "                \n",
    "        # Set extent\n",
    "        minx, miny, maxx, maxy = basin_web.total_bounds\n",
    "        pad = 5000\n",
    "        ax.set_xlim(minx - pad, maxx + pad)\n",
    "        ax.set_ylim(miny - pad, maxy + pad)\n",
    "        \n",
    "        ax.set_title('Bow River Watershed at Banff \\n All water from this area flows to the pour point', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Agnostic Data Pre-Processing - Observed data\n",
    "For a lumped model, the entire watershed becomes a single Hydrologic Response Unit (HRU). This simplification assumes uniform characteristics across the watershed - obviously an approximation, but useful for many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Agnostic Data Pre-Processing\n",
    "print(\"Processing observed streamflow data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize observed streamflow data\n",
    "obs_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "if obs_path.exists():\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(obs_df['datetime'], obs_df['discharge_cms'], \n",
    "            linewidth=1.5, color='blue', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "    ax.set_title(f'Observed Streamflow - Bow River at Banff (WSC Station: {confluence.config[\"STATION_ID\"]})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    ax.text(0.02, 0.95, f'Mean: {obs_df[\"discharge_cms\"].mean():.1f} mÂ³/s\\\\nMax: {obs_df[\"discharge_cms\"].max():.1f} mÂ³/s', \n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Agnostic Data Pre-Processing - Forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data: {confluence.config['FORCING_DATASET']}\")\n",
    "#confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Agnostic Data Pre-Processing - Remapping and zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Specific Preprocessing\n",
    "Now we prepare inputs specific to our chosen hydrological model (SUMMA in this case). Each model has its own requirements for input format and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Specific Processing \n",
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "confluence.managers['model'].preprocess_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} model...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nModel run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of results\n",
    "\n",
    "Let's see how our streamflow hydrograph looks for the default instantiation sof SUMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Observed vs. Simulated Streamflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.dates as mdates\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load the observed streamflow data - using lumped case approach\n",
    "obs_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "if not obs_path.exists():\n",
    "    print(f\"Warning: Observed streamflow data not found at {obs_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    alt_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"**/observations/streamflow/preprocessed/*_streamflow_processed.csv\"))\n",
    "    if alt_paths:\n",
    "        obs_path = alt_paths[0]\n",
    "        print(f\"Found alternative streamflow data at: {obs_path}\")\n",
    "    else:\n",
    "        print(\"No observed streamflow data found. Only simulated data will be displayed.\")\n",
    "\n",
    "# 2. Load the simulated streamflow data from SUMMA output - using lumped case approach\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_timestep.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "        f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Extract averageRoutedRunoff\n",
    "print(\"Extracting 'averageRoutedRunoff' variable...\")\n",
    "if 'averageRoutedRunoff' in ds:\n",
    "    # Extract and convert to DataFrame\n",
    "    sim_runoff = ds['averageRoutedRunoff'].to_dataframe().reset_index()\n",
    "    \n",
    "    # Get catchment area from the river basin shapefile to convert from m/s to mÂ³/s\n",
    "    basin_shapefile = config_dict.get('RIVER_BASINS_NAME', 'default')\n",
    "    if basin_shapefile == 'default':\n",
    "        basin_shapefile = f\"{config_dict['DOMAIN_NAME']}_riverBasins_{config_dict.get('DOMAIN_DEFINITION_METHOD', 'lumped')}.shp\"\n",
    "    \n",
    "    basin_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"shapefiles\" / \"river_basins\" / basin_shapefile\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading catchment shapefile from: {basin_path}\")\n",
    "        basin_gdf = gpd.read_file(basin_path)\n",
    "        area_col = config_dict.get('RIVER_BASIN_SHP_AREA', 'GRU_area')\n",
    "        \n",
    "        # Area should be in mÂ²\n",
    "        if area_col in basin_gdf.columns:\n",
    "            area_m2 = basin_gdf[area_col].sum()\n",
    "            print(f\"Catchment area: {area_m2:.2f} mÂ² ({area_m2/1e6:.2f} kmÂ²)\")\n",
    "            \n",
    "            # Convert from m/s to mÂ³/s by multiplying by area in mÂ²\n",
    "            # Assuming first GRU for lumped basin simulation if multiple GRUs exist\n",
    "            if 'gru' in sim_runoff.columns:\n",
    "                sim_runoff = sim_runoff[sim_runoff['gru'] == 1][['time', 'averageRoutedRunoff']]\n",
    "            else:\n",
    "                sim_runoff = sim_runoff[['time', 'averageRoutedRunoff']]\n",
    "            \n",
    "            # Convert units: m/s -> mÂ³/s\n",
    "            sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff'] * area_m2\n",
    "            print(f\"Converted runoff from m/s to mÂ³/s (multiplied by basin area)\")\n",
    "        else:\n",
    "            print(f\"Warning: Area column '{area_col}' not found in catchment shapefile\")\n",
    "            sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff']  # Use raw values as fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting basin area: {str(e)}. Using raw values.\")\n",
    "        sim_runoff['discharge_cms'] = sim_runoff['averageRoutedRunoff']  # Use raw values as fallback\n",
    "    \n",
    "    # Set index to time for easier processing\n",
    "    sim_runoff.set_index('time', inplace=True)\n",
    "    sim_df = sim_runoff[['discharge_cms']]\n",
    "else:\n",
    "    print(\"Warning: 'averageRoutedRunoff' variable not found in the SUMMA output\")\n",
    "    print(\"Available variables:\", list(ds.data_vars))\n",
    "    raise ValueError(\"Required 'averageRoutedRunoff' variable not found in SUMMA output\")\n",
    "\n",
    "# Load observed data\n",
    "obs_df = None\n",
    "if obs_path.exists():\n",
    "    print(f\"Loading observed streamflow data from: {obs_path}\")\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    obs_df.set_index('datetime', inplace=True)\n",
    "    print(f\"Observed data period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "    print(f\"Observed streamflow range: {obs_df['discharge_cms'].min():.2f} to {obs_df['discharge_cms'].max():.2f} mÂ³/s\")\n",
    "\n",
    "# Show simulated data info\n",
    "print(f\"Simulated data period: {sim_df.index.min()} to {sim_df.index.max()}\")\n",
    "print(f\"Simulated streamflow range: {sim_df['discharge_cms'].min():.2f} to {sim_df['discharge_cms'].max():.2f} mÂ³/s\")\n",
    "\n",
    "# Visualization using distributed case approach\n",
    "if obs_df is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_daily = obs_df.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    sim_daily = sim_df.resample('D').mean()  # Daily mean if sub-daily sim data\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_daily.index.min(), sim_daily.index.min())\n",
    "    end_date = min(obs_daily.index.max(), sim_daily.index.max())\n",
    "    \n",
    "    # Advance start date by 1 month to skip initial spinup\n",
    "    start_date = start_date + pd.DateOffset(months=1)\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_daily.loc[start_date:end_date]\n",
    "    sim_period = sim_daily.loc[start_date:end_date]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    # Calculate root mean square error (RMSE)\n",
    "    rmse = np.sqrt(((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).mean())\n",
    "    \n",
    "    # Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "    mean_obs = obs_period['discharge_cms'].mean()\n",
    "    numerator = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).sum()\n",
    "    denominator = ((obs_period['discharge_cms'] - mean_obs)**2).sum()\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    # Calculate Percent Bias (PBIAS)\n",
    "    pbias = 100 * (sim_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "    \n",
    "    # Calculate Kling-Gupta Efficiency (KGE)\n",
    "    r = obs_period['discharge_cms'].corr(sim_period['discharge_cms'])  # Correlation\n",
    "    alpha = sim_period['discharge_cms'].std() / obs_period['discharge_cms'].std()  # Relative variability\n",
    "    beta = sim_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()  # Bias ratio\n",
    "    kge = 1 - ((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)**0.5\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} mÂ³/s\")\n",
    "    print(f\"  - NSE: {nse:.2f}\")\n",
    "    print(f\"  - PBIAS: {pbias:.2f}%\")\n",
    "    print(f\"  - KGE: {kge:.2f}\")\n",
    "    \n",
    "    # Create figure with two subplots for time series and flow duration curve - using distributed case style\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 16))\n",
    "    fig.suptitle(f\"Lumped Model Results - {confluence.config['DOMAIN_NAME'].replace('_', ' ').title()}\", \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot time series\n",
    "    ax1.plot(obs_period.index, obs_period['discharge_cms'], \n",
    "             'b-', label='Observed', linewidth=1.5, alpha=0.7)\n",
    "    ax1.plot(sim_period.index, sim_period['discharge_cms'], \n",
    "             'r-', label='Simulated (Lumped)', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "    ax1.set_xlabel('Date', fontsize=12)\n",
    "    ax1.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "    ax1.set_title('Streamflow Comparison', fontsize=14)\n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "    ax1.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    \n",
    "    # Add metrics as text\n",
    "    ax1.text(0.02, 0.95, \n",
    "             f\"RMSE: {rmse:.2f} mÂ³/s\\nNSE: {nse:.2f}\\nPBIAS: {pbias:.2f}%\\nKGE: {kge:.2f}\",\n",
    "             transform=ax1.transAxes, \n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Plot flow duration curve\n",
    "    # Sort values in descending order\n",
    "    obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "    sim_sorted = sim_period['discharge_cms'].sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate exceedance probabilities\n",
    "    obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "    sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted)\n",
    "    \n",
    "    # Plot Flow Duration Curves\n",
    "    ax2.loglog(obs_ranks * 100, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "    ax2.loglog(sim_ranks * 100, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "    ax2.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "    ax2.set_title('Flow Duration Curve', fontsize=14)\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, which='both', linestyle=':', alpha=0.6)\n",
    "    ax2.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Add flow regime regions\n",
    "    ax2.axvspan(0, 20, alpha=0.2, color='blue', label='High Flows')\n",
    "    ax2.axvspan(20, 70, alpha=0.2, color='green', label='Medium Flows')\n",
    "    ax2.axvspan(70, 100, alpha=0.2, color='red', label='Low Flows')\n",
    "    \n",
    "    # Save the plot to file\n",
    "    plot_folder = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"plots\" / \"results\"\n",
    "    plot_folder.mkdir(parents=True, exist_ok=True)\n",
    "    plot_filename = plot_folder / f\"{confluence.config['EXPERIMENT_ID']}_streamflow_comparison.png\"\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to: {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    # If no observations, just plot simulation\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(sim_df.index, sim_df['discharge_cms'], \n",
    "            color='red', linewidth=1.5, label='Simulated (Lumped)')\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "    ax.set_title(f'Lumped Model Results - {confluence.config[\"DOMAIN_NAME\"].replace(\"_\", \" \").title()}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nStreamflow visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Optional Steps - Optimization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 & 6: Optional Steps (Optimization and Analysis)\n",
    "print(\"=== Step 5 & 6: Optional Steps ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - Run Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete workflow in one step\n",
    "# (Uncomment to use this instead of the step-by-step approach)\n",
    "\n",
    "# confluence.run_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Understanding the CONFLUENCE Workflow\n",
    "Congratulations! You've completed a full lumped basin modeling workflow with CONFLUENCE. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
