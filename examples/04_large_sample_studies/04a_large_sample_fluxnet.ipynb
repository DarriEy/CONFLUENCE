{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6c1149",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 8: Large Sample Studies (FLUXNET Multi-Site Analysis)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial represents the culmination of our CONFLUENCE series, demonstrating the evolution from single-domain modeling to large sample studies. While previous tutorials focused on modeling individual domains across different scales, large sample studies leverage CONFLUENCE's workflow efficiency to systematically analyze hundreds or thousands of sites, enabling comparative hydrology and robust statistical analysis.\n",
    "\n",
    "### Large Sample Studies in Hydrology\n",
    "\n",
    "Large sample studies in hydrology involve systematic analysis across many sites, watersheds, or regions to identify patterns in hydrological processes across different environments, test theoretical concepts under diverse conditions, and develop improved model parameterizations based on multi-site evidence. This approach enables researchers to quantify uncertainty, assess model performance and reliability across different settings, and advance comparative hydrology by comparing hydrological responses across climates, landscapes, and scales.\n",
    "\n",
    "### FLUXNET as a Framework for Large Sample Analysis\n",
    "\n",
    "The FLUXNET network provides an ideal framework for large sample hydrological analysis due to its global coverage spanning all continents and diverse ecosystems including forests, grasslands, wetlands, and croplands across multiple climate zones from tropical to boreal and arid regions. The network's standardized eddy covariance methodology ensures consistent, quality-controlled data processing with comparable variables across sites at standardized temporal resolution.\n",
    "\n",
    "FLUXNET data offers significant scientific value for energy balance validation of land surface models, ecosystem-scale process understanding, climate-vegetation interaction studies, and model benchmarking across diverse environmental conditions. This makes it particularly well-suited for systematic multi-site hydrological analysis.\n",
    "\n",
    "### CONFLUENCE's Large Sample Capabilities\n",
    "\n",
    "CONFLUENCE's design makes it particularly effective for large sample analysis through workflow automation that reduces manual effort per site while maintaining consistent methodology across all sites to ensure comparability. The system's template-based configuration enables rapid site setup, while complete workflow documentation ensures reproducible science. High-performance computing capabilities allow parallel execution across multiple sites, and standardized output formats facilitate multi-site analysis.\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "Large sample studies with CONFLUENCE involve several key components: strategic site selection across environmental gradients, automated configuration generation for site-specific setups, efficient batch processing of CONFLUENCE across multiple sites, systematic results aggregation and standardization from all sites, and comprehensive comparative analysis to identify patterns and relationships across sites.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "This tutorial will teach you to design large sample experiments with appropriate site selection strategies, automate configuration generation for hundreds of sites, manage batch processing of multiple CONFLUENCE runs, aggregate and analyze results from multi-site experiments, visualize patterns across environmental gradients, and apply statistical methods to understand hydrological controls.\n",
    "\n",
    "The combination of CONFLUENCE's workflow efficiency with large sample methodologies opens new possibilities for understanding how hydrological processes vary across Earth's diverse environments, from individual flux towers to global patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573758bb-ff23-4672-83a5-fc5ed9912307",
   "metadata": {},
   "source": [
    "## Step 1: Large Sample Template Configuration and Experimental Design\n",
    "This tutorial represents the next evolution in our CONFLUENCE series: large sample studies. Rather than scaling to larger spatial domains, we now leverage CONFLUENCE's workflow efficiency to systematically analyze hundreds of sites across global environmental gradients. Using the FLUXNET network, we demonstrate how to transform CONFLUENCE from a single-domain modeling platform into a powerful engine for comparative hydrology and statistical analysis across diverse ecosystems.\n",
    "\n",
    "The same CONFLUENCE framework now scales to handle systematic multi-site analysis while maintaining the workflow consistency and scientific rigor established throughout our tutorial series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "confluence_path = Path('../').resolve()\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE EXPERIMENTAL DESIGN CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data')  # ‚Üê Update this path\n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# Load point scale configuration template or create from base template\n",
    "na_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "with open(na_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update for tutorial-specific settings\n",
    "config_updates = {\n",
    "    'CONFLUENCE_CODE_DIR': str(CONFLUENCE_CODE_DIR),\n",
    "    'CONFLUENCE_DATA_DIR': str(CONFLUENCE_DATA_DIR),\n",
    "    'DOMAIN_NAME': 'fluxnet',\n",
    "    'EXPERIMENT_ID': 'run_1',\n",
    "    'EXPERIMENT_TIME_START': '2018-01-01 01:00',\n",
    "    'EXPERIMENT_TIME_END': '2018-03-31 23:00',  # Short for tutorial demonstration\n",
    "}\n",
    "\n",
    "config_dict.update(config_updates)\n",
    "\n",
    "# Save continental configuration\n",
    "continental_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_fluxnet_template.yaml'\n",
    "with open(continental_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Fluxnet templare configuration saved: {continental_config_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND EXAMINE FLUXNET DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nLoading FLUXNET Site Database...\")\n",
    "\n",
    "# Load the FLUXNET sites database\n",
    "try:\n",
    "    fluxnet_df = pd.read_csv('fluxnet_towers.csv')\n",
    "    print(f\"Successfully loaded FLUXNET database: {len(fluxnet_df)} sites available\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: FLUXNET database not found\")\n",
    "    print(f\"Please ensure 'fluxnet_transformed.csv' is in the current directory\")\n",
    "    raise\n",
    "\n",
    "# Display basic dataset information\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"  Total sites: {len(fluxnet_df)}\")\n",
    "print(f\"  Columns: {len(fluxnet_df.columns)}\")\n",
    "print(f\"  Column names: {', '.join(fluxnet_df.columns)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT SPATIAL COORDINATES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nExtracting Spatial Information...\")\n",
    "\n",
    "# Parse coordinate information\n",
    "try:\n",
    "    coords = fluxnet_df['POUR_POINT_COORDS'].str.split('/', expand=True)\n",
    "    fluxnet_df['latitude'] = coords[0].astype(float)\n",
    "    fluxnet_df['longitude'] = coords[1].astype(float)\n",
    "    \n",
    "    print(f\"Coordinate extraction successful\")\n",
    "    print(f\"  Latitude range: {fluxnet_df['latitude'].min():.1f}¬∞ to {fluxnet_df['latitude'].max():.1f}¬∞\")\n",
    "    print(f\"  Longitude range: {fluxnet_df['longitude'].min():.1f}¬∞ to {fluxnet_df['longitude'].max():.1f}¬∞\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting coordinates: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET CHARACTERISTICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nAnalyzing Dataset Characteristics...\")\n",
    "\n",
    "# Climate classification analysis\n",
    "if 'KG' in fluxnet_df.columns:\n",
    "    climate_counts = fluxnet_df['KG'].value_counts()\n",
    "    print(f\"  Climate types (K√∂ppen-Geiger): {len(climate_counts)}\")\n",
    "    print(f\"    Most common: {climate_counts.index[0]} ({climate_counts.iloc[0]} sites)\")\n",
    "\n",
    "# Land cover analysis\n",
    "if 'Dominant_LC' in fluxnet_df.columns:\n",
    "    landcover_counts = fluxnet_df['Dominant_LC'].value_counts()\n",
    "    print(f\"  Land cover types: {len(landcover_counts)}\")\n",
    "    print(f\"    Most common: {landcover_counts.index[0]} ({landcover_counts.iloc[0]} sites)\")\n",
    "\n",
    "# Area analysis\n",
    "if 'Area_km2' in fluxnet_df.columns:\n",
    "    area_stats = fluxnet_df['Area_km2'].describe()\n",
    "    print(f\"  Area range: {area_stats['min']:.2f} to {area_stats['max']:.2f} km¬≤\")\n",
    "    print(f\"  Mean area: {area_stats['mean']:.2f} km¬≤\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nCreating Dataset Overview Visualization...\")\n",
    "\n",
    "# Create comprehensive dataset overview\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Global distribution map\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(fluxnet_df['longitude'], fluxnet_df['latitude'], \n",
    "           c='blue', alpha=0.6, s=30)\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'Global FLUXNET Site Distribution\\n({len(fluxnet_df)} sites)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-180, 180)\n",
    "ax1.set_ylim(-60, 80)\n",
    "\n",
    "# Climate type distribution\n",
    "ax2 = axes[0, 1]\n",
    "if 'KG' in fluxnet_df.columns:\n",
    "    climate_counts = fluxnet_df['KG'].value_counts()\n",
    "    bars = ax2.bar(range(len(climate_counts)), climate_counts.values, \n",
    "                   color='skyblue', alpha=0.7)\n",
    "    ax2.set_xticks(range(len(climate_counts)))\n",
    "    ax2.set_xticklabels(climate_counts.index, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Number of Sites')\n",
    "    ax2.set_title('Climate Type Distribution')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Land cover distribution\n",
    "ax3 = axes[1, 0]\n",
    "if 'Dominant_LC' in fluxnet_df.columns:\n",
    "    lc_counts = fluxnet_df['Dominant_LC'].value_counts()\n",
    "    bars = ax3.bar(range(len(lc_counts)), lc_counts.values, \n",
    "                   color='lightgreen', alpha=0.7)\n",
    "    ax3.set_xticks(range(len(lc_counts)))\n",
    "    ax3.set_xticklabels(lc_counts.index, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Number of Sites')\n",
    "    ax3.set_title('Land Cover Distribution')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Area distribution\n",
    "ax4 = axes[1, 1]\n",
    "if 'Area_km2' in fluxnet_df.columns:\n",
    "    ax4.hist(fluxnet_df['Area_km2'], bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Area (km¬≤)')\n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Site Area Distribution')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('FLUXNET Dataset Overview', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Geographic coverage: {len(fluxnet_df)} sites globally\")\n",
    "print(f\"  Latitudinal span: {fluxnet_df['latitude'].max() - fluxnet_df['latitude'].min():.0f}¬∞\")\n",
    "print(f\"  Environmental diversity:\")\n",
    "if 'KG' in fluxnet_df.columns:\n",
    "    print(f\"    Climate types: {len(fluxnet_df['KG'].unique())}\")\n",
    "if 'Dominant_LC' in fluxnet_df.columns:\n",
    "    print(f\"    Land cover types: {len(fluxnet_df['Dominant_LC'].unique())}\")\n",
    "if 'Area_km2' in fluxnet_df.columns:\n",
    "    print(f\"    Area range: {fluxnet_df['Area_km2'].min():.1f} - {fluxnet_df['Area_km2'].max():.1f} km¬≤\")\n",
    "\n",
    "print(f\"\\nSection 1 Complete: FLUXNET dataset loaded and analyzed\")\n",
    "print(f\"Ready for large sample processing workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543f4a3-1daf-4bcc-b105-7ae491988144",
   "metadata": {},
   "source": [
    "## Step 2: Automated CONFLUENCE Configuration and Batch Processing\n",
    "\n",
    "Building on the FLUXNET dataset analysis and default configuration from Step 1, this step demonstrates automated large sample processing using the `run_watersheds_fluxnet.py` script. This script performs two key functions:\n",
    "\n",
    "**Configuration Generation**: The script reads the FLUXNET site database and automatically creates individual CONFLUENCE configuration files for each site. Each configuration is customized with site-specific parameters including domain coordinates, bounding box definitions, and unique identifiers, while maintaining consistent model settings across all sites.\n",
    "\n",
    "**Batch Job Submission**: The script submits SLURM jobs to execute the complete CONFLUENCE workflow for each FLUXNET site in parallel. Each job processes geographic data, prepares meteorological forcing, processes FLUXNET observations, runs the hydrological model, and generates standardized output files.\n",
    "\n",
    "This automated approach scales CONFLUENCE from single-domain modeling to systematic multi-site analysis across hundreds of locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcddeb7-9aa0-4c26-a6a3-e50aa79d15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fluxnet_script_from_notebook():\n",
    "    \"\"\"\n",
    "    Execute the run_watersheds_fluxnet.py script from within the notebook\n",
    "    \"\"\"\n",
    "    print(f\"\\n Executing FLUXNET Large Sample Processing Script...\")\n",
    "    \n",
    "    script_path = \"./run_watersheds_fluxnet.py\"\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"‚ùå Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"   Script location: {script_path}\")\n",
    "    print(f\"   Target sites: {len(selected_df)} FLUXNET sites\")\n",
    "    print(f\"   Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Run the script with automated responses\n",
    "        # Note: This assumes the script will use the CSV file created in Step 1\n",
    "        \n",
    "        # Create a process with input automation\n",
    "        process = subprocess.Popen(\n",
    "            ['python', script_path],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Send 'y' to confirm job submission when prompted\n",
    "        stdout, stderr = process.communicate(input='y\\n')\n",
    "        \n",
    "        # Print the output\n",
    "        if stdout:\n",
    "            print(\"üìã Script Output:\")\n",
    "            for line in stdout.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"‚ö†Ô∏è  Script Warnings/Errors:\")\n",
    "            for line in stderr.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"‚úÖ FLUXNET processing script completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Script failed with return code: {process.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running script: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute the FLUXNET processing script\n",
    "script_success = run_fluxnet_script_from_notebook()\n",
    "\n",
    "if script_success:\n",
    "    print(f\"\\n‚úÖ Step 2 Complete: Large sample processing initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbaeef-85a4-4d8b-a0a9-ee908f9631cd",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Site Output Analysis and ET Validation\n",
    "\n",
    "Having executed large sample processing, this step demonstrates the analytical power that emerges from systematic multi-site CONFLUENCE results. The analysis showcases comprehensive spatial analysis, statistical comparison, and process validation across diverse environmental gradients.\n",
    "\n",
    "### From Case Studies to Comparative Hydrology\n",
    "\n",
    "Traditional hydrological analysis relies on individual site interpretation and validation, resulting in site-specific model evaluation with limited generalizability. This approach makes it difficult to distinguish universal processes from local effects and requires manual comparison across disparate studies with limited statistical power for robust pattern identification.\n",
    "\n",
    "Large sample analysis enables systematic multi-site comparative hydrology through spatial pattern recognition across global environmental gradients. This approach provides sufficient sample sizes for statistical hypothesis testing, allows assessment of process universality by distinguishing general from site-specific patterns, and enables evaluation of model transferability across diverse environmental conditions.\n",
    "\n",
    "The transition from individual case studies to large sample analysis represents a fundamental shift in hydrological science, moving from descriptive site-specific understanding to predictive process-based knowledge applicable across diverse environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360883a-8a2d-4eee-bfa7-a1990b1455f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_completed_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed FLUXNET domain directories and their outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Discovering Completed FLUXNET Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    data_dir_pattern = str(CONFLUENCE_DATA_DIR / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(data_dir_pattern)\n",
    "    \n",
    "    print(f\"   üìÅ Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a FLUXNET domain (should match our selected sites)\n",
    "        if any(domain_name in site for site in selected_df['DOMAIN_NAME'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"catchment\" / f\"{domain_name}_HRUs.shp\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'simulation_files': []\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                nc_files = list(simulation_dir.glob(\"**/*.nc\"))\n",
    "                domain_info['simulation_files'] = nc_files\n",
    "                domain_info['has_results'] = len(nc_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   üéØ FLUXNET domains found: {len(completed_domains)}\")\n",
    "    print(f\"   üìä Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   üìà Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\nüó∫Ô∏è  Creating Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    ax1.scatter(selected_df['longitude'], selected_df['latitude'], \n",
    "               c='lightgray', alpha=0.5, s=30, label='Selected sites', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in selected_df\n",
    "        site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            lat = site_row['latitude'].iloc[0]\n",
    "            lon = site_row['longitude'].iloc[0]\n",
    "            \n",
    "            # Color based on completion status\n",
    "            if domain['has_results']:\n",
    "                color = 'green'\n",
    "                label = 'Complete with results'\n",
    "                marker = 's'\n",
    "                size = 50\n",
    "            elif domain['has_shapefile']:\n",
    "                color = 'orange' \n",
    "                label = 'Shapefile only'\n",
    "                marker = '^'\n",
    "                size = 40\n",
    "            else:\n",
    "                color = 'red'\n",
    "                label = 'Processing started'\n",
    "                marker = 'v'\n",
    "                size = 30\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=color, s=size, marker=marker, alpha=0.8,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('FLUXNET Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(-60, 80)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='green', s=50, marker='s', label='Complete with results'),\n",
    "        plt.scatter([], [], c='orange', s=40, marker='^', label='Shapefile generated'),\n",
    "        plt.scatter([], [], c='red', s=30, marker='v', label='Processing started'),\n",
    "        plt.scatter([], [], c='lightgray', s=30, marker='o', label='Selected sites')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    # Map 2: Completion statistics by climate type\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if 'KG' in selected_df.columns:\n",
    "        # Count completion by climate type\n",
    "        climate_completion = {}\n",
    "        \n",
    "        for domain in completed_domains:\n",
    "            domain_name = domain['domain_name']\n",
    "            site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "            \n",
    "            if not site_row.empty:\n",
    "                climate = site_row['KG'].iloc[0]\n",
    "                \n",
    "                if climate not in climate_completion:\n",
    "                    climate_completion[climate] = {'total': 0, 'complete': 0, 'partial': 0}\n",
    "                \n",
    "                climate_completion[climate]['total'] += 1\n",
    "                \n",
    "                if domain['has_results']:\n",
    "                    climate_completion[climate]['complete'] += 1\n",
    "                elif domain['has_shapefile']:\n",
    "                    climate_completion[climate]['partial'] += 1\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        climates = list(climate_completion.keys())\n",
    "        complete_counts = [climate_completion[c]['complete'] for c in climates]\n",
    "        partial_counts = [climate_completion[c]['partial'] for c in climates]\n",
    "        pending_counts = [climate_completion[c]['total'] - \n",
    "                         climate_completion[c]['complete'] - \n",
    "                         climate_completion[c]['partial'] for c in climates]\n",
    "        \n",
    "        x_pos = range(len(climates))\n",
    "        \n",
    "        ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.7)\n",
    "        ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.7)\n",
    "        ax2.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(climates, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Number of Sites')\n",
    "        ax2.set_title('Processing Status by Climate Type')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Domain area distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    domain_areas = []\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty and 'Area_km2' in site_row.columns:\n",
    "            area = site_row['Area_km2'].iloc[0]\n",
    "            domain_areas.append(area)\n",
    "    \n",
    "    if domain_areas:\n",
    "        ax3.hist(domain_areas, bins=15, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "        ax3.set_xlabel('Domain Area (km¬≤)')\n",
    "        ax3.set_ylabel('Number of Domains')\n",
    "        ax3.set_title('Completed Domain Area Distribution')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add statistics\n",
    "        stats_text = f\"Mean: {np.mean(domain_areas):.1f} km¬≤\\nMedian: {np.median(domain_areas):.1f} km¬≤\"\n",
    "        ax3.text(0.98, 0.98, stats_text, transform=ax3.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=10,\n",
    "                ha='right', va='top')\n",
    "    \n",
    "    # Map 4: Processing timeline (if log files available)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(selected_df)\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_shapefiles = sum(1 for d in completed_domains if d['has_shapefile'])\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    \n",
    "    categories = ['Selected', 'Processing\\nStarted', 'Shapefiles\\nGenerated', 'Results\\nComplete']\n",
    "    counts = [total_selected, total_discovered, total_with_shapefiles, total_with_results]\n",
    "    colors = ['lightblue', 'yellow', 'orange', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Large Sample Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample Study - Domain Overview', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'domain_overview_map.png'\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_shapefiles, total_with_results\n",
    "\n",
    "def extract_et_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract ET simulation results from all completed domains\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Extracting ET Results from Completed Domains...\")\n",
    "    \n",
    "    et_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_results': 0,\n",
    "        'domains_with_et': 0,\n",
    "        'failed_extractions': 0\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_results']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        processing_summary['domains_with_results'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   üîÑ Processing {domain_name}...\")\n",
    "            \n",
    "            # Find simulation output files\n",
    "            nc_files = domain['simulation_files']\n",
    "            \n",
    "            # Look for daily or monthly output files\n",
    "            daily_files = [f for f in nc_files if 'day' in f.name.lower()]\n",
    "            monthly_files = [f for f in nc_files if 'month' in f.name.lower()]\n",
    "            \n",
    "            output_file = None\n",
    "            if daily_files:\n",
    "                output_file = daily_files[0]\n",
    "            elif monthly_files:\n",
    "                output_file = monthly_files[0]\n",
    "            elif nc_files:\n",
    "                output_file = nc_files[0]  # Use any available file\n",
    "            \n",
    "            if output_file is None:\n",
    "                print(f\"     ‚ùå No suitable output files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for ET variables\n",
    "            et_vars = [var for var in ds.data_vars \n",
    "                      if any(et_term in var.lower() \n",
    "                            for et_term in ['et', 'evap', 'latent', 'latheat'])]\n",
    "            \n",
    "            if not et_vars:\n",
    "                print(f\"     ‚ö†Ô∏è  No ET variables found in {output_file.name}\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Use the first ET variable found\n",
    "            et_var = et_vars[0]\n",
    "            print(f\"     üìà Using ET variable: {et_var}\")\n",
    "            \n",
    "            # Extract ET data\n",
    "            et_data = ds[et_var]\n",
    "            \n",
    "            # Handle multi-dimensional data (take spatial mean if needed)\n",
    "            if len(et_data.dims) > 1:\n",
    "                spatial_dims = [dim for dim in et_data.dims if dim != 'time']\n",
    "                if spatial_dims:\n",
    "                    et_data = et_data.mean(dim=spatial_dims)\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            et_series = et_data.to_pandas()\n",
    "            \n",
    "            # Handle unit conversion if needed\n",
    "            # Check for negative values (SUMMA convention)\n",
    "            if et_series.median() < 0:\n",
    "                et_series = -et_series\n",
    "            \n",
    "            # Convert units to mm/day if needed\n",
    "            if 'latent' in et_var.lower() or 'latheat' in et_var.lower():\n",
    "                # Assume W/m¬≤ to mm/day conversion\n",
    "                et_series = et_series * 0.0353\n",
    "            elif et_series.max() < 1:  # Assume kg/m¬≤/s\n",
    "                et_series = et_series * 86400  # Convert to mm/day\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "            \n",
    "            if site_row.empty:\n",
    "                print(f\"     ‚ö†Ô∏è  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'site_id': site_row['ID'].iloc[0] if 'ID' in site_row.columns else domain_name,\n",
    "                'latitude': site_row['latitude'].iloc[0],\n",
    "                'longitude': site_row['longitude'].iloc[0],\n",
    "                'climate': site_row['KG'].iloc[0] if 'KG' in site_row.columns else 'Unknown',\n",
    "                'landcover': site_row['Dominant_LC'].iloc[0] if 'Dominant_LC' in site_row.columns else 'Unknown',\n",
    "                'et_timeseries': et_series,\n",
    "                'et_mean': et_series.mean(),\n",
    "                'et_std': et_series.std(),\n",
    "                'et_min': et_series.min(),\n",
    "                'et_max': et_series.max(),\n",
    "                'data_period': f\"{et_series.index.min()} to {et_series.index.max()}\",\n",
    "                'data_points': len(et_series),\n",
    "                'et_variable': et_var,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            et_results.append(result)\n",
    "            processing_summary['domains_with_et'] += 1\n",
    "            \n",
    "            print(f\"     ‚úÖ ET extracted: {result['et_mean']:.2f} ¬± {result['et_std']:.2f} mm/day\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\nüìä ET Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with results: {processing_summary['domains_with_results']}\")\n",
    "    print(f\"   Successful ET extractions: {processing_summary['domains_with_et']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    return et_results, processing_summary\n",
    "\n",
    "def load_fluxnet_observations():\n",
    "    \"\"\"\n",
    "    Load FLUXNET observation data for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Loading FLUXNET Observation Data...\")\n",
    "    \n",
    "    fluxnet_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_et': 0,\n",
    "        'total_observations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for processed FLUXNET data in domain directories\n",
    "    for _, site in selected_df.iterrows():\n",
    "        domain_name = site['DOMAIN_NAME']\n",
    "        \n",
    "        # Construct path to processed FLUXNET data\n",
    "        obs_path = CONFLUENCE_DATA_DIR / f\"domain_{domain_name}\" / \"observations\" / \"energy_fluxes\" / \"fluxnet\" / \"processed\" / f\"{domain_name}_fluxnet_processed.csv\"\n",
    "        \n",
    "        if obs_path.exists():\n",
    "            try:\n",
    "                print(f\"   üìä Loading {domain_name}...\")\n",
    "                \n",
    "                obs_df = pd.read_csv(obs_path)\n",
    "                obs_df['timestamp'] = pd.to_datetime(obs_df['timestamp'])\n",
    "                obs_df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                obs_summary['sites_found'] += 1\n",
    "                \n",
    "                # Check for ET data\n",
    "                if 'ET_from_LE_mm_per_day' in obs_df.columns:\n",
    "                    et_obs = obs_df['ET_from_LE_mm_per_day'].dropna()\n",
    "                    \n",
    "                    if len(et_obs) > 0:\n",
    "                        fluxnet_obs[domain_name] = {\n",
    "                            'et_timeseries': et_obs,\n",
    "                            'et_mean': et_obs.mean(),\n",
    "                            'et_std': et_obs.std(),\n",
    "                            'et_min': et_obs.min(),\n",
    "                            'et_max': et_obs.max(),\n",
    "                            'data_points': len(et_obs),\n",
    "                            'data_period': f\"{et_obs.index.min()} to {et_obs.index.max()}\",\n",
    "                            'latitude': site['latitude'],\n",
    "                            'longitude': site['longitude'],\n",
    "                            'climate': site['KG'] if 'KG' in site else 'Unknown',\n",
    "                            'landcover': site['Dominant_LC'] if 'Dominant_LC' in site else 'Unknown'\n",
    "                        }\n",
    "                        \n",
    "                        obs_summary['sites_with_et'] += 1\n",
    "                        obs_summary['total_observations'] += len(et_obs)\n",
    "                        \n",
    "                        print(f\"     ‚úÖ ET obs: {et_obs.mean():.2f} ¬± {et_obs.std():.2f} mm/day ({len(et_obs)} points)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä FLUXNET Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with ET observations: {obs_summary['sites_with_et']}\")\n",
    "    print(f\"   Total ET observations: {obs_summary['total_observations']}\")\n",
    "    \n",
    "    return fluxnet_obs, obs_summary\n",
    "\n",
    "def create_et_comparison_analysis(et_results, fluxnet_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive ET comparison analysis between simulated and observed\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà Creating ET Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in et_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in fluxnet_obs:\n",
    "            # Align time periods\n",
    "            sim_et = sim_result['et_timeseries']\n",
    "            obs_et = fluxnet_obs[domain_name]['et_timeseries']\n",
    "            \n",
    "            # Find common time period\n",
    "            common_start = max(sim_et.index.min(), obs_et.index.min())\n",
    "            common_end = min(sim_et.index.max(), obs_et.index.max())\n",
    "            \n",
    "            if common_start < common_end:\n",
    "                # Resample to daily and align\n",
    "                sim_daily = sim_et.resample('D').mean().loc[common_start:common_end]\n",
    "                obs_daily = obs_et.resample('D').mean().loc[common_start:common_end]\n",
    "                \n",
    "                # Remove NaN values\n",
    "                valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                sim_valid = sim_daily[valid_mask]\n",
    "                obs_valid = obs_daily[valid_mask]\n",
    "                \n",
    "                if len(sim_valid) > 10:  # Need minimum data for meaningful comparison\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                    bias = (sim_valid - obs_valid).mean()\n",
    "                    mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                    \n",
    "                    # Correlation\n",
    "                    try:\n",
    "                        correlation = obs_valid.corr(sim_valid)\n",
    "                        if pd.isna(correlation):\n",
    "                            correlation = 0.0\n",
    "                    except:\n",
    "                        correlation = 0.0\n",
    "                    \n",
    "                    # Nash-Sutcliffe Efficiency\n",
    "                    if obs_valid.var() > 0:\n",
    "                        nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                    else:\n",
    "                        nse = np.nan\n",
    "                    \n",
    "                    common_site = {\n",
    "                        'domain_name': domain_name,\n",
    "                        'latitude': sim_result['latitude'],\n",
    "                        'longitude': sim_result['longitude'],\n",
    "                        'climate': sim_result['climate'],\n",
    "                        'landcover': sim_result['landcover'],\n",
    "                        'sim_et': sim_valid,\n",
    "                        'obs_et': obs_valid,\n",
    "                        'sim_mean': sim_valid.mean(),\n",
    "                        'obs_mean': obs_valid.mean(),\n",
    "                        'rmse': rmse,\n",
    "                        'bias': bias,\n",
    "                        'mae': mae,\n",
    "                        'correlation': correlation,\n",
    "                        'nse': nse,\n",
    "                        'n_points': len(sim_valid),\n",
    "                        'common_period': f\"{common_start.date()} to {common_end.date()}\"\n",
    "                    }\n",
    "                    \n",
    "                    common_sites.append(common_site)\n",
    "                    \n",
    "                    print(f\"   ‚úÖ {domain_name}: r={correlation:.3f}, RMSE={rmse:.2f}, Bias={bias:+.2f} ({len(sim_valid)} points)\")\n",
    "    \n",
    "    print(f\"\\nüìä ET Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(\"   ‚ö†Ô∏è  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive comparison visualization\n",
    "    n_sites = len(common_sites)\n",
    "    \n",
    "    # Figure 1: Overview comparison plots\n",
    "    fig1, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Scatter plot of all sites\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    all_obs = np.concatenate([site['obs_et'].values for site in common_sites])\n",
    "    all_sim = np.concatenate([site['sim_et'].values for site in common_sites])\n",
    "    \n",
    "    ax1.scatter(all_obs, all_sim, alpha=0.5, s=10, c='blue')\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(all_obs.min(), all_sim.min())\n",
    "    max_val = max(all_obs.max(), all_sim.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    ax1.set_xlabel('Observed ET (mm/day)')\n",
    "    ax1.set_ylabel('Simulated ET (mm/day)')\n",
    "    ax1.set_title('All Sites: Simulated vs Observed ET')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overall statistics\n",
    "    overall_corr = np.corrcoef(all_obs, all_sim)[0,1]\n",
    "    overall_rmse = np.sqrt(np.mean((all_obs - all_sim)**2))\n",
    "    overall_bias = np.mean(all_sim - all_obs)\n",
    "    \n",
    "    stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.2f}\\nBias = {overall_bias:+.2f}'\n",
    "    ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance metrics by climate\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    climate_stats = {}\n",
    "    for site in common_sites:\n",
    "        climate = site['climate']\n",
    "        if climate not in climate_stats:\n",
    "            climate_stats[climate] = {'correlations': [], 'rmses': [], 'biases': []}\n",
    "        \n",
    "        climate_stats[climate]['correlations'].append(site['correlation'])\n",
    "        climate_stats[climate]['rmses'].append(site['rmse'])\n",
    "        climate_stats[climate]['biases'].append(site['bias'])\n",
    "    \n",
    "    # Plot correlation by climate\n",
    "    climates = list(climate_stats.keys())\n",
    "    corr_means = [np.mean(climate_stats[c]['correlations']) for c in climates]\n",
    "    corr_stds = [np.std(climate_stats[c]['correlations']) for c in climates]\n",
    "    \n",
    "    x_pos = range(len(climates))\n",
    "    ax2.bar(x_pos, corr_means, yerr=corr_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(climates, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Correlation')\n",
    "    ax2.set_title('ET Performance by Climate Type')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Bias distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    all_biases = [site['bias'] for site in common_sites]\n",
    "    ax3.hist(all_biases, bins=15, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "    ax3.set_xlabel('Bias (mm/day)')\n",
    "    ax3.set_ylabel('Number of Sites')\n",
    "    ax3.set_title('Distribution of ET Bias')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # RMSE vs site characteristics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    site_lats = [site['latitude'] for site in common_sites]\n",
    "    site_rmses = [site['rmse'] for site in common_sites]\n",
    "    \n",
    "    ax4.scatter(site_lats, site_rmses, alpha=0.7, s=30, c='green')\n",
    "    ax4.set_xlabel('Latitude')\n",
    "    ax4.set_ylabel('RMSE (mm/day)')\n",
    "    ax4.set_title('ET Performance vs Latitude')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample ET Comparison Analysis', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'et_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ ET comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    # Figure 2: Spatial map with performance metrics\n",
    "    fig2, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Map 1: Correlation map\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites]\n",
    "    lons = [site['longitude'] for site in common_sites]\n",
    "    corrs = [site['correlation'] for site in common_sites]\n",
    "    \n",
    "    scatter1 = ax1.scatter(lons, lats, c=corrs, cmap='RdYlBu', s=60, \n",
    "                          vmin=0, vmax=1, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('ET Model Performance: Correlation')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(-60, 80)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "    cbar1.set_label('Correlation')\n",
    "    \n",
    "    # Map 2: Bias map\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    biases = [site['bias'] for site in common_sites]\n",
    "    max_abs_bias = max(abs(min(biases)), abs(max(biases)))\n",
    "    \n",
    "    scatter2 = ax2.scatter(lons, lats, c=biases, cmap='RdBu_r', s=60,\n",
    "                          vmin=-max_abs_bias, vmax=max_abs_bias, \n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title('ET Model Performance: Bias (Sim - Obs)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-180, 180)\n",
    "    ax2.set_ylim(-60, 80)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "    cbar2.set_label('Bias (mm/day)')\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample ET Performance - Spatial Distribution', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save spatial analysis\n",
    "    spatial_path = experiment_dir / 'plots' / 'et_spatial_performance.png'\n",
    "    plt.savefig(spatial_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ ET spatial performance map saved: {spatial_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\nüîç Step 3.1: Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_domains()\n",
    "\n",
    "# Create domain overview map\n",
    "total_selected, total_discovered, total_with_shapefiles, total_with_results = create_domain_overview_map(completed_domains)\n",
    "\n",
    "print(f\"\\nüìä Step 3.2: ET Results Extraction\")\n",
    "\n",
    "# Extract ET results from simulations\n",
    "et_results, et_processing_summary = extract_et_results_from_domains(completed_domains)\n",
    "\n",
    "# Load FLUXNET observations\n",
    "fluxnet_obs, obs_summary = load_fluxnet_observations()\n",
    "\n",
    "print(f\"\\nüìà Step 3.3: ET Comparison Analysis\")\n",
    "\n",
    "# Create ET comparison analysis\n",
    "if et_results and fluxnet_obs:\n",
    "    common_sites = create_et_comparison_analysis(et_results, fluxnet_obs)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Insufficient data for ET comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final summary report\n",
    "print(f\"\\nüìã Creating Final Large Sample Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'large_sample_final_report.txt'\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"FLUXNET Large Sample Study - Final Analysis Report\\n\")\n",
    "    f.write(\"=\" * 55 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PROCESSING SUMMARY:\\n\")\n",
    "    f.write(f\"  Sites selected for analysis: {total_selected}\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\n\")\n",
    "    f.write(f\"  Shapefiles generated: {total_with_shapefiles}\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\n\")\n",
    "    f.write(f\"  ET extractions successful: {et_processing_summary['domains_with_et']}\\n\")\n",
    "    f.write(f\"  FLUXNET observations available: {obs_summary['sites_with_et']}\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"ET PERFORMANCE SUMMARY:\\n\")\n",
    "        correlations = [site['correlation'] for site in common_sites]\n",
    "        rmses = [site['rmse'] for site in common_sites]\n",
    "        biases = [site['bias'] for site in common_sites]\n",
    "        \n",
    "        f.write(f\"  Mean correlation: {np.mean(correlations):.3f} ¬± {np.std(correlations):.3f}\\n\")\n",
    "        f.write(f\"  Mean RMSE: {np.mean(rmses):.2f} ¬± {np.std(rmses):.2f} mm/day\\n\")\n",
    "        f.write(f\"  Mean bias: {np.mean(biases):+.2f} ¬± {np.std(biases):.2f} mm/day\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING SITES (by correlation):\\n\")\n",
    "        sorted_sites = sorted(common_sites, key=lambda x: x['correlation'], reverse=True)\n",
    "        for i, site in enumerate(sorted_sites[:5]):\n",
    "            f.write(f\"  {i+1}. {site['domain_name']}: r={site['correlation']:.3f}, RMSE={site['rmse']:.2f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Final summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Step 3 Complete: Large Sample Output Analysis\")\n",
    "print(f\"   üìÅ Results saved to: {experiment_dir}\")\n",
    "print(f\"   üó∫Ô∏è  Domain overview: {total_with_results}/{total_selected} sites with results\")\n",
    "print(f\"   üìä ET analysis: {len(common_sites) if common_sites else 0} sites with sim/obs comparison\")\n",
    "print(f\"   üìà Performance: Mean r = {np.mean([s['correlation'] for s in common_sites]):.3f}\" if common_sites else \"   üìà Performance: Awaiting more results\")\n",
    "\n",
    "print(f\"\\n‚úÖ Large Sample FLUXNET Analysis Complete!\")\n",
    "print(f\"   üåç Multi-site comparative hydrology achieved\")\n",
    "print(f\"   üìä Statistical patterns identified across environmental gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0da33e-252c-4b14-9085-9d05a5710bd2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated the scaling capabilies of CONFLUENCE, transitioning from single-domain modeling to large sample comparative hydrology. Using the global FLUXNET network as our example, we explored how CONFLUENCE's workflow efficiency enables systematic analysis across hundreds of sites with diverse environmental conditions.\n",
    "\n",
    "The tutorial progressed through three key phases: comprehensive analysis of the FLUXNET dataset to understand its global coverage and environmental diversity; automated configuration generation and batch processing using the `run_watersheds_fluxnet.py` script to execute CONFLUENCE across multiple sites simultaneously; and multi-site output analysis to extract, compare, and validate evapotranspiration results across environmental gradients.\n",
    "\n",
    "This large sample approach represents a fundamental advancement in hydrological modeling, moving beyond site-specific case studies to enable robust statistical analysis, pattern recognition, and process understanding at unprecedented scales. The combination of CONFLUENCE's standardized workflow with large sample methodologies opens new possibilities for comparative hydrology, allowing researchers to distinguish universal hydrological processes from site-specific effects and evaluate model transferability across diverse climatic and ecological conditions.\n",
    "\n",
    "The systematic processing of FLUXNET sites demonstrates how modern computational infrastructure and standardized modeling workflows can transform our understanding of hydrological processes from local observations to global patterns, advancing both theoretical understanding and practical applications in water resources management and climate science.\n",
    "\n",
    "### Next Focus: Large Sample Experiments - NorSWE\n",
    "**Ready to explore large sample snow simulations?** ‚Üí **[Tutorial 04b: Large Sample Studies - NORSWE](./04b_large_sample_norswe.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
