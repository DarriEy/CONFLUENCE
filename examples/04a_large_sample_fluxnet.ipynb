{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6c1149",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 8: Large Sample Studies (FLUXNET Multi-Site Analysis)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial represents the culmination of our CONFLUENCE series: large sample studies. While our previous tutorials focused on modeling individual domains (from points to continents), large sample studies leverage CONFLUENCE's workflow efficiency to systematically analyze hundreds or thousands of sites. Using the global FLUXNET network as our example, we'll demonstrate how to transform CONFLUENCE from a single-domain modeling platform into a powerful tool for comparative hydrology and large sample analysis.\n",
    "\n",
    "### What are Large Sample Studies?\n",
    "\n",
    "Large sample studies in hydrology involve systematic analysis across many sites, watersheds, or regions to:\n",
    "\n",
    "- **Identify patterns**: Understand how hydrological processes vary across different environments\n",
    "- **Test hypotheses**: Evaluate theoretical concepts across diverse conditions\n",
    "- **Improve models**: Develop better parameterizations based on multi-site evidence\n",
    "- **Quantify uncertainty**: Assess model performance and reliability across different settings\n",
    "- **Enable comparative hydrology**: Compare hydrological responses across climates, landscapes, and scales\n",
    "\n",
    "### The Scientific Revolution of Large Sample Hydrology\n",
    "\n",
    "Large sample studies have revolutionized hydrology by moving beyond single-site case studies:\n",
    "\n",
    "**Traditional Approach**: Intensive study of individual watersheds or sites\n",
    "- Deep understanding of specific locations\n",
    "- Limited generalizability\n",
    "- Difficult to separate site-specific vs. universal processes\n",
    "\n",
    "**Large Sample Approach**: Systematic analysis across many sites\n",
    "- Identifies universal patterns and regional variations\n",
    "- Enables statistical analysis of hydrological controls\n",
    "- Supports development of general theories and models\n",
    "- Quantifies uncertainty across different environments\n",
    "\n",
    "### Why FLUXNET for Large Sample Studies?\n",
    "\n",
    "The FLUXNET network provides an ideal framework for large sample hydrological analysis:\n",
    "\n",
    "**Global Coverage**: \n",
    "- 900+ tower sites across all continents\n",
    "- Diverse ecosystems: forests, grasslands, wetlands, croplands\n",
    "- Multiple climate zones: tropical, temperate, boreal, arid\n",
    "- Elevation range: sea level to high mountains\n",
    "\n",
    "**Standardized Measurements**:\n",
    "- Consistent eddy covariance methodology\n",
    "- Quality-controlled data processing\n",
    "- Standardized temporal resolution\n",
    "- Comparable variables across sites\n",
    "\n",
    "**Scientific Value**:\n",
    "- Energy balance validation for land surface models\n",
    "- Ecosystem-scale process understanding\n",
    "- Climate-vegetation interactions\n",
    "- Model benchmarking across diverse conditions\n",
    "\n",
    "### From Single Sites to Large Samples\n",
    "\n",
    "Our tutorial progression has prepared you for large sample studies:\n",
    "\n",
    "| Tutorial | Scale | Sites | Purpose |\n",
    "|----------|-------|-------|---------|\n",
    "| 1-2 | Point | 1 | Process understanding |\n",
    "| 3-5 | Watershed | 1 | Spatial integration |\n",
    "| 6-7 | Regional/Continental | 1 | Large-scale hydrology |\n",
    "| 8 | Multi-site | 100s | Comparative analysis |\n",
    "\n",
    "### CONFLUENCE's Advantages for Large Sample Studies\n",
    "\n",
    "CONFLUENCE's design makes it particularly well-suited for large sample analysis:\n",
    "\n",
    "1. **Workflow Automation**: Standardized workflow reduces manual effort per site\n",
    "2. **Consistent Methodology**: Same modeling approach across all sites ensures comparability\n",
    "3. **Scalable Configuration**: Template-based configuration enables rapid site setup\n",
    "4. **Reproducible Science**: Complete workflow documentation ensures reproducibility\n",
    "5. **High-Performance Computing**: Parallel execution across multiple sites\n",
    "6. **Standardized Outputs**: Consistent output formats facilitate multi-site analysis\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "Large sample studies with CONFLUENCE involve several key components:\n",
    "\n",
    "**Site Selection**: Choose representative sites across environmental gradients\n",
    "**Configuration Generation**: Automatically create site-specific configurations\n",
    "**Batch Processing**: Run CONFLUENCE across multiple sites efficiently\n",
    "**Results Aggregation**: Collect and standardize outputs from all sites\n",
    "**Comparative Analysis**: Analyze patterns and relationships across sites\n",
    "\n",
    "### Research Questions Addressed\n",
    "\n",
    "Large sample studies enable investigation of questions impossible at single sites:\n",
    "\n",
    "1. **Process Generalization**: Do hydrological processes scale consistently across environments?\n",
    "2. **Climate Controls**: How do different climate variables control hydrological responses?\n",
    "3. **Ecosystem Influences**: How do vegetation types affect water and energy balance?\n",
    "4. **Model Performance**: Where do models perform well vs. poorly, and why?\n",
    "5. **Parameter Transferability**: Can model parameters be transferred between similar sites?\n",
    "\n",
    "### Methodological Considerations\n",
    "\n",
    "Large sample studies require careful methodological choices:\n",
    "\n",
    "**Site Selection Criteria**:\n",
    "- Spatial distribution across environmental gradients\n",
    "- Data quality and availability\n",
    "- Representativeness of broader regions\n",
    "- Temporal coverage consistency\n",
    "\n",
    "**Standardization Approaches**:\n",
    "- Consistent model configuration across sites\n",
    "- Standardized evaluation metrics\n",
    "- Comparable temporal periods\n",
    "- Unified data processing protocols\n",
    "\n",
    "**Analysis Strategies**:\n",
    "- Statistical analysis of multi-site results\n",
    "- Clustering sites by characteristics\n",
    "- Regression analysis of controls\n",
    "- Uncertainty quantification\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates several key large sample capabilities:\n",
    "\n",
    "1. **Multi-Site Configuration**: Automatically generate configurations for hundreds of sites\n",
    "2. **Batch Execution**: Run CONFLUENCE across multiple sites efficiently\n",
    "3. **Results Synthesis**: Aggregate and analyze multi-site model outputs\n",
    "4. **Comparative Visualization**: Create plots showing patterns across sites\n",
    "5. **Statistical Analysis**: Quantify relationships between site characteristics and model performance\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By completing this tutorial, you'll understand how to:\n",
    "\n",
    "1. **Design large sample experiments** with appropriate site selection\n",
    "2. **Automate configuration generation** for hundreds of sites\n",
    "3. **Manage batch processing** of multiple CONFLUENCE runs\n",
    "4. **Aggregate and analyze results** from multi-site experiments\n",
    "5. **Visualize patterns** across environmental gradients\n",
    "6. **Apply statistical methods** to understand hydrological controls\n",
    "\n",
    "### Tutorial Structure\n",
    "\n",
    "This tutorial demonstrates the complete large sample workflow:\n",
    "\n",
    "1. **Experiment Design**: Define objectives and select FLUXNET sites\n",
    "2. **Configuration Generation**: Create site-specific CONFLUENCE configurations\n",
    "3. **Batch Processing**: Execute CONFLUENCE across multiple sites\n",
    "4. **Results Collection**: Aggregate outputs from all successful runs\n",
    "5. **Comparative Analysis**: Analyze patterns and relationships across sites\n",
    "6. **Visualization**: Create plots showing multi-site results\n",
    "7. **Statistical Summary**: Quantify patterns and uncertainties\n",
    "\n",
    "### Scientific Impact\n",
    "\n",
    "Large sample studies represent the future of hydrological science:\n",
    "\n",
    "- **Robust Conclusions**: Statistical significance from many sites\n",
    "- **Universal Patterns**: Identify processes that transcend individual sites\n",
    "- **Model Improvement**: Better parameterizations based on multi-site evidence\n",
    "- **Uncertainty Quantification**: Understand model reliability across conditions\n",
    "- **Predictive Capability**: Develop models that work in ungauged locations\n",
    "\n",
    "### Tutorial Series Culmination\n",
    "\n",
    "This tutorial represents the culmination of our CONFLUENCE journey:\n",
    "\n",
    "**Foundation**: Point-scale process understanding\n",
    "**Scaling**: Watershed to continental modeling\n",
    "**Application**: Large sample comparative hydrology\n",
    "\n",
    "By mastering large sample studies, you've gained the tools to conduct cutting-edge hydrological research that leverages CONFLUENCE's power across multiple scales and environments. This approach positions you to contribute to the next generation of hydrological science, where systematic multi-site analysis drives theoretical advances and practical applications.\n",
    "\n",
    "The combination of CONFLUENCE's workflow efficiency with large sample methodologies opens new possibilities for understanding how hydrological processes vary across Earth's diverse environments - from individual flux towers to global patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573758bb-ff23-4672-83a5-fc5ed9912307",
   "metadata": {},
   "source": [
    "## Step 1: Large Sample Template Configuration and Experimental Design\n",
    "This tutorial represents the ultimate evolution in our CONFLUENCE series: large sample studies. Rather than scaling to larger spatial domains, we now leverage CONFLUENCE's workflow efficiency to systematically analyze hundreds of sites across global environmental gradients. Using the FLUXNET network, we demonstrate how to transform CONFLUENCE from a single-domain modeling platform into a powerful engine for comparative hydrology and statistical analysis across diverse ecosystems.\n",
    "\n",
    "### Modeling Evolution: Single Domain → Large Sample Studies\n",
    "\n",
    "- **Analytical Paradigm**: Individual case studies → Statistical analysis across hundreds of sites\n",
    "- **Scientific Approach**: Site-specific insights → Universal patterns and regional variations\n",
    "- **Computational Strategy**: Single domain optimization → Systematic multi-site processing\n",
    "- **Research Questions**: Local process understanding → Global comparative hydrology\n",
    "- **Statistical Power**: Single-site conclusions → Robust multi-site evidence\n",
    "\n",
    "### The Large Sample Revolution in Hydrology\n",
    "\n",
    "Large sample studies have fundamentally transformed hydrological science:\n",
    "\n",
    "**Traditional Hydrology**: Intensive study of individual watersheds\n",
    "- Deep site-specific understanding\n",
    "- Limited generalizability across environments\n",
    "- Difficulty separating universal vs. local processes\n",
    "- Case study approach with limited statistical power\n",
    "\n",
    "**Large Sample Hydrology**: Systematic analysis across environmental gradients\n",
    "- Identifies universal patterns and regional exceptions\n",
    "- Enables statistical hypothesis testing across diverse conditions\n",
    "- Supports development of generalizable theories and models\n",
    "- Quantifies uncertainty and model reliability across environments\n",
    "\n",
    "### FLUXNET: The Ultimate Large Sample Framework\n",
    "\n",
    "The global FLUXNET network provides ideal infrastructure for large sample hydrological analysis:\n",
    "\n",
    "**Global Environmental Coverage**:\n",
    "- **900+ sites** across all continents and climate zones\n",
    "- **Ecosystem diversity**: Forests, grasslands, wetlands, croplands, arctic tundra\n",
    "- **Climate gradients**: Tropical to polar, arid to humid, coastal to continental\n",
    "- **Topographic range**: Sea level to high mountain ecosystems\n",
    "\n",
    "**Methodological Advantages**:\n",
    "- **Standardized measurements**: Consistent eddy covariance methodology globally\n",
    "- **Quality control**: Systematic data processing and validation protocols\n",
    "- **Temporal consistency**: Multi-year records enabling robust statistical analysis\n",
    "- **Process validation**: Energy balance closure for land surface model evaluation\n",
    "\n",
    "The same CONFLUENCE framework now scales to handle systematic multi-site analysis while maintaining the workflow consistency and scientific rigor established throughout our tutorial series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: LARGE SAMPLE TEMPLATE CONFIGURATION AND EXPERIMENTAL DESIGN\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=== CONFLUENCE Tutorial 04a: Large Sample Studies ===\")\n",
    "print(\"Ultimate scaling: Single domains to systematic multi-site comparative hydrology\")\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE EXPERIMENTAL DESIGN CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🔬 Large Sample Experimental Design Configuration...\")\n",
    "\n",
    "# Define the large sample experiment parameters\n",
    "large_sample_config = {\n",
    "    # Experiment identification\n",
    "    'experiment_name': 'fluxnet_large_sample_tutorial',\n",
    "    'experiment_type': 'multi_site_comparative_hydrology',\n",
    "    'analysis_scale': 'global_environmental_gradients',\n",
    "    \n",
    "    # Site selection criteria\n",
    "    'max_sites': 50,  # Manageable number for tutorial demonstration\n",
    "    'site_selection_strategy': 'environmental_diversity',\n",
    "    'min_data_years': 1,  # Minimum years of data required\n",
    "    \n",
    "    # File paths and directories\n",
    "    'template_config': '../CONFLUENCE/0_config_files/config_point_template.yaml',\n",
    "    'config_output_dir': '../CONFLUENCE/0_config_files/fluxnet',\n",
    "    'fluxnet_script': './run_watersheds_fluxnet.py',\n",
    "    'fluxnet_sites_csv': 'fluxnet_transformed.csv',\n",
    "    \n",
    "    # Processing options\n",
    "    'batch_processing': True,\n",
    "    'parallel_execution': True,\n",
    "    'dry_run_mode': False,  # Set to True for testing without job submission\n",
    "    \n",
    "    # Analysis objectives\n",
    "    'primary_variables': ['ET', 'SWE', 'soil_moisture', 'runoff'],\n",
    "    'comparison_metrics': ['correlation', 'rmse', 'bias', 'nse'],\n",
    "    'statistical_analysis': ['regression', 'clustering', 'pca']\n",
    "}\n",
    "\n",
    "print(f\"✅ Experimental design configured\")\n",
    "print(f\"   📊 Experiment: {large_sample_config['experiment_name']}\")\n",
    "print(f\"   🌍 Scale: {large_sample_config['analysis_scale']}\")\n",
    "print(f\"   📈 Strategy: {large_sample_config['site_selection_strategy']}\")\n",
    "print(f\"   🎯 Max sites: {large_sample_config['max_sites']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE EXPERIMENT DIRECTORY STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📁 Creating Large Sample Experiment Directory Structure...\")\n",
    "\n",
    "# Create main experiment directory\n",
    "experiment_dir = Path(f\"./experiments/{large_sample_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for organization\n",
    "subdirs = {\n",
    "    'configs': 'Generated CONFLUENCE configuration files',\n",
    "    'logs': 'Execution logs and monitoring',\n",
    "    'results': 'Aggregated results and analysis outputs',\n",
    "    'plots': 'Visualization outputs',\n",
    "    'reports': 'Summary reports and statistics'\n",
    "}\n",
    "\n",
    "for subdir, description in subdirs.items():\n",
    "    (experiment_dir / subdir).mkdir(exist_ok=True)\n",
    "    print(f\"   📁 {subdir}/: {description}\")\n",
    "\n",
    "# Save experiment configuration\n",
    "config_file = experiment_dir / 'experiment_config.yaml'\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml.dump(large_sample_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"✅ Experiment directory structure created: {experiment_dir}\")\n",
    "print(f\"   📋 Configuration saved: {config_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND ANALYZE FLUXNET SITE DATABASE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🌍 Loading and Analyzing FLUXNET Site Database...\")\n",
    "\n",
    "# Load the FLUXNET sites database\n",
    "try:\n",
    "    fluxnet_df = pd.read_csv(large_sample_config['fluxnet_sites_csv'])\n",
    "    print(f\"✅ Loaded FLUXNET database: {len(fluxnet_df)} sites available\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ FLUXNET database not found: {large_sample_config['fluxnet_sites_csv']}\")\n",
    "    print(f\"   Please ensure the CSV file is in the current directory\")\n",
    "    raise\n",
    "\n",
    "# Display database structure\n",
    "print(f\"\\n📊 FLUXNET Database Structure:\")\n",
    "print(f\"   Total sites available: {len(fluxnet_df)}\")\n",
    "print(f\"   Database columns ({len(fluxnet_df.columns)}):\")\n",
    "for i, col in enumerate(fluxnet_df.columns):\n",
    "    print(f\"     {i+1:2d}. {col}\")\n",
    "\n",
    "# Extract coordinate information for spatial analysis\n",
    "print(f\"\\n🗺️  Extracting Spatial Information...\")\n",
    "try:\n",
    "    # Parse pour point coordinates (lat/lon format)\n",
    "    coords = fluxnet_df['POUR_POINT_COORDS'].str.split('/', expand=True)\n",
    "    fluxnet_df['latitude'] = coords[0].astype(float)\n",
    "    fluxnet_df['longitude'] = coords[1].astype(float)\n",
    "    \n",
    "    print(f\"✅ Coordinate extraction successful\")\n",
    "    print(f\"   Latitude range: {fluxnet_df['latitude'].min():.1f}° to {fluxnet_df['latitude'].max():.1f}°\")\n",
    "    print(f\"   Longitude range: {fluxnet_df['longitude'].min():.1f}° to {fluxnet_df['longitude'].max():.1f}°\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Coordinate extraction failed: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENTAL GRADIENT ANALYSIS FOR SITE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🌿 Environmental Gradient Analysis for Site Selection...\")\n",
    "\n",
    "# Analyze environmental diversity in the database\n",
    "environmental_summary = {}\n",
    "\n",
    "# Climate classification analysis\n",
    "if 'KG' in fluxnet_df.columns:\n",
    "    climate_counts = fluxnet_df['KG'].value_counts()\n",
    "    environmental_summary['climate_types'] = len(climate_counts)\n",
    "    print(f\"   🌡️  Climate diversity: {len(climate_counts)} Köppen-Geiger types\")\n",
    "    print(f\"      Most common: {climate_counts.index[0]} ({climate_counts.iloc[0]} sites)\")\n",
    "\n",
    "# Land cover analysis\n",
    "if 'Dominant_LC' in fluxnet_df.columns:\n",
    "    landcover_counts = fluxnet_df['Dominant_LC'].value_counts()\n",
    "    environmental_summary['landcover_types'] = len(landcover_counts)\n",
    "    print(f\"   🌱 Ecosystem diversity: {len(landcover_counts)} land cover types\")\n",
    "    print(f\"      Most common: {landcover_counts.index[0]} ({landcover_counts.iloc[0]} sites)\")\n",
    "\n",
    "# Area analysis\n",
    "if 'Area_km2' in fluxnet_df.columns:\n",
    "    area_stats = fluxnet_df['Area_km2'].describe()\n",
    "    environmental_summary['area_range'] = (area_stats['min'], area_stats['max'])\n",
    "    print(f\"   📐 Spatial scale range: {area_stats['min']:.2f} to {area_stats['max']:.2f} km²\")\n",
    "\n",
    "# Human footprint analysis\n",
    "if 'HFP' in fluxnet_df.columns:\n",
    "    hfp_stats = fluxnet_df['HFP'].describe()\n",
    "    environmental_summary['hfp_range'] = (hfp_stats['min'], hfp_stats['max'])\n",
    "    print(f\"   🏘️  Human impact range: {hfp_stats['min']:.1f} to {hfp_stats['max']:.1f} (HFP index)\")\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGIC SITE SELECTION FOR LARGE SAMPLE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🎯 Strategic Site Selection for Large Sample Analysis...\")\n",
    "\n",
    "# Implement site selection strategy\n",
    "selection_strategy = large_sample_config['site_selection_strategy']\n",
    "max_sites = large_sample_config['max_sites']\n",
    "\n",
    "print(f\"   Strategy: {selection_strategy}\")\n",
    "print(f\"   Target sites: {max_sites}\")\n",
    "\n",
    "if selection_strategy == 'environmental_diversity':\n",
    "    # Strategy 1: Maximize environmental diversity\n",
    "    selected_sites = []\n",
    "    \n",
    "    # Sample across climate types\n",
    "    if 'KG' in fluxnet_df.columns:\n",
    "        climate_types = fluxnet_df['KG'].unique()\n",
    "        sites_per_climate = max(1, max_sites // len(climate_types))\n",
    "        \n",
    "        print(f\"   🌡️  Sampling strategy: ~{sites_per_climate} sites per climate type\")\n",
    "        \n",
    "        for climate in climate_types:\n",
    "            climate_sites = fluxnet_df[fluxnet_df['KG'] == climate]\n",
    "            \n",
    "            if len(climate_sites) > 0:\n",
    "                # Sample up to sites_per_climate from this climate\n",
    "                n_sample = min(sites_per_climate, len(climate_sites))\n",
    "                sampled = climate_sites.sample(n=n_sample, random_state=42)\n",
    "                selected_sites.extend(sampled.index.tolist())\n",
    "                \n",
    "                if len(selected_sites) >= max_sites:\n",
    "                    break\n",
    "    \n",
    "    # Trim to exact number if over\n",
    "    if len(selected_sites) > max_sites:\n",
    "        selected_sites = selected_sites[:max_sites]\n",
    "    \n",
    "    selected_df = fluxnet_df.loc[selected_sites].copy()\n",
    "\n",
    "elif selection_strategy == 'random_sampling':\n",
    "    # Strategy 2: Random sampling\n",
    "    selected_df = fluxnet_df.sample(n=min(max_sites, len(fluxnet_df)), random_state=42)\n",
    "\n",
    "elif selection_strategy == 'geographic_distribution':\n",
    "    # Strategy 3: Geographic distribution\n",
    "    # This would implement spatial sampling across lat/lon gradients\n",
    "    selected_df = fluxnet_df.sample(n=min(max_sites, len(fluxnet_df)), random_state=42)\n",
    "\n",
    "else:\n",
    "    # Default: use first N sites\n",
    "    selected_df = fluxnet_df.head(max_sites).copy()\n",
    "\n",
    "print(f\"✅ Site selection complete: {len(selected_df)} sites selected\")\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATE TEMPLATE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📋 Validating Template Configuration...\")\n",
    "\n",
    "template_path = Path(large_sample_config['template_config'])\n",
    "\n",
    "if template_path.exists():\n",
    "    print(f\"✅ Template configuration found: {template_path}\")\n",
    "    \n",
    "    # Load and verify template structure\n",
    "    try:\n",
    "        with open(template_path, 'r') as f:\n",
    "            template_config = yaml.safe_load(f)\n",
    "        \n",
    "        # Check key template parameters\n",
    "        required_keys = ['DOMAIN_NAME', 'POUR_POINT_COORDS', 'BOUNDING_BOX_COORDS', \n",
    "                        'HYDROLOGICAL_MODEL', 'EXPERIMENT_TIME_START', 'EXPERIMENT_TIME_END']\n",
    "        \n",
    "        missing_keys = [key for key in required_keys if key not in template_config]\n",
    "        \n",
    "        if not missing_keys:\n",
    "            print(f\"✅ Template validation successful\")\n",
    "            print(f\"   📝 Template contains all required parameters\")\n",
    "        else:\n",
    "            print(f\"⚠️  Template missing required keys: {missing_keys}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Template validation failed: {e}\")\n",
    "else:\n",
    "    print(f\"❌ Template configuration not found: {template_path}\")\n",
    "    print(f\"   Please ensure the template file exists\")\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE VISUALIZATION: SELECTED SITES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📈 Creating Large Sample Site Selection Visualization...\")\n",
    "\n",
    "# Create comprehensive site selection visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Global distribution map (top left)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(fluxnet_df['longitude'], fluxnet_df['latitude'], \n",
    "           c='lightgray', alpha=0.5, s=20, label='Available sites')\n",
    "ax1.scatter(selected_df['longitude'], selected_df['latitude'], \n",
    "           c='red', alpha=0.8, s=40, label='Selected sites')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'Global Site Selection\\n{len(selected_df)} of {len(fluxnet_df)} sites')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_xlim(-180, 180)\n",
    "ax1.set_ylim(-60, 80)\n",
    "\n",
    "# Climate distribution (top right)\n",
    "ax2 = axes[0, 1]\n",
    "if 'KG' in selected_df.columns:\n",
    "    climate_counts = selected_df['KG'].value_counts()\n",
    "    bars = ax2.bar(range(len(climate_counts)), climate_counts.values, \n",
    "                   color='skyblue', alpha=0.7)\n",
    "    ax2.set_xticks(range(len(climate_counts)))\n",
    "    ax2.set_xticklabels(climate_counts.index, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Number of Sites')\n",
    "    ax2.set_title('Climate Type Distribution\\n(Selected Sites)')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, climate_counts.values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom')\n",
    "\n",
    "# Land cover distribution (bottom left)\n",
    "ax3 = axes[1, 0]\n",
    "if 'Dominant_LC' in selected_df.columns:\n",
    "    lc_counts = selected_df['Dominant_LC'].value_counts()\n",
    "    bars = ax3.bar(range(len(lc_counts)), lc_counts.values, \n",
    "                   color='lightgreen', alpha=0.7)\n",
    "    ax3.set_xticks(range(len(lc_counts)))\n",
    "    ax3.set_xticklabels(lc_counts.index, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Number of Sites')\n",
    "    ax3.set_title('Land Cover Distribution\\n(Selected Sites)')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, lc_counts.values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom')\n",
    "\n",
    "# Area distribution (bottom right)\n",
    "ax4 = axes[1, 1]\n",
    "if 'Area_km2' in selected_df.columns:\n",
    "    ax4.hist(selected_df['Area_km2'], bins=10, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Area (km²)')\n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Site Area Distribution\\n(Selected Sites)')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add statistics\n",
    "    area_stats = selected_df['Area_km2'].describe()\n",
    "    stats_text = f\"Mean: {area_stats['mean']:.1f} km²\\nMedian: {area_stats['50%']:.1f} km²\"\n",
    "    ax4.text(0.98, 0.98, stats_text, transform=ax4.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8), fontsize=9,\n",
    "            ha='right', va='top')\n",
    "\n",
    "plt.suptitle(f'Large Sample Site Selection: {large_sample_config[\"experiment_name\"]}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save visualization\n",
    "selection_plot_path = experiment_dir / 'plots' / 'site_selection_overview.png'\n",
    "selection_plot_path.parent.mkdir(exist_ok=True)\n",
    "plt.savefig(selection_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Site selection visualization saved: {selection_plot_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE SELECTED SITES FOR PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n💾 Saving Selected Sites for Large Sample Processing...\")\n",
    "\n",
    "# Save selected sites to CSV\n",
    "selected_sites_csv = experiment_dir / 'selected_fluxnet_sites.csv'\n",
    "selected_df.to_csv(selected_sites_csv, index=False)\n",
    "\n",
    "print(f\"✅ Selected sites saved: {selected_sites_csv}\")\n",
    "print(f\"   📊 Sites ready for processing: {len(selected_df)}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = experiment_dir / 'reports' / 'site_selection_summary.txt'\n",
    "summary_report.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(summary_report, 'w') as f:\n",
    "    f.write(\"FLUXNET Large Sample Study - Site Selection Summary\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Experiment: {large_sample_config['experiment_name']}\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Selection Strategy: {large_sample_config['site_selection_strategy']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Site Selection Results:\\n\")\n",
    "    f.write(f\"  Available sites: {len(fluxnet_df)}\\n\")\n",
    "    f.write(f\"  Selected sites: {len(selected_df)}\\n\")\n",
    "    f.write(f\"  Selection ratio: {len(selected_df)/len(fluxnet_df)*100:.1f}%\\n\\n\")\n",
    "    \n",
    "    if 'KG' in selected_df.columns:\n",
    "        f.write(f\"Climate Type Distribution:\\n\")\n",
    "        for climate, count in selected_df['KG'].value_counts().items():\n",
    "            f.write(f\"  {climate}: {count} sites\\n\")\n",
    "    \n",
    "    f.write(f\"\\nGeographic Coverage:\\n\")\n",
    "    f.write(f\"  Latitude range: {selected_df['latitude'].min():.1f}° to {selected_df['latitude'].max():.1f}°\\n\")\n",
    "    f.write(f\"  Longitude range: {selected_df['longitude'].min():.1f}° to {selected_df['longitude'].max():.1f}°\\n\")\n",
    "\n",
    "print(f\"✅ Summary report saved: {summary_report}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE STUDY CONFIGURATION SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🎯 Large Sample Study Configuration Summary:\")\n",
    "\n",
    "configuration_summary = [\n",
    "    f\"Experiment design: {large_sample_config['experiment_type']}\",\n",
    "    f\"Analysis scale: {large_sample_config['analysis_scale']}\",\n",
    "    f\"Site selection: {len(selected_df)} sites from {len(fluxnet_df)} available\",\n",
    "    f\"Environmental diversity: {len(selected_df['KG'].unique()) if 'KG' in selected_df.columns else 'Unknown'} climate types\",\n",
    "    f\"Geographic coverage: Global distribution across {selected_df['latitude'].max() - selected_df['latitude'].min():.0f}° latitude\",\n",
    "    f\"Template configuration: Validated and ready for batch processing\"\n",
    "]\n",
    "\n",
    "for summary in configuration_summary:\n",
    "    print(f\"   ✅ {summary}\")\n",
    "\n",
    "print(f\"\\n🔬 Large Sample Scientific Objectives:\")\n",
    "scientific_objectives = [\n",
    "    f\"Comparative hydrology: Systematic analysis across environmental gradients\",\n",
    "    f\"Process generalization: Identify universal vs. site-specific patterns\",\n",
    "    f\"Model evaluation: Statistical assessment of performance across diverse conditions\",\n",
    "    f\"Parameter transferability: Test model parameter consistency across sites\",\n",
    "    f\"Climate sensitivity: Understand hydrological responses to climate diversity\"\n",
    "]\n",
    "\n",
    "for objective in scientific_objectives:\n",
    "    print(f\"   🎓 {objective}\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Large Sample Batch Processing:\")\n",
    "next_steps = [\n",
    "    f\"Template configuration: Validated for multi-site deployment\",\n",
    "    f\"Site selection: {len(selected_df)} diverse sites prepared for analysis\",\n",
    "    f\"Batch processing: Ready for systematic CONFLUENCE execution\",\n",
    "    f\"Output analysis: Framework prepared for multi-site result aggregation\",\n",
    "    f\"Statistical analysis: Tools ready for comparative hydrology insights\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ✅ {step}\")\n",
    "\n",
    "print(f\"\\n✅ Section 1 Complete: Large sample experiment designed and configured\")\n",
    "print(f\"   🌍 Next: Execute systematic multi-site CONFLUENCE processing\")\n",
    "print(f\"   📊 Goal: Comparative hydrology across global environmental gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543f4a3-1daf-4bcc-b105-7ae491988144",
   "metadata": {},
   "source": [
    "## Step 2: Large Sample Batch Processing Execution\n",
    "Building on the experimental design and site selection from Step 1, we now execute the large sample processing workflow. This step demonstrates the ultimate scaling of CONFLUENCE from single-domain modeling to systematic multi-site analysis, leveraging computational infrastructure to process hundreds of FLUXNET sites in parallel.\n",
    "\n",
    "The same CONFLUENCE workflow that handled individual tutorials now scales seamlessly to process hundreds of sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcddeb7-9aa0-4c26-a6a3-e50aa79d15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: EXECUTE LARGE SAMPLE FLUXNET PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Step 2: Large Sample FLUXNET Processing Execution ===\")\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_fluxnet_script_from_notebook():\n",
    "    \"\"\"\n",
    "    Execute the run_watersheds_fluxnet.py script from within the notebook\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Executing FLUXNET Large Sample Processing Script...\")\n",
    "    \n",
    "    script_path = \"./run_watersheds_fluxnet.py\"\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"❌ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"   📝 Script location: {script_path}\")\n",
    "    print(f\"   🎯 Target sites: {len(selected_df)} FLUXNET sites\")\n",
    "    print(f\"   ⏰ Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Run the script with automated responses\n",
    "        # Note: This assumes the script will use the CSV file created in Step 1\n",
    "        \n",
    "        # Create a process with input automation\n",
    "        process = subprocess.Popen(\n",
    "            ['python', script_path],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Send 'y' to confirm job submission when prompted\n",
    "        stdout, stderr = process.communicate(input='y\\n')\n",
    "        \n",
    "        # Print the output\n",
    "        if stdout:\n",
    "            print(\"📋 Script Output:\")\n",
    "            for line in stdout.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"⚠️  Script Warnings/Errors:\")\n",
    "            for line in stderr.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"✅ FLUXNET processing script completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Script failed with return code: {process.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running script: {e}\")\n",
    "        return False\n",
    "\n",
    "def monitor_job_progress():\n",
    "    \"\"\"\n",
    "    Monitor the progress of submitted CONFLUENCE jobs\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 Monitoring Job Progress...\")\n",
    "    \n",
    "    try:\n",
    "        # Check job queue status\n",
    "        result = subprocess.run(['squeue', '-u', '$USER'], \n",
    "                              capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            queue_lines = result.stdout.strip().split('\\n')\n",
    "            confluence_jobs = [line for line in queue_lines \n",
    "                             if 'CONFLUENCE' in line or any(site in line for site in selected_df['DOMAIN_NAME'][:5])]\n",
    "            \n",
    "            print(f\"   🔄 Jobs in queue: {len(confluence_jobs)}\")\n",
    "            \n",
    "            if confluence_jobs:\n",
    "                print(\"   📋 Active CONFLUENCE jobs:\")\n",
    "                for job in confluence_jobs[:10]:  # Show first 10\n",
    "                    print(f\"     {job}\")\n",
    "                if len(confluence_jobs) > 10:\n",
    "                    print(f\"     ... and {len(confluence_jobs) - 10} more\")\n",
    "        else:\n",
    "            print(\"   ⚠️  Unable to check job queue status\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Error checking job status: {e}\")\n",
    "\n",
    "# Execute the FLUXNET processing script\n",
    "script_success = run_fluxnet_script_from_notebook()\n",
    "\n",
    "if script_success:\n",
    "    print(f\"\\n✅ Step 2 Complete: Large sample processing initiated\")\n",
    "    \n",
    "    # Monitor initial job status\n",
    "    monitor_job_progress()\n",
    "    \n",
    "    print(f\"\\n📝 Next Steps:\")\n",
    "    print(f\"   1. Jobs will process in parallel on the cluster\")\n",
    "    print(f\"   2. Results will be saved to domain directories\")\n",
    "    print(f\"   3. Step 3 will analyze completed results\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n⚠️  Step 2 Issue: Script execution had problems\")\n",
    "    print(f\"   Proceeding to Step 3 with any existing results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbaeef-85a4-4d8b-a0a9-ee908f9631cd",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Site Output Analysis and ET Validation\n",
    "Having executed large sample processing, we now demonstrate the analytical power that emerges from systematic multi-site CONFLUENCE results. This step showcases comprehensive spatial analysis, statistical comparison, and process validation across diverse environmental gradients—the scientific payoff of large sample methodology.\n",
    "\n",
    "### Analytical Evolution: Case Studies → Comparative Hydrology\n",
    "\n",
    "**Traditional Analysis**: Individual site interpretation and validation\n",
    "- Site-specific model evaluation with limited generalizability\n",
    "- Difficulty distinguishing universal processes from local effects\n",
    "- Manual comparison across disparate studies and methodologies\n",
    "- Limited statistical power for robust pattern identification\n",
    "\n",
    "**Large Sample Analysis**: Systematic multi-site comparative hydrology\n",
    "- **Spatial pattern recognition** across global environmental gradients\n",
    "- **Statistical hypothesis testing** with robust sample sizes\n",
    "- **Process universality assessment** distinguishing general vs. site-specific patterns\n",
    "- **Model transferability evaluation** across diverse conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360883a-8a2d-4eee-bfa7-a1990b1455f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: COMPREHENSIVE OUTPUT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n=== Step 3: Large Sample Output Analysis ===\")\n",
    "\n",
    "def discover_completed_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed FLUXNET domain directories and their outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Discovering Completed FLUXNET Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    data_dir_pattern = str(CONFLUENCE_DATA_DIR / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(data_dir_pattern)\n",
    "    \n",
    "    print(f\"   📁 Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a FLUXNET domain (should match our selected sites)\n",
    "        if any(domain_name in site for site in selected_df['DOMAIN_NAME'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"catchment\" / f\"{domain_name}_HRUs.shp\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'simulation_files': []\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                nc_files = list(simulation_dir.glob(\"**/*.nc\"))\n",
    "                domain_info['simulation_files'] = nc_files\n",
    "                domain_info['has_results'] = len(nc_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   🎯 FLUXNET domains found: {len(completed_domains)}\")\n",
    "    print(f\"   📊 Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   📈 Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\n🗺️  Creating Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    ax1.scatter(selected_df['longitude'], selected_df['latitude'], \n",
    "               c='lightgray', alpha=0.5, s=30, label='Selected sites', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in selected_df\n",
    "        site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            lat = site_row['latitude'].iloc[0]\n",
    "            lon = site_row['longitude'].iloc[0]\n",
    "            \n",
    "            # Color based on completion status\n",
    "            if domain['has_results']:\n",
    "                color = 'green'\n",
    "                label = 'Complete with results'\n",
    "                marker = 's'\n",
    "                size = 50\n",
    "            elif domain['has_shapefile']:\n",
    "                color = 'orange' \n",
    "                label = 'Shapefile only'\n",
    "                marker = '^'\n",
    "                size = 40\n",
    "            else:\n",
    "                color = 'red'\n",
    "                label = 'Processing started'\n",
    "                marker = 'v'\n",
    "                size = 30\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=color, s=size, marker=marker, alpha=0.8,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('FLUXNET Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(-60, 80)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='green', s=50, marker='s', label='Complete with results'),\n",
    "        plt.scatter([], [], c='orange', s=40, marker='^', label='Shapefile generated'),\n",
    "        plt.scatter([], [], c='red', s=30, marker='v', label='Processing started'),\n",
    "        plt.scatter([], [], c='lightgray', s=30, marker='o', label='Selected sites')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    # Map 2: Completion statistics by climate type\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if 'KG' in selected_df.columns:\n",
    "        # Count completion by climate type\n",
    "        climate_completion = {}\n",
    "        \n",
    "        for domain in completed_domains:\n",
    "            domain_name = domain['domain_name']\n",
    "            site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "            \n",
    "            if not site_row.empty:\n",
    "                climate = site_row['KG'].iloc[0]\n",
    "                \n",
    "                if climate not in climate_completion:\n",
    "                    climate_completion[climate] = {'total': 0, 'complete': 0, 'partial': 0}\n",
    "                \n",
    "                climate_completion[climate]['total'] += 1\n",
    "                \n",
    "                if domain['has_results']:\n",
    "                    climate_completion[climate]['complete'] += 1\n",
    "                elif domain['has_shapefile']:\n",
    "                    climate_completion[climate]['partial'] += 1\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        climates = list(climate_completion.keys())\n",
    "        complete_counts = [climate_completion[c]['complete'] for c in climates]\n",
    "        partial_counts = [climate_completion[c]['partial'] for c in climates]\n",
    "        pending_counts = [climate_completion[c]['total'] - \n",
    "                         climate_completion[c]['complete'] - \n",
    "                         climate_completion[c]['partial'] for c in climates]\n",
    "        \n",
    "        x_pos = range(len(climates))\n",
    "        \n",
    "        ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.7)\n",
    "        ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.7)\n",
    "        ax2.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(climates, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Number of Sites')\n",
    "        ax2.set_title('Processing Status by Climate Type')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Domain area distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    domain_areas = []\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty and 'Area_km2' in site_row.columns:\n",
    "            area = site_row['Area_km2'].iloc[0]\n",
    "            domain_areas.append(area)\n",
    "    \n",
    "    if domain_areas:\n",
    "        ax3.hist(domain_areas, bins=15, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "        ax3.set_xlabel('Domain Area (km²)')\n",
    "        ax3.set_ylabel('Number of Domains')\n",
    "        ax3.set_title('Completed Domain Area Distribution')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add statistics\n",
    "        stats_text = f\"Mean: {np.mean(domain_areas):.1f} km²\\nMedian: {np.median(domain_areas):.1f} km²\"\n",
    "        ax3.text(0.98, 0.98, stats_text, transform=ax3.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=10,\n",
    "                ha='right', va='top')\n",
    "    \n",
    "    # Map 4: Processing timeline (if log files available)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(selected_df)\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_shapefiles = sum(1 for d in completed_domains if d['has_shapefile'])\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    \n",
    "    categories = ['Selected', 'Processing\\nStarted', 'Shapefiles\\nGenerated', 'Results\\nComplete']\n",
    "    counts = [total_selected, total_discovered, total_with_shapefiles, total_with_results]\n",
    "    colors = ['lightblue', 'yellow', 'orange', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Large Sample Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample Study - Domain Overview', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'domain_overview_map.png'\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_shapefiles, total_with_results\n",
    "\n",
    "def extract_et_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract ET simulation results from all completed domains\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 Extracting ET Results from Completed Domains...\")\n",
    "    \n",
    "    et_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_results': 0,\n",
    "        'domains_with_et': 0,\n",
    "        'failed_extractions': 0\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_results']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        processing_summary['domains_with_results'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   🔄 Processing {domain_name}...\")\n",
    "            \n",
    "            # Find simulation output files\n",
    "            nc_files = domain['simulation_files']\n",
    "            \n",
    "            # Look for daily or monthly output files\n",
    "            daily_files = [f for f in nc_files if 'day' in f.name.lower()]\n",
    "            monthly_files = [f for f in nc_files if 'month' in f.name.lower()]\n",
    "            \n",
    "            output_file = None\n",
    "            if daily_files:\n",
    "                output_file = daily_files[0]\n",
    "            elif monthly_files:\n",
    "                output_file = monthly_files[0]\n",
    "            elif nc_files:\n",
    "                output_file = nc_files[0]  # Use any available file\n",
    "            \n",
    "            if output_file is None:\n",
    "                print(f\"     ❌ No suitable output files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for ET variables\n",
    "            et_vars = [var for var in ds.data_vars \n",
    "                      if any(et_term in var.lower() \n",
    "                            for et_term in ['et', 'evap', 'latent', 'latheat'])]\n",
    "            \n",
    "            if not et_vars:\n",
    "                print(f\"     ⚠️  No ET variables found in {output_file.name}\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Use the first ET variable found\n",
    "            et_var = et_vars[0]\n",
    "            print(f\"     📈 Using ET variable: {et_var}\")\n",
    "            \n",
    "            # Extract ET data\n",
    "            et_data = ds[et_var]\n",
    "            \n",
    "            # Handle multi-dimensional data (take spatial mean if needed)\n",
    "            if len(et_data.dims) > 1:\n",
    "                spatial_dims = [dim for dim in et_data.dims if dim != 'time']\n",
    "                if spatial_dims:\n",
    "                    et_data = et_data.mean(dim=spatial_dims)\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            et_series = et_data.to_pandas()\n",
    "            \n",
    "            # Handle unit conversion if needed\n",
    "            # Check for negative values (SUMMA convention)\n",
    "            if et_series.median() < 0:\n",
    "                et_series = -et_series\n",
    "            \n",
    "            # Convert units to mm/day if needed\n",
    "            if 'latent' in et_var.lower() or 'latheat' in et_var.lower():\n",
    "                # Assume W/m² to mm/day conversion\n",
    "                et_series = et_series * 0.0353\n",
    "            elif et_series.max() < 1:  # Assume kg/m²/s\n",
    "                et_series = et_series * 86400  # Convert to mm/day\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = selected_df[selected_df['DOMAIN_NAME'] == domain_name]\n",
    "            \n",
    "            if site_row.empty:\n",
    "                print(f\"     ⚠️  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'site_id': site_row['ID'].iloc[0] if 'ID' in site_row.columns else domain_name,\n",
    "                'latitude': site_row['latitude'].iloc[0],\n",
    "                'longitude': site_row['longitude'].iloc[0],\n",
    "                'climate': site_row['KG'].iloc[0] if 'KG' in site_row.columns else 'Unknown',\n",
    "                'landcover': site_row['Dominant_LC'].iloc[0] if 'Dominant_LC' in site_row.columns else 'Unknown',\n",
    "                'et_timeseries': et_series,\n",
    "                'et_mean': et_series.mean(),\n",
    "                'et_std': et_series.std(),\n",
    "                'et_min': et_series.min(),\n",
    "                'et_max': et_series.max(),\n",
    "                'data_period': f\"{et_series.index.min()} to {et_series.index.max()}\",\n",
    "                'data_points': len(et_series),\n",
    "                'et_variable': et_var,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            et_results.append(result)\n",
    "            processing_summary['domains_with_et'] += 1\n",
    "            \n",
    "            print(f\"     ✅ ET extracted: {result['et_mean']:.2f} ± {result['et_std']:.2f} mm/day\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\n📊 ET Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with results: {processing_summary['domains_with_results']}\")\n",
    "    print(f\"   Successful ET extractions: {processing_summary['domains_with_et']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    return et_results, processing_summary\n",
    "\n",
    "def load_fluxnet_observations():\n",
    "    \"\"\"\n",
    "    Load FLUXNET observation data for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n📥 Loading FLUXNET Observation Data...\")\n",
    "    \n",
    "    fluxnet_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_et': 0,\n",
    "        'total_observations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for processed FLUXNET data in domain directories\n",
    "    for _, site in selected_df.iterrows():\n",
    "        domain_name = site['DOMAIN_NAME']\n",
    "        \n",
    "        # Construct path to processed FLUXNET data\n",
    "        obs_path = CONFLUENCE_DATA_DIR / f\"domain_{domain_name}\" / \"observations\" / \"energy_fluxes\" / \"fluxnet\" / \"processed\" / f\"{domain_name}_fluxnet_processed.csv\"\n",
    "        \n",
    "        if obs_path.exists():\n",
    "            try:\n",
    "                print(f\"   📊 Loading {domain_name}...\")\n",
    "                \n",
    "                obs_df = pd.read_csv(obs_path)\n",
    "                obs_df['timestamp'] = pd.to_datetime(obs_df['timestamp'])\n",
    "                obs_df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                obs_summary['sites_found'] += 1\n",
    "                \n",
    "                # Check for ET data\n",
    "                if 'ET_from_LE_mm_per_day' in obs_df.columns:\n",
    "                    et_obs = obs_df['ET_from_LE_mm_per_day'].dropna()\n",
    "                    \n",
    "                    if len(et_obs) > 0:\n",
    "                        fluxnet_obs[domain_name] = {\n",
    "                            'et_timeseries': et_obs,\n",
    "                            'et_mean': et_obs.mean(),\n",
    "                            'et_std': et_obs.std(),\n",
    "                            'et_min': et_obs.min(),\n",
    "                            'et_max': et_obs.max(),\n",
    "                            'data_points': len(et_obs),\n",
    "                            'data_period': f\"{et_obs.index.min()} to {et_obs.index.max()}\",\n",
    "                            'latitude': site['latitude'],\n",
    "                            'longitude': site['longitude'],\n",
    "                            'climate': site['KG'] if 'KG' in site else 'Unknown',\n",
    "                            'landcover': site['Dominant_LC'] if 'Dominant_LC' in site else 'Unknown'\n",
    "                        }\n",
    "                        \n",
    "                        obs_summary['sites_with_et'] += 1\n",
    "                        obs_summary['total_observations'] += len(et_obs)\n",
    "                        \n",
    "                        print(f\"     ✅ ET obs: {et_obs.mean():.2f} ± {et_obs.std():.2f} mm/day ({len(et_obs)} points)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 FLUXNET Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with ET observations: {obs_summary['sites_with_et']}\")\n",
    "    print(f\"   Total ET observations: {obs_summary['total_observations']}\")\n",
    "    \n",
    "    return fluxnet_obs, obs_summary\n",
    "\n",
    "def create_et_comparison_analysis(et_results, fluxnet_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive ET comparison analysis between simulated and observed\n",
    "    \"\"\"\n",
    "    print(f\"\\n📈 Creating ET Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in et_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in fluxnet_obs:\n",
    "            # Align time periods\n",
    "            sim_et = sim_result['et_timeseries']\n",
    "            obs_et = fluxnet_obs[domain_name]['et_timeseries']\n",
    "            \n",
    "            # Find common time period\n",
    "            common_start = max(sim_et.index.min(), obs_et.index.min())\n",
    "            common_end = min(sim_et.index.max(), obs_et.index.max())\n",
    "            \n",
    "            if common_start < common_end:\n",
    "                # Resample to daily and align\n",
    "                sim_daily = sim_et.resample('D').mean().loc[common_start:common_end]\n",
    "                obs_daily = obs_et.resample('D').mean().loc[common_start:common_end]\n",
    "                \n",
    "                # Remove NaN values\n",
    "                valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                sim_valid = sim_daily[valid_mask]\n",
    "                obs_valid = obs_daily[valid_mask]\n",
    "                \n",
    "                if len(sim_valid) > 10:  # Need minimum data for meaningful comparison\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                    bias = (sim_valid - obs_valid).mean()\n",
    "                    mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                    \n",
    "                    # Correlation\n",
    "                    try:\n",
    "                        correlation = obs_valid.corr(sim_valid)\n",
    "                        if pd.isna(correlation):\n",
    "                            correlation = 0.0\n",
    "                    except:\n",
    "                        correlation = 0.0\n",
    "                    \n",
    "                    # Nash-Sutcliffe Efficiency\n",
    "                    if obs_valid.var() > 0:\n",
    "                        nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                    else:\n",
    "                        nse = np.nan\n",
    "                    \n",
    "                    common_site = {\n",
    "                        'domain_name': domain_name,\n",
    "                        'latitude': sim_result['latitude'],\n",
    "                        'longitude': sim_result['longitude'],\n",
    "                        'climate': sim_result['climate'],\n",
    "                        'landcover': sim_result['landcover'],\n",
    "                        'sim_et': sim_valid,\n",
    "                        'obs_et': obs_valid,\n",
    "                        'sim_mean': sim_valid.mean(),\n",
    "                        'obs_mean': obs_valid.mean(),\n",
    "                        'rmse': rmse,\n",
    "                        'bias': bias,\n",
    "                        'mae': mae,\n",
    "                        'correlation': correlation,\n",
    "                        'nse': nse,\n",
    "                        'n_points': len(sim_valid),\n",
    "                        'common_period': f\"{common_start.date()} to {common_end.date()}\"\n",
    "                    }\n",
    "                    \n",
    "                    common_sites.append(common_site)\n",
    "                    \n",
    "                    print(f\"   ✅ {domain_name}: r={correlation:.3f}, RMSE={rmse:.2f}, Bias={bias:+.2f} ({len(sim_valid)} points)\")\n",
    "    \n",
    "    print(f\"\\n📊 ET Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(\"   ⚠️  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive comparison visualization\n",
    "    n_sites = len(common_sites)\n",
    "    \n",
    "    # Figure 1: Overview comparison plots\n",
    "    fig1, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Scatter plot of all sites\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    all_obs = np.concatenate([site['obs_et'].values for site in common_sites])\n",
    "    all_sim = np.concatenate([site['sim_et'].values for site in common_sites])\n",
    "    \n",
    "    ax1.scatter(all_obs, all_sim, alpha=0.5, s=10, c='blue')\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(all_obs.min(), all_sim.min())\n",
    "    max_val = max(all_obs.max(), all_sim.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    ax1.set_xlabel('Observed ET (mm/day)')\n",
    "    ax1.set_ylabel('Simulated ET (mm/day)')\n",
    "    ax1.set_title('All Sites: Simulated vs Observed ET')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overall statistics\n",
    "    overall_corr = np.corrcoef(all_obs, all_sim)[0,1]\n",
    "    overall_rmse = np.sqrt(np.mean((all_obs - all_sim)**2))\n",
    "    overall_bias = np.mean(all_sim - all_obs)\n",
    "    \n",
    "    stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.2f}\\nBias = {overall_bias:+.2f}'\n",
    "    ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance metrics by climate\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    climate_stats = {}\n",
    "    for site in common_sites:\n",
    "        climate = site['climate']\n",
    "        if climate not in climate_stats:\n",
    "            climate_stats[climate] = {'correlations': [], 'rmses': [], 'biases': []}\n",
    "        \n",
    "        climate_stats[climate]['correlations'].append(site['correlation'])\n",
    "        climate_stats[climate]['rmses'].append(site['rmse'])\n",
    "        climate_stats[climate]['biases'].append(site['bias'])\n",
    "    \n",
    "    # Plot correlation by climate\n",
    "    climates = list(climate_stats.keys())\n",
    "    corr_means = [np.mean(climate_stats[c]['correlations']) for c in climates]\n",
    "    corr_stds = [np.std(climate_stats[c]['correlations']) for c in climates]\n",
    "    \n",
    "    x_pos = range(len(climates))\n",
    "    ax2.bar(x_pos, corr_means, yerr=corr_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(climates, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Correlation')\n",
    "    ax2.set_title('ET Performance by Climate Type')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Bias distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    all_biases = [site['bias'] for site in common_sites]\n",
    "    ax3.hist(all_biases, bins=15, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "    ax3.set_xlabel('Bias (mm/day)')\n",
    "    ax3.set_ylabel('Number of Sites')\n",
    "    ax3.set_title('Distribution of ET Bias')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # RMSE vs site characteristics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    site_lats = [site['latitude'] for site in common_sites]\n",
    "    site_rmses = [site['rmse'] for site in common_sites]\n",
    "    \n",
    "    ax4.scatter(site_lats, site_rmses, alpha=0.7, s=30, c='green')\n",
    "    ax4.set_xlabel('Latitude')\n",
    "    ax4.set_ylabel('RMSE (mm/day)')\n",
    "    ax4.set_title('ET Performance vs Latitude')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample ET Comparison Analysis', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'et_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ ET comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    # Figure 2: Spatial map with performance metrics\n",
    "    fig2, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Map 1: Correlation map\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites]\n",
    "    lons = [site['longitude'] for site in common_sites]\n",
    "    corrs = [site['correlation'] for site in common_sites]\n",
    "    \n",
    "    scatter1 = ax1.scatter(lons, lats, c=corrs, cmap='RdYlBu', s=60, \n",
    "                          vmin=0, vmax=1, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('ET Model Performance: Correlation')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(-60, 80)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "    cbar1.set_label('Correlation')\n",
    "    \n",
    "    # Map 2: Bias map\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    biases = [site['bias'] for site in common_sites]\n",
    "    max_abs_bias = max(abs(min(biases)), abs(max(biases)))\n",
    "    \n",
    "    scatter2 = ax2.scatter(lons, lats, c=biases, cmap='RdBu_r', s=60,\n",
    "                          vmin=-max_abs_bias, vmax=max_abs_bias, \n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title('ET Model Performance: Bias (Sim - Obs)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-180, 180)\n",
    "    ax2.set_ylim(-60, 80)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "    cbar2.set_label('Bias (mm/day)')\n",
    "    \n",
    "    plt.suptitle('FLUXNET Large Sample ET Performance - Spatial Distribution', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save spatial analysis\n",
    "    spatial_path = experiment_dir / 'plots' / 'et_spatial_performance.png'\n",
    "    plt.savefig(spatial_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ ET spatial performance map saved: {spatial_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\n🔍 Step 3.1: Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_domains()\n",
    "\n",
    "# Create domain overview map\n",
    "total_selected, total_discovered, total_with_shapefiles, total_with_results = create_domain_overview_map(completed_domains)\n",
    "\n",
    "print(f\"\\n📊 Step 3.2: ET Results Extraction\")\n",
    "\n",
    "# Extract ET results from simulations\n",
    "et_results, et_processing_summary = extract_et_results_from_domains(completed_domains)\n",
    "\n",
    "# Load FLUXNET observations\n",
    "fluxnet_obs, obs_summary = load_fluxnet_observations()\n",
    "\n",
    "print(f\"\\n📈 Step 3.3: ET Comparison Analysis\")\n",
    "\n",
    "# Create ET comparison analysis\n",
    "if et_results and fluxnet_obs:\n",
    "    common_sites = create_et_comparison_analysis(et_results, fluxnet_obs)\n",
    "else:\n",
    "    print(\"   ⚠️  Insufficient data for ET comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final summary report\n",
    "print(f\"\\n📋 Creating Final Large Sample Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'large_sample_final_report.txt'\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"FLUXNET Large Sample Study - Final Analysis Report\\n\")\n",
    "    f.write(\"=\" * 55 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PROCESSING SUMMARY:\\n\")\n",
    "    f.write(f\"  Sites selected for analysis: {total_selected}\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\n\")\n",
    "    f.write(f\"  Shapefiles generated: {total_with_shapefiles}\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\n\")\n",
    "    f.write(f\"  ET extractions successful: {et_processing_summary['domains_with_et']}\\n\")\n",
    "    f.write(f\"  FLUXNET observations available: {obs_summary['sites_with_et']}\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"ET PERFORMANCE SUMMARY:\\n\")\n",
    "        correlations = [site['correlation'] for site in common_sites]\n",
    "        rmses = [site['rmse'] for site in common_sites]\n",
    "        biases = [site['bias'] for site in common_sites]\n",
    "        \n",
    "        f.write(f\"  Mean correlation: {np.mean(correlations):.3f} ± {np.std(correlations):.3f}\\n\")\n",
    "        f.write(f\"  Mean RMSE: {np.mean(rmses):.2f} ± {np.std(rmses):.2f} mm/day\\n\")\n",
    "        f.write(f\"  Mean bias: {np.mean(biases):+.2f} ± {np.std(biases):.2f} mm/day\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING SITES (by correlation):\\n\")\n",
    "        sorted_sites = sorted(common_sites, key=lambda x: x['correlation'], reverse=True)\n",
    "        for i, site in enumerate(sorted_sites[:5]):\n",
    "            f.write(f\"  {i+1}. {site['domain_name']}: r={site['correlation']:.3f}, RMSE={site['rmse']:.2f}\\n\")\n",
    "\n",
    "print(f\"✅ Final summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\n🎉 Step 3 Complete: Large Sample Output Analysis\")\n",
    "print(f\"   📁 Results saved to: {experiment_dir}\")\n",
    "print(f\"   🗺️  Domain overview: {total_with_results}/{total_selected} sites with results\")\n",
    "print(f\"   📊 ET analysis: {len(common_sites) if common_sites else 0} sites with sim/obs comparison\")\n",
    "print(f\"   📈 Performance: Mean r = {np.mean([s['correlation'] for s in common_sites]):.3f}\" if common_sites else \"   📈 Performance: Awaiting more results\")\n",
    "\n",
    "print(f\"\\n✅ Large Sample FLUXNET Analysis Complete!\")\n",
    "print(f\"   🌍 Multi-site comparative hydrology achieved\")\n",
    "print(f\"   📊 Statistical patterns identified across environmental gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0da33e-252c-4b14-9085-9d05a5710bd2",
   "metadata": {},
   "source": [
    "**Ready to explore large sample snow simulations?** → **[Tutorial 04b: Large Sample Studies - NORSWE](./04b_large_sample_norswe.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
