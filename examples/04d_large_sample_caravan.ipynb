{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 11: CARAVAN Large Sample Study (Global Multi-Basin Streamflow Analysis)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates systematic streamflow modeling across multiple watersheds using the CARAVAN dataset (Kratzert et al., 2023). Unlike previous tutorials focused on regional analysis, this study addresses global-scale watershed streamflow prediction across diverse international environmental conditions and hydrological regimes.\n",
    "\n",
    "### CARAVAN Dataset\n",
    "\n",
    "The CARAVAN dataset provides standardized data for over 9,000 watersheds across multiple continents, representing the most comprehensive global collection of catchment-scale hydrological data. The dataset includes harmonized meteorological forcing, quality-controlled daily discharge observations, and comprehensive catchment attributes spanning North America, Europe, Australia, Brazil, and Chile. Watersheds range from small headwater basins to large river systems and encompass the full spectrum of global climate zones while maintaining focus on basins with reliable observational records.\n",
    "\n",
    "### Global Streamflow Modeling Challenges\n",
    "\n",
    "Streamflow represents the integrated watershed response to precipitation, evapotranspiration, snowmelt, groundwater interactions, and routing processes. Global multi-basin analysis presents unique challenges including extreme spatial heterogeneity across continents, diverse climate regimes from arctic to tropical conditions, varying physiographic controls across different geological provinces, multiple temporal dynamics spanning sub-daily to multi-decadal scales, and scale interactions between local processes and regional climate patterns.\n",
    "\n",
    "### Research Objectives\n",
    "\n",
    "This tutorial addresses fundamental questions about universal hydrological principles across global environmental gradients, model performance variations across different climate zones and physiographic regions, parameter transferability between watersheds across continents, streamflow sensitivity to diverse climate forcing patterns, and identification of globally consistent versus regionally specific hydrological behaviors. The analysis employs multiple performance metrics including Nash-Sutcliffe efficiency, Kling-Gupta efficiency, bias assessment, and flow signature analysis.\n",
    "\n",
    "### Methodological Framework\n",
    "\n",
    "The approach involves strategic site selection across global environmental gradients, standardized model configuration adaptable to diverse basin characteristics worldwide, automated batch processing execution across multiple continental regions, and systematic performance evaluation using internationally comparable metrics. Sites are selected to represent global climate diversity from polar to tropical zones, physiographic variation across different geological and topographic settings, multiple watershed scales from headwaters to major river basins, and diverse hydrological regimes including snow-dominated, rain-dominated, and mixed systems.\n",
    "\n",
    "### CONFLUENCE Advantages for Global Analysis\n",
    "\n",
    "CONFLUENCE provides consistent methodology across diverse global watersheds, automated processing capabilities scalable to thousands of basins, systematic quality control adaptable to different data standards, and comprehensive uncertainty assessment across varying environmental conditions. The framework emphasizes process-based modeling with flexible structure adaptable to different global watershed characteristics, standardized output formats enabling cross-regional comparison, and robust performance evaluation suitable for international hydrological studies.\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates global watershed-scale configuration across multiple continents, streamflow validation through comprehensive observed-simulated comparisons, performance analysis across diverse climate zones and physiographic regions, identification of universal versus region-specific hydrological patterns, and process diagnostics revealing global controls on watershed function. Results contribute to improved understanding of global hydrological controls, enhanced model development for international applications, and applications in global water resources assessment and climate impact evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Global Multi-Basin Streamflow Experimental Design and Site Selection\n",
    "Transitioning from regional CAMELS-SPAT analysis to comprehensive global streamflow hydrology simulations, \n",
    "this step establishes the foundation for large sample hydrological modeling using the comprehensive CARAVAN dataset. \n",
    "We demonstrate how CONFLUENCE's workflow efficiency enables systematic streamflow evaluation across the full \n",
    "spectrum of global hydroclimate conditions spanning multiple continents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "# Set up plotting style for global watershed visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "%matplotlib inline\n",
    "confluence_path = Path('../').resolve()\n",
    "\n",
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/anvil/scratch/x-deythorsson/CONFLUENCE_data')  # Update this path\n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# =============================================================================\n",
    "# CARAVAN GLOBAL TEMPLATE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Load streamflow configuration template or create from base template\n",
    "streamflow_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "with open(streamflow_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update for CARAVAN tutorial-specific settings\n",
    "config_updates = {\n",
    "    'CONFLUENCE_CODE_DIR': str(CONFLUENCE_CODE_DIR),\n",
    "    'CONFLUENCE_DATA_DIR': str(CONFLUENCE_DATA_DIR),\n",
    "    'DOMAIN_NAME': 'caravan_template',\n",
    "    'EXPERIMENT_ID': 'run_1',\n",
    "    'EXPERIMENT_TIME_START': '2000-01-01 01:00',\n",
    "    'EXPERIMENT_TIME_END': '2020-12-31 23:00',  # 20-year period for global streamflow analysis\n",
    "}\n",
    "\n",
    "config_dict.update(config_updates)\n",
    "\n",
    "# Save CARAVAN configuration template\n",
    "caravan_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_caravan_template.yaml'\n",
    "with open(caravan_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"CARAVAN global template configuration saved: {caravan_config_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND EXAMINE CARAVAN GLOBAL WATERSHED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nLoading CARAVAN Global Watershed Database...\")\n",
    "\n",
    "# Load the CARAVAN watersheds database\n",
    "try:\n",
    "    caravan_df = pd.read_csv('caravan-global-metadata.csv')\n",
    "    print(f\"Successfully loaded CARAVAN database: {len(caravan_df)} watersheds available\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"CARAVAN database not found, creating demonstration dataset...\")\n",
    "    \n",
    "    # Create comprehensive demonstration CARAVAN dataset for tutorial\n",
    "    np.random.seed(42)\n",
    "    n_watersheds = 200\n",
    "    \n",
    "    # Generate realistic global watershed locations across continents\n",
    "    # Focus on major hydrological regions with good streamflow data\n",
    "    regions = [\n",
    "        # North America\n",
    "        {'name': 'NA_Pacific_Northwest', 'lat_range': (42, 55), 'lon_range': (-135, -117), 'n': 25, 'continent': 'North America'},\n",
    "        {'name': 'NA_Rocky_Mountains', 'lat_range': (37, 50), 'lon_range': (-115, -105), 'n': 30, 'continent': 'North America'},\n",
    "        {'name': 'NA_Great_Lakes', 'lat_range': (42, 49), 'lon_range': (-95, -75), 'n': 20, 'continent': 'North America'},\n",
    "        {'name': 'NA_Southeastern', 'lat_range': (25, 40), 'lon_range': (-95, -75), 'n': 25, 'continent': 'North America'},\n",
    "        \n",
    "        # Europe\n",
    "        {'name': 'EU_Scandinavia', 'lat_range': (55, 70), 'lon_range': (5, 30), 'n': 20, 'continent': 'Europe'},\n",
    "        {'name': 'EU_Central_Europe', 'lat_range': (45, 55), 'lon_range': (5, 25), 'n': 25, 'continent': 'Europe'},\n",
    "        {'name': 'EU_Mediterranean', 'lat_range': (35, 45), 'lon_range': (-5, 25), 'n': 15, 'continent': 'Europe'},\n",
    "        \n",
    "        # Australia\n",
    "        {'name': 'AU_Eastern', 'lat_range': (-40, -25), 'lon_range': (140, 155), 'n': 20, 'continent': 'Australia'},\n",
    "        {'name': 'AU_Southeastern', 'lat_range': (-40, -30), 'lon_range': (130, 150), 'n': 15, 'continent': 'Australia'},\n",
    "        \n",
    "        # South America\n",
    "        {'name': 'SA_Brazil_Atlantic', 'lat_range': (-25, -5), 'lon_range': (-50, -35), 'n': 10, 'continent': 'South America'},\n",
    "        {'name': 'SA_Chile_Central', 'lat_range': (-40, -30), 'lon_range': (-75, -70), 'n': 10, 'continent': 'South America'}\n",
    "    ]\n",
    "    \n",
    "    watersheds_data = []\n",
    "    watershed_id = 1\n",
    "    \n",
    "    for region in regions:\n",
    "        for i in range(region['n']):\n",
    "            lat = np.random.uniform(region['lat_range'][0], region['lat_range'][1])\n",
    "            lon = np.random.uniform(region['lon_range'][0], region['lon_range'][1])\n",
    "            \n",
    "            # Area based on typical CARAVAN watersheds (log-normal distribution)\n",
    "            area = np.random.lognormal(np.log(500), 1.5)\n",
    "            area = np.clip(area, 10, 50000)  # Clip to reasonable global range\n",
    "            \n",
    "            # Elevation varies by region and continent\n",
    "            if 'Rocky_Mountains' in region['name'] or 'Chile' in region['name']:\n",
    "                elevation = np.random.uniform(1500, 4000)\n",
    "            elif 'Scandinavia' in region['name']:\n",
    "                elevation = np.random.uniform(200, 1500)\n",
    "            elif 'Australia' in region['continent']:\n",
    "                elevation = np.random.uniform(50, 800)\n",
    "            elif region['continent'] == 'Europe':\n",
    "                elevation = np.random.uniform(100, 2000)\n",
    "            else:\n",
    "                elevation = np.random.uniform(50, 2500)\n",
    "            \n",
    "            # Climate characteristics affecting streamflow - vary by continent and latitude\n",
    "            # Temperature based on latitude and continent\n",
    "            if abs(lat) < 20:  # Tropical\n",
    "                mat_temp = np.random.uniform(20, 28)\n",
    "                map_precip = np.random.uniform(1000, 3000)\n",
    "            elif abs(lat) < 40:  # Temperate\n",
    "                mat_temp = np.random.uniform(8, 20)\n",
    "                map_precip = np.random.uniform(400, 2000)\n",
    "            elif abs(lat) < 60:  # Boreal/Cool temperate\n",
    "                mat_temp = np.random.uniform(-5, 15)\n",
    "                map_precip = np.random.uniform(300, 1500)\n",
    "            else:  # Arctic/Subarctic\n",
    "                mat_temp = np.random.uniform(-15, 5)\n",
    "                map_precip = np.random.uniform(200, 1000)\n",
    "            \n",
    "            # Adjust for regional patterns\n",
    "            if region['continent'] == 'Australia':\n",
    "                map_precip *= 0.7  # Generally drier\n",
    "            elif 'Mediterranean' in region['name']:\n",
    "                map_precip = np.random.uniform(300, 800)  # Mediterranean climate\n",
    "            elif 'Pacific_Northwest' in region['name']:\n",
    "                map_precip = np.random.uniform(1200, 3500)  # Very wet\n",
    "            \n",
    "            # Derived characteristics\n",
    "            pet = max(1, (mat_temp + 5) * 365 * 0.5)  # Simple PET estimate\n",
    "            aridity = pet / map_precip if map_precip > 0 else 10\n",
    "            seasonality = np.random.uniform(0.1, 0.9)  # Precipitation seasonality\n",
    "            \n",
    "            # Snow fraction based on temperature and elevation\n",
    "            if mat_temp < -2:\n",
    "                snow_fraction = np.random.uniform(0.6, 0.95)\n",
    "            elif mat_temp < 5 and elevation > 1000:\n",
    "                snow_fraction = np.random.uniform(0.3, 0.8)\n",
    "            elif mat_temp < 10 and elevation > 2000:\n",
    "                snow_fraction = np.random.uniform(0.2, 0.6)\n",
    "            else:\n",
    "                snow_fraction = np.random.uniform(0.0, 0.3)\n",
    "            \n",
    "            # Forest fraction varies by climate and continent\n",
    "            if region['continent'] == 'Australia':\n",
    "                forest_frac = np.random.uniform(0.05, 0.5)\n",
    "            elif 'Mediterranean' in region['name']:\n",
    "                forest_frac = np.random.uniform(0.1, 0.6)\n",
    "            elif mat_temp > 15 and map_precip > 1000:  # Tropical/humid\n",
    "                forest_frac = np.random.uniform(0.6, 0.95)\n",
    "            elif mat_temp > 5 and map_precip > 600:  # Temperate\n",
    "                forest_frac = np.random.uniform(0.3, 0.8)\n",
    "            else:  # Arid/cold\n",
    "                forest_frac = np.random.uniform(0.05, 0.4)\n",
    "            \n",
    "            # Hydro-climatic classification\n",
    "            if aridity > 3:\n",
    "                climate_class = 'Arid'\n",
    "            elif aridity > 1.5:\n",
    "                climate_class = 'Semi-arid'\n",
    "            elif aridity > 0.8:\n",
    "                climate_class = 'Sub-humid'\n",
    "            else:\n",
    "                climate_class = 'Humid'\n",
    "            \n",
    "            # Scale classification based on area\n",
    "            if area < 100:\n",
    "                scale = 'headwater'\n",
    "            elif area < 1000:\n",
    "                scale = 'meso'\n",
    "            elif area < 10000:\n",
    "                scale = 'macro'\n",
    "            else:\n",
    "                scale = 'large'\n",
    "            \n",
    "            # Streamflow characteristics\n",
    "            runoff_coeff = np.random.uniform(0.1, 0.7)\n",
    "            if climate_class in ['Humid', 'Sub-humid']:\n",
    "                runoff_coeff = np.random.uniform(0.3, 0.8)\n",
    "            elif climate_class == 'Semi-arid':\n",
    "                runoff_coeff = np.random.uniform(0.1, 0.4)\n",
    "            else:  # Arid\n",
    "                runoff_coeff = np.random.uniform(0.05, 0.2)\n",
    "            \n",
    "            mean_q = area * map_precip * 0.001 * runoff_coeff / 31.536  # Convert to m³/s\n",
    "            baseflow_index = np.random.uniform(0.1, 0.8)\n",
    "            \n",
    "            # Flow regime classification\n",
    "            if snow_fraction > 0.5:\n",
    "                flow_regime = 'snow_dominated'\n",
    "            elif snow_fraction > 0.2:\n",
    "                flow_regime = 'mixed'\n",
    "            else:\n",
    "                flow_regime = 'rain_dominated'\n",
    "            \n",
    "            # Create watershed entry\n",
    "            watershed = {\n",
    "                'gauge_id': f\"CARAVAN_{region['continent'][:2]}_{watershed_id:05d}\",\n",
    "                'gauge_name': f\"{region['name']}_Basin_{i+1:03d}\",\n",
    "                'country': region['continent'],\n",
    "                'continent': region['continent'],\n",
    "                'gauge_lat': round(lat, 4),\n",
    "                'gauge_lon': round(lon, 4),\n",
    "                'area': round(area, 1),\n",
    "                'elev_mean': round(elevation, 0),\n",
    "                'p_mean': round(map_precip, 0),  # Mean annual precipitation\n",
    "                't_mean': round(mat_temp, 1),    # Mean annual temperature\n",
    "                'pet_mean': round(pet, 0),       # Potential ET\n",
    "                'aridity': round(aridity, 3),\n",
    "                'seasonality_p': round(seasonality, 3),\n",
    "                'frac_snow': round(snow_fraction, 3),\n",
    "                'forest_frac': round(forest_frac, 3),\n",
    "                'q_mean': round(mean_q, 2),\n",
    "                'runoff_ratio': round(runoff_coeff, 3),\n",
    "                'baseflow_index': round(baseflow_index, 3),\n",
    "                'climate_class': climate_class,\n",
    "                'flow_regime': flow_regime,\n",
    "                'scale': scale,\n",
    "                'region': region['name'],\n",
    "                'data_length': np.random.randint(15, 25),  # Years of data\n",
    "                'data_quality': np.random.choice(['excellent', 'good', 'fair'], p=[0.4, 0.5, 0.1])\n",
    "            }\n",
    "            \n",
    "            # Add CONFLUENCE formatting\n",
    "            buffer = 0.1\n",
    "            watershed['BOUNDING_BOX_COORDS'] = f\"{lat + buffer}/{lon - buffer}/{lat - buffer}/{lon + buffer}\"\n",
    "            watershed['POUR_POINT_COORDS'] = f\"{lat}/{lon}\"\n",
    "            watershed['Watershed_Name'] = watershed['gauge_id'].replace(' ', '_')\n",
    "            \n",
    "            watersheds_data.append(watershed)\n",
    "            watershed_id += 1\n",
    "    \n",
    "    caravan_df = pd.DataFrame(watersheds_data)\n",
    "    \n",
    "    # Save demonstration dataset\n",
    "    caravan_df.to_csv('caravan-global-metadata.csv', index=False)\n",
    "    print(f\"Created demonstration CARAVAN dataset: {len(caravan_df)} watersheds\")\n",
    "\n",
    "# Display basic dataset information\n",
    "print(f\"\\nGlobal Dataset Overview:\")\n",
    "print(f\"  Total watersheds: {len(caravan_df)}\")\n",
    "print(f\"  Columns: {len(caravan_df.columns)}\")\n",
    "print(f\"  Column names: {', '.join(caravan_df.columns[:8])}...\")\n",
    "\n",
    "# Extract coordinates for analysis\n",
    "if 'gauge_lat' in caravan_df.columns and 'gauge_lon' in caravan_df.columns:\n",
    "    caravan_df['latitude'] = caravan_df['gauge_lat']\n",
    "    caravan_df['longitude'] = caravan_df['gauge_lon']\n",
    "    caravan_df['drainage_area'] = caravan_df['area']\n",
    "    \n",
    "    print(f\"Coordinate extraction successful\")\n",
    "    print(f\"  Latitude range: {caravan_df['latitude'].min():.1f}° to {caravan_df['latitude'].max():.1f}°\")\n",
    "    print(f\"  Longitude range: {caravan_df['longitude'].min():.1f}° to {caravan_df['longitude'].max():.1f}°\")\n",
    "    print(f\"  Drainage area range: {caravan_df['drainage_area'].min():.0f} to {caravan_df['drainage_area'].max():.0f} km²\")\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL WATERSHED-SPECIFIC DATASET CHARACTERISTICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nAnalyzing Global Watershed Dataset Characteristics...\")\n",
    "\n",
    "# Continental distribution\n",
    "if 'continent' in caravan_df.columns:\n",
    "    continent_counts = caravan_df['continent'].value_counts()\n",
    "    print(f\"  Continental distribution: {len(continent_counts)} continents\")\n",
    "    for continent, count in continent_counts.items():\n",
    "        print(f\"    {continent}: {count} watersheds\")\n",
    "\n",
    "# Area-based watershed scale zones\n",
    "area_zones = [\n",
    "    (0, 100, 'Headwater'),\n",
    "    (100, 1000, 'Meso-scale'),\n",
    "    (1000, 10000, 'Macro-scale'),\n",
    "    (10000, 100000, 'Large-scale')\n",
    "]\n",
    "\n",
    "caravan_df['area_class'] = 'Unknown'\n",
    "for min_area, max_area, zone_name in area_zones:\n",
    "    mask = (caravan_df['drainage_area'] >= min_area) & (caravan_df['drainage_area'] < max_area)\n",
    "    caravan_df.loc[mask, 'area_class'] = zone_name\n",
    "\n",
    "area_counts = caravan_df['area_class'].value_counts()\n",
    "print(f\"  Watershed scales: {len(area_counts)}\")\n",
    "print(f\"    Most common: {area_counts.index[0]} ({area_counts.iloc[0]} watersheds)\")\n",
    "\n",
    "# Climate-based zones using aridity\n",
    "if 'aridity' in caravan_df.columns:\n",
    "    climate_counts = caravan_df['climate_class'].value_counts()\n",
    "    print(f\"  Climate zones: {len(climate_counts)}\")\n",
    "    for climate, count in climate_counts.items():\n",
    "        print(f\"    {climate}: {count} watersheds\")\n",
    "\n",
    "# Flow regime analysis\n",
    "if 'flow_regime' in caravan_df.columns:\n",
    "    regime_counts = caravan_df['flow_regime'].value_counts()\n",
    "    print(f\"  Flow regimes: {len(regime_counts)}\")\n",
    "    for regime, count in regime_counts.items():\n",
    "        print(f\"    {regime}: {count} watersheds\")\n",
    "\n",
    "# Climate characteristics\n",
    "if 'p_mean' in caravan_df.columns:\n",
    "    precip_stats = caravan_df['p_mean'].describe()\n",
    "    print(f\"  Precipitation range: {precip_stats['min']:.0f} to {precip_stats['max']:.0f} mm/yr\")\n",
    "\n",
    "if 't_mean' in caravan_df.columns:\n",
    "    temp_stats = caravan_df['t_mean'].describe()\n",
    "    print(f\"  Temperature range: {temp_stats['min']:.1f} to {temp_stats['max']:.1f} °C\")\n",
    "\n",
    "# Streamflow characteristics\n",
    "if 'q_mean' in caravan_df.columns:\n",
    "    flow_stats = caravan_df['q_mean'].describe()\n",
    "    print(f\"  Mean streamflow range: {flow_stats['min']:.1f} to {flow_stats['max']:.1f} m³/s\")\n",
    "\n",
    "# =============================================================================\n",
    "# CARAVAN GLOBAL DATASET VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nCreating CARAVAN Global Dataset Overview Visualization...\")\n",
    "\n",
    "# Create comprehensive global watershed dataset overview\n",
    "fig, axes = plt.subplots(2, 3, figsize=(24, 16))\n",
    "\n",
    "# 1. Global watershed distribution map\n",
    "ax1 = axes[0, 0]\n",
    "if 'continent' in caravan_df.columns:\n",
    "    # Color by continent\n",
    "    continent_colors = {'North America': 'red', 'Europe': 'blue', 'Australia': 'green', \n",
    "                       'South America': 'orange', 'Asia': 'purple', 'Africa': 'brown'}\n",
    "    \n",
    "    for continent in caravan_df['continent'].unique():\n",
    "        subset = caravan_df[caravan_df['continent'] == continent]\n",
    "        color = continent_colors.get(continent, 'gray')\n",
    "        ax1.scatter(subset['longitude'], subset['latitude'], \n",
    "                   c=color, alpha=0.7, s=30, label=continent, edgecolors='black', linewidth=0.3)\n",
    "else:\n",
    "    scatter = ax1.scatter(caravan_df['longitude'], caravan_df['latitude'], \n",
    "                         c=caravan_df['drainage_area'], cmap='viridis', \n",
    "                         alpha=0.7, s=30, edgecolors='black', linewidth=0.3)\n",
    "\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'CARAVAN Global Watershed Distribution\\\\n({len(caravan_df)} watersheds)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-180, 180)\n",
    "ax1.set_ylim(-60, 80)\n",
    "if 'continent' in caravan_df.columns:\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Continental distribution\n",
    "ax2 = axes[0, 1]\n",
    "if 'continent' in caravan_df.columns:\n",
    "    continent_counts = caravan_df['continent'].value_counts()\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown'][:len(continent_counts)]\n",
    "    \n",
    "    bars = ax2.bar(range(len(continent_counts)), continent_counts.values, \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xticks(range(len(continent_counts)))\n",
    "    ax2.set_xticklabels(continent_counts.index, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Number of Watersheds')\n",
    "    ax2.set_title('Watersheds by Continent')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, continent_counts.values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Climate classification\n",
    "ax3 = axes[0, 2]\n",
    "if 'climate_class' in caravan_df.columns:\n",
    "    climate_counts = caravan_df['climate_class'].value_counts()\n",
    "    colors = ['brown', 'orange', 'lightgreen', 'blue'][:len(climate_counts)]\n",
    "    bars = ax3.bar(range(len(climate_counts)), climate_counts.values, \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xticks(range(len(climate_counts)))\n",
    "    ax3.set_xticklabels(climate_counts.index, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Number of Watersheds')\n",
    "    ax3.set_title('Watersheds by Climate')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, climate_counts.values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Flow regime distribution\n",
    "ax4 = axes[1, 0]\n",
    "if 'flow_regime' in caravan_df.columns:\n",
    "    regime_counts = caravan_df['flow_regime'].value_counts()\n",
    "    colors = ['lightblue', 'lightcoral', 'lightgreen'][:len(regime_counts)]\n",
    "    bars = ax4.bar(range(len(regime_counts)), regime_counts.values,\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xticks(range(len(regime_counts)))\n",
    "    ax4.set_xticklabels([r.replace('_', '-').title() for r in regime_counts.index], rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Number of Watersheds')\n",
    "    ax4.set_title('Watersheds by Flow Regime')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, regime_counts.values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Aridity vs Temperature scatter\n",
    "ax5 = axes[1, 1]\n",
    "if 'aridity' in caravan_df.columns and 't_mean' in caravan_df.columns:\n",
    "    scatter5 = ax5.scatter(caravan_df['t_mean'], caravan_df['aridity'], \n",
    "                          c=caravan_df['drainage_area'], cmap='viridis', \n",
    "                          alpha=0.6, s=40, edgecolors='black', linewidth=0.3)\n",
    "    ax5.set_xlabel('Mean Annual Temperature (°C)')\n",
    "    ax5.set_ylabel('Aridity Index')\n",
    "    ax5.set_title('Climate Space: Temperature vs Aridity')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.set_yscale('log')\n",
    "    \n",
    "    # Add climate zone boundaries\n",
    "    ax5.axhline(y=3, color='red', linestyle='--', alpha=0.5, label='Arid threshold')\n",
    "    ax5.axhline(y=1.5, color='orange', linestyle='--', alpha=0.5, label='Semi-arid threshold')\n",
    "    ax5.legend()\n",
    "\n",
    "# 6. Scale distribution by area\n",
    "ax6 = axes[1, 2]\n",
    "scale_counts = caravan_df['area_class'].value_counts()\n",
    "colors = ['lightcyan', 'lightblue', 'blue', 'darkblue'][:len(scale_counts)]\n",
    "bars = ax6.bar(range(len(scale_counts)), scale_counts.values,\n",
    "               color=colors, alpha=0.7, edgecolor='black')\n",
    "ax6.set_xticks(range(len(scale_counts)))\n",
    "ax6.set_xticklabels(scale_counts.index, rotation=45, ha='right')\n",
    "ax6.set_ylabel('Number of Watersheds')\n",
    "ax6.set_title('Watersheds by Scale')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, scale_counts.values):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('CARAVAN Global Watershed Dataset - Comprehensive Overview', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Step 1 Complete: CARAVAN Global Dataset Analysis and Experimental Design\")\n",
    "print(f\"   🌍 Global coverage: {len(caravan_df)} watersheds across multiple continents\")\n",
    "if 'continent' in caravan_df.columns:\n",
    "    print(f\"   🗺️  Continental representation: {', '.join(caravan_df['continent'].unique())}\")\n",
    "if 'climate_class' in caravan_df.columns:\n",
    "    print(f\"   🌡️  Climate diversity: {', '.join(caravan_df['climate_class'].unique())}\")\n",
    "if 'flow_regime' in caravan_df.columns:\n",
    "    print(f\"   🌊 Flow regimes: {', '.join(caravan_df['flow_regime'].unique())}\")\n",
    "print(f\"   📊 Configuration template created for global streamflow analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Automated CONFLUENCE Configuration and Global Batch Processing\n",
    "\n",
    "Building on the global dataset analysis and configuration from Step 1, this step demonstrates \n",
    "automated large sample processing using the `run_watersheds_caravan.py` script. This script \n",
    "performs key functions for global-scale hydrological modeling:\n",
    "\n",
    "**Global Configuration Generation**: The script reads the CARAVAN global database and automatically \n",
    "creates individual CONFLUENCE configuration files for each watershed across multiple continents. \n",
    "Each configuration is customized with site-specific parameters including domain coordinates, \n",
    "bounding box definitions, continental identifiers, and climate-specific settings, while maintaining \n",
    "consistent model settings across all global basins.\n",
    "\n",
    "**Multi-Continental Batch Job Submission**: The script submits SLURM jobs to execute the complete \n",
    "CONFLUENCE workflow for each basin in parallel across diverse global environments. Each job processes \n",
    "geographic data, prepares meteorological forcing, processes CARAVAN observations, runs the hydrological \n",
    "model, and generates standardized output files suitable for cross-continental comparison.\n",
    "\n",
    "This automated approach scales CONFLUENCE from regional modeling to systematic global analysis \n",
    "across 9000+ watersheds spanning multiple continents and diverse environmental conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GLOBAL WATERSHED SELECTION AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🌍 Step 2.1: Global Watershed Selection for CONFLUENCE Processing\")\n",
    "\n",
    "# Configuration for the global sample experiment\n",
    "streamflow_config = {\n",
    "    'dataset': 'caravan',\n",
    "    'max_watersheds': 10,  # Start with smaller number for demonstration\n",
    "    'dry_run_mode': True,  # Set to False to actually submit jobs\n",
    "    'experiment_name': 'caravan_global_tutorial',\n",
    "    'template_config': str(caravan_config_path),\n",
    "    'config_dir': str(CONFLUENCE_CODE_DIR / '0_config_files' / 'caravan'),\n",
    "    'base_data_path': str(CONFLUENCE_DATA_DIR / 'caravan'),\n",
    "    'script_path': str(CONFLUENCE_CODE_DIR / 'examples' / 'run_watersheds_caravan.py')\n",
    "}\n",
    "\n",
    "# Create experiment directory structure\n",
    "experiment_dir = Path(f\"./experiments/{streamflow_config['experiment_name']}\")\n",
    "(experiment_dir / 'plots').mkdir(parents=True, exist_ok=True)\n",
    "(experiment_dir / 'reports').mkdir(parents=True, exist_ok=True)\n",
    "(experiment_dir / 'configs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(experiment_dir / 'global_experiment_config.yaml', 'w') as f:\n",
    "    yaml.dump(streamflow_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"   📁 Experiment directory: {experiment_dir}\")\n",
    "print(f\"   🌍 Processing scope: {streamflow_config['max_watersheds']} global watersheds\")\n",
    "print(f\"   🗂️  Template config: {streamflow_config['template_config']}\")\n",
    "\n",
    "# Global watershed selection strategy\n",
    "print(f\"\\n🎯 Step 2.2: Strategic Global Watershed Selection\")\n",
    "\n",
    "# Select watersheds to represent global diversity\n",
    "def select_global_representative_watersheds(caravan_df, max_watersheds=10):\n",
    "    \"\"\"\n",
    "    Select watersheds to maximize global environmental diversity\n",
    "    \"\"\"\n",
    "    print(f\"   🔍 Selecting {max_watersheds} globally representative watersheds...\")\n",
    "    \n",
    "    selected_watersheds = []\n",
    "    \n",
    "    # Strategy 1: Ensure continental representation\n",
    "    if 'continent' in caravan_df.columns:\n",
    "        continents = caravan_df['continent'].unique()\n",
    "        watersheds_per_continent = max(1, max_watersheds // len(continents))\n",
    "        \n",
    "        print(f\"   🌍 Continental strategy: {watersheds_per_continent} watersheds per continent\")\n",
    "        \n",
    "        for continent in continents:\n",
    "            continent_data = caravan_df[caravan_df['continent'] == continent]\n",
    "            \n",
    "            if len(continent_data) > 0:\n",
    "                # Select diverse watersheds within continent\n",
    "                if len(continent_data) <= watersheds_per_continent:\n",
    "                    selected = continent_data\n",
    "                else:\n",
    "                    # Diversify by climate and scale\n",
    "                    selected = []\n",
    "                    \n",
    "                    # Climate diversity\n",
    "                    if 'climate_class' in continent_data.columns:\n",
    "                        climate_classes = continent_data['climate_class'].unique()\n",
    "                        per_climate = max(1, watersheds_per_continent // len(climate_classes))\n",
    "                        \n",
    "                        for climate in climate_classes:\n",
    "                            climate_subset = continent_data[continent_data['climate_class'] == climate]\n",
    "                            if len(climate_subset) > 0:\n",
    "                                # Select by different scales within climate\n",
    "                                if 'area_class' in climate_subset.columns:\n",
    "                                    scales = climate_subset['area_class'].unique()\n",
    "                                    for scale in scales[:per_climate]:\n",
    "                                        scale_subset = climate_subset[climate_subset['area_class'] == scale]\n",
    "                                        if len(scale_subset) > 0:\n",
    "                                            selected.append(scale_subset.iloc[0])\n",
    "                                            if len(selected) >= watersheds_per_continent:\n",
    "                                                break\n",
    "                                    if len(selected) >= watersheds_per_continent:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    selected.extend(climate_subset.head(per_climate).to_dict('records'))\n",
    "                            if len(selected) >= watersheds_per_continent:\n",
    "                                break\n",
    "                    else:\n",
    "                        # Random selection if no climate data\n",
    "                        selected = continent_data.sample(n=min(watersheds_per_continent, len(continent_data)))\n",
    "                    \n",
    "                    selected = pd.DataFrame(selected) if isinstance(selected, list) else selected\n",
    "                \n",
    "                selected_watersheds.append(selected)\n",
    "                \n",
    "                print(f\"     {continent}: {len(selected)} watersheds selected\")\n",
    "    \n",
    "    # Combine all selected watersheds\n",
    "    if selected_watersheds:\n",
    "        final_selection = pd.concat(selected_watersheds, ignore_index=True)\n",
    "    else:\n",
    "        # Fallback: random selection\n",
    "        final_selection = caravan_df.sample(n=min(max_watersheds, len(caravan_df)))\n",
    "    \n",
    "    # Ensure we don't exceed max_watersheds\n",
    "    if len(final_selection) > max_watersheds:\n",
    "        final_selection = final_selection.head(max_watersheds)\n",
    "    \n",
    "    return final_selection\n",
    "\n",
    "# Select representative watersheds\n",
    "selected_watersheds = select_global_representative_watersheds(caravan_df, streamflow_config['max_watersheds'])\n",
    "\n",
    "print(f\"\\n📊 Global Selection Summary:\")\n",
    "print(f\"   Total selected: {len(selected_watersheds)} watersheds\")\n",
    "\n",
    "if 'continent' in selected_watersheds.columns:\n",
    "    continent_summary = selected_watersheds['continent'].value_counts()\n",
    "    print(f\"   Continental distribution:\")\n",
    "    for continent, count in continent_summary.items():\n",
    "        print(f\"     {continent}: {count} watersheds\")\n",
    "\n",
    "if 'climate_class' in selected_watersheds.columns:\n",
    "    climate_summary = selected_watersheds['climate_class'].value_counts()\n",
    "    print(f\"   Climate diversity:\")\n",
    "    for climate, count in climate_summary.items():\n",
    "        print(f\"     {climate}: {count} watersheds\")\n",
    "\n",
    "if 'flow_regime' in selected_watersheds.columns:\n",
    "    regime_summary = selected_watersheds['flow_regime'].value_counts()\n",
    "    print(f\"   Flow regime diversity:\")\n",
    "    for regime, count in regime_summary.items():\n",
    "        print(f\"     {regime}: {count} watersheds\")\n",
    "\n",
    "# Add required columns for CONFLUENCE processing\n",
    "if 'gauge_id' in selected_watersheds.columns:\n",
    "    selected_watersheds['ID'] = selected_watersheds['gauge_id']\n",
    "if 'gauge_lat' in selected_watersheds.columns:\n",
    "    selected_watersheds['Lat'] = selected_watersheds['gauge_lat']\n",
    "if 'gauge_lon' in selected_watersheds.columns:\n",
    "    selected_watersheds['Lon'] = selected_watersheds['gauge_lon']\n",
    "if 'area' in selected_watersheds.columns:\n",
    "    selected_watersheds['Area_km2'] = selected_watersheds['area']\n",
    "if 'scale' in selected_watersheds.columns:\n",
    "    selected_watersheds['Scale'] = selected_watersheds['scale']\n",
    "\n",
    "# Save selected watersheds\n",
    "selected_watersheds_file = experiment_dir / 'selected_global_watersheds.csv'\n",
    "selected_watersheds.to_csv(selected_watersheds_file, index=False)\n",
    "print(f\"   💾 Selected watersheds saved: {selected_watersheds_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL PROCESSING VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🗺️  Step 2.3: Global Processing Setup Visualization\")\n",
    "\n",
    "# Create global processing setup map\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Map 1: Global overview with selected watersheds\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Plot all available watersheds\n",
    "ax1.scatter(caravan_df['longitude'], caravan_df['latitude'], \n",
    "           c='lightgray', alpha=0.3, s=20, label='Available watersheds')\n",
    "\n",
    "# Plot selected watersheds with continental colors\n",
    "if 'continent' in selected_watersheds.columns:\n",
    "    continent_colors = {\n",
    "        'North America': 'red', \n",
    "        'Europe': 'blue', \n",
    "        'Australia': 'green', \n",
    "        'South America': 'orange',\n",
    "        'Asia': 'purple',\n",
    "        'Africa': 'brown'\n",
    "    }\n",
    "    \n",
    "    for continent in selected_watersheds['continent'].unique():\n",
    "        subset = selected_watersheds[selected_watersheds['continent'] == continent]\n",
    "        color = continent_colors.get(continent, 'black')\n",
    "        ax1.scatter(subset['longitude'], subset['latitude'], \n",
    "                   c=color, s=100, alpha=0.8, \n",
    "                   edgecolors='black', linewidth=2, \n",
    "                   label=f'Selected: {continent}', marker='*')\n",
    "\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'CARAVAN Global Processing Setup\\\\n{len(selected_watersheds)} Selected Watersheds')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-180, 180)\n",
    "ax1.set_ylim(-60, 80)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Map 2: Selection diversity analysis\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Create diversity comparison\n",
    "categories = []\n",
    "all_counts = []\n",
    "selected_counts = []\n",
    "\n",
    "# Continental diversity\n",
    "if 'continent' in caravan_df.columns:\n",
    "    all_continents = caravan_df['continent'].value_counts()\n",
    "    selected_continents = selected_watersheds['continent'].value_counts()\n",
    "    \n",
    "    for continent in all_continents.index:\n",
    "        categories.append(continent)\n",
    "        all_counts.append(all_continents[continent])\n",
    "        selected_counts.append(selected_continents.get(continent, 0))\n",
    "\n",
    "# Plot diversity comparison\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x_pos - width/2, all_counts, width, \n",
    "               label='Available', alpha=0.6, color='lightblue')\n",
    "bars2 = ax2.bar(x_pos + width/2, selected_counts, width,\n",
    "               label='Selected', alpha=0.8, color='darkblue')\n",
    "\n",
    "ax2.set_xlabel('Continent')\n",
    "ax2.set_ylabel('Number of Watersheds')\n",
    "ax2.set_title('Global Selection Representativeness')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars2, selected_counts):\n",
    "    if count > 0:\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('CARAVAN Global Watershed Selection for CONFLUENCE Processing', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the processing setup map\n",
    "setup_map_path = experiment_dir / 'plots' / 'global_processing_setup.png'\n",
    "plt.savefig(setup_map_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Global processing setup map saved: {setup_map_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# AUTOMATED CARAVAN PROCESSING EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def execute_caravan_global_processing():\n",
    "    \"\"\"\n",
    "    Execute the run_watersheds_caravan.py script for global processing\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Step 2.4: Executing CARAVAN Global Processing Script\")\n",
    "    \n",
    "    script_path = streamflow_config['script_path']\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"❌ Script not found: {script_path}\")\n",
    "        print(f\"   📝 Expected location: {script_path}\")\n",
    "        print(f\"   🔍 Looking for alternative locations...\")\n",
    "        \n",
    "        # Look for the script in common locations\n",
    "        possible_paths = [\n",
    "            CONFLUENCE_CODE_DIR / \"examples\" / \"run_watersheds_caravan.py\",\n",
    "            CONFLUENCE_CODE_DIR / \"scripts\" / \"run_watersheds_caravan.py\", \n",
    "            CONFLUENCE_CODE_DIR / \"run_watersheds_caravan.py\",\n",
    "            Path(\"./run_watersheds_caravan.py\")\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                script_path = str(path)\n",
    "                print(f\"   ✅ Found script at: {script_path}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"   ⚠️  Script not found in expected locations\")\n",
    "            print(f\"   📋 Creating demonstration execution log...\")\n",
    "            return create_demonstration_processing_log()\n",
    "    \n",
    "    print(f\"   📄 Script location: {script_path}\")\n",
    "    print(f\"   🌍 Target watersheds: {len(selected_watersheds)} global basins\")\n",
    "    print(f\"   🕐 Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   🔧 Mode: {'DRY RUN' if streamflow_config['dry_run_mode'] else 'PRODUCTION'}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare command for CARAVAN processing\n",
    "        cmd = [\n",
    "            'python', script_path,\n",
    "            '--dataset', streamflow_config['dataset'],\n",
    "            '--template', streamflow_config['template_config'],\n",
    "            '--config-dir', streamflow_config['config_dir'],\n",
    "            '--max-watersheds', str(streamflow_config['max_watersheds']),\n",
    "            '--watersheds-csv', str(selected_watersheds_file)\n",
    "        ]\n",
    "        \n",
    "        if streamflow_config['dry_run_mode']:\n",
    "            cmd.append('--dry-run')\n",
    "        \n",
    "        print(f\"   💻 Command: {' '.join(cmd)}\")\n",
    "        \n",
    "        # Execute the script\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        # Process results\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ CARAVAN processing script completed successfully\")\n",
    "            \n",
    "            if result.stdout:\n",
    "                print(f\"\\n📋 Script Output:\")\n",
    "                for line in result.stdout.split('\\\\n')[:20]:  # Show first 20 lines\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "                if len(result.stdout.split('\\\\n')) > 20:\n",
    "                    print(f\"   ... (output truncated)\")\n",
    "            \n",
    "            # Save execution log\n",
    "            log_file = experiment_dir / 'processing_execution.log'\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"CARAVAN Global Processing Execution Log\\\\n\")\n",
    "                f.write(f\"{'='*50}\\\\n\")\n",
    "                f.write(f\"Execution time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                f.write(f\"Command: {' '.join(cmd)}\\\\n\")\n",
    "                f.write(f\"Return code: {result.returncode}\\\\n\\\\n\")\n",
    "                f.write(\"STDOUT:\\\\n\")\n",
    "                f.write(result.stdout)\n",
    "                if result.stderr:\n",
    "                    f.write(\"\\\\n\\\\nSTDERR:\\\\n\")\n",
    "                    f.write(result.stderr)\n",
    "            \n",
    "            print(f\"   📁 Execution log saved: {log_file}\")\n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ Script failed with return code: {result.returncode}\")\n",
    "            if result.stderr:\n",
    "                print(f\"⚠️  Error output:\")\n",
    "                for line in result.stderr.split('\\\\n')[:10]:\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"⏰ Script execution timeout (5 minutes)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error executing script: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_demonstration_processing_log():\n",
    "    \"\"\"\n",
    "    Create a demonstration processing log when script is not available\n",
    "    \"\"\"\n",
    "    print(f\"   📋 Creating demonstration processing log...\")\n",
    "    \n",
    "    # Simulate processing results\n",
    "    processing_results = {\n",
    "        'total_selected': len(selected_watersheds),\n",
    "        'configs_generated': len(selected_watersheds),\n",
    "        'jobs_submitted': len(selected_watersheds) if not streamflow_config['dry_run_mode'] else 0,\n",
    "        'estimated_completion': '2-4 hours per watershed',\n",
    "        'expected_outputs': [\n",
    "            'Domain shapefiles',\n",
    "            'Meteorological forcing',\n",
    "            'SUMMA simulation results',\n",
    "            'mizuRoute streamflow outputs',\n",
    "            'Processed observations'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create demonstration log\n",
    "    demo_log = experiment_dir / 'demonstration_processing.log'\n",
    "    with open(demo_log, 'w') as f:\n",
    "        f.write(\"CARAVAN Global Processing - Demonstration Log\\\\n\")\n",
    "        f.write(\"=\"*50 + \"\\\\n\\\\n\")\n",
    "        f.write(f\"Processing mode: {'DRY RUN' if streamflow_config['dry_run_mode'] else 'PRODUCTION'}\\\\n\")\n",
    "        f.write(f\"Total watersheds selected: {processing_results['total_selected']}\\\\n\")\n",
    "        f.write(f\"Configuration files to generate: {processing_results['configs_generated']}\\\\n\")\n",
    "        f.write(f\"SLURM jobs to submit: {processing_results['jobs_submitted']}\\\\n\")\n",
    "        f.write(f\"Estimated processing time: {processing_results['estimated_completion']}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"Expected outputs per watershed:\\\\n\")\n",
    "        for output in processing_results['expected_outputs']:\n",
    "            f.write(f\"  - {output}\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\nProcessing workflow:\\\\n\")\n",
    "        f.write(\"  1. Generate domain-specific CONFLUENCE configurations\\\\n\")\n",
    "        f.write(\"  2. Download and process geographic data\\\\n\")\n",
    "        f.write(\"  3. Prepare meteorological forcing data\\\\n\")\n",
    "        f.write(\"  4. Process CARAVAN streamflow observations\\\\n\")\n",
    "        f.write(\"  5. Execute SUMMA hydrological modeling\\\\n\")\n",
    "        f.write(\"  6. Run mizuRoute streamflow routing\\\\n\")\n",
    "        f.write(\"  7. Generate standardized output files\\\\n\")\n",
    "    \n",
    "    print(f\"   📄 Demonstration log created: {demo_log}\")\n",
    "    \n",
    "    # Display processing summary\n",
    "    print(f\"\\\\n📊 Global Processing Summary:\")\n",
    "    print(f\"   🌍 Watersheds: {processing_results['total_selected']} across multiple continents\")\n",
    "    print(f\"   ⚙️  Configurations: {processing_results['configs_generated']} to be generated\")\n",
    "    print(f\"   🖥️  Jobs: {processing_results['jobs_submitted']} {'(dry run)' if streamflow_config['dry_run_mode'] else 'to submit'}\")\n",
    "    print(f\"   ⏱️  Estimated time: {processing_results['estimated_completion']}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Execute the global processing\n",
    "processing_success = execute_caravan_global_processing()\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESSING STATUS AND MONITORING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\\\n📈 Step 2.5: Global Processing Status and Monitoring\")\n",
    "\n",
    "def create_processing_status_summary():\n",
    "    \"\"\"\n",
    "    Create comprehensive processing status summary\n",
    "    \"\"\"\n",
    "    \n",
    "    status_summary = {\n",
    "        'experiment_name': streamflow_config['experiment_name'],\n",
    "        'processing_mode': 'DRY RUN' if streamflow_config['dry_run_mode'] else 'PRODUCTION',\n",
    "        'total_watersheds': len(selected_watersheds),\n",
    "        'script_executed': processing_success,\n",
    "        'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Continental breakdown\n",
    "    if 'continent' in selected_watersheds.columns:\n",
    "        continental_breakdown = selected_watersheds['continent'].value_counts().to_dict()\n",
    "        status_summary['continental_breakdown'] = continental_breakdown\n",
    "    \n",
    "    # Climate breakdown\n",
    "    if 'climate_class' in selected_watersheds.columns:\n",
    "        climate_breakdown = selected_watersheds['climate_class'].value_counts().to_dict()\n",
    "        status_summary['climate_breakdown'] = climate_breakdown\n",
    "    \n",
    "    # Scale breakdown\n",
    "    if 'area_class' in selected_watersheds.columns:\n",
    "        scale_breakdown = selected_watersheds['area_class'].value_counts().to_dict()\n",
    "        status_summary['scale_breakdown'] = scale_breakdown\n",
    "    \n",
    "    # Expected outputs\n",
    "    status_summary['expected_outputs'] = {\n",
    "        'domain_directories': len(selected_watersheds),\n",
    "        'shapefile_sets': len(selected_watersheds),\n",
    "        'forcing_datasets': len(selected_watersheds),\n",
    "        'simulation_results': len(selected_watersheds),\n",
    "        'streamflow_outputs': len(selected_watersheds),\n",
    "        'observation_files': len(selected_watersheds)\n",
    "    }\n",
    "    \n",
    "    # Save status summary\n",
    "    status_file = experiment_dir / 'processing_status_summary.yaml'\n",
    "    with open(status_file, 'w') as f:\n",
    "        yaml.dump(status_summary, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"   📊 Processing status summary:\")\n",
    "    print(f\"     Experiment: {status_summary['experiment_name']}\")\n",
    "    print(f\"     Mode: {status_summary['processing_mode']}\")\n",
    "    print(f\"     Watersheds: {status_summary['total_watersheds']}\")\n",
    "    print(f\"     Script executed: {status_summary['script_executed']}\")\n",
    "    \n",
    "    if 'continental_breakdown' in status_summary:\n",
    "        print(f\"     Continental distribution:\")\n",
    "        for continent, count in status_summary['continental_breakdown'].items():\n",
    "            print(f\"       {continent}: {count} watersheds\")\n",
    "    \n",
    "    print(f\"   💾 Status summary saved: {status_file}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Create processing status summary\n",
    "processing_status = create_processing_status_summary()\n",
    "\n",
    "print(f\"\\\\n✅ Step 2 Complete: CARAVAN Global Processing Setup and Execution\")\n",
    "print(f\"   🌍 Global scope: {len(selected_watersheds)} watersheds across continents\")\n",
    "print(f\"   ⚙️  Configuration: Template and processing scripts prepared\")\n",
    "print(f\"   🚀 Execution: {'Completed' if processing_success else 'Attempted'}\")\n",
    "print(f\"   📁 Results: All outputs saved to {experiment_dir}\")\n",
    "\n",
    "if streamflow_config['dry_run_mode']:\n",
    "    print(f\"   🔧 Mode: DRY RUN - Switch to production mode to submit actual jobs\")\n",
    "else:\n",
    "    print(f\"   🔧 Mode: PRODUCTION - Jobs submitted for processing\")\n",
    "\n",
    "print(f\"\\\\n🎯 Next: Proceed to Step 3 for global streamflow validation and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Global Multi-Basin Streamflow Validation and Continental Analysis\n",
    "\n",
    "Having executed large sample global streamflow modeling, we now demonstrate the analytical power \n",
    "that emerges from systematic multi-continental streamflow validation using CARAVAN observations. \n",
    "This step showcases comprehensive watershed response evaluation across continents, continental \n",
    "performance assessment, and integrated global process validation—the scientific culmination of \n",
    "our entire CONFLUENCE tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def discover_completed_global_streamflow_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed CARAVAN domain directories and their streamflow outputs across continents\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Discovering Completed CARAVAN Global Streamflow Modeling Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    base_path = Path(streamflow_config['base_data_path'])\n",
    "    domain_pattern = str(base_path / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(domain_pattern)\n",
    "    \n",
    "    print(f\"   📁 Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a CARAVAN domain (should match our selected watersheds)\n",
    "        if any(domain_name.startswith(ws) for ws in selected_watersheds['ID'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"river_basins\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            obs_dir = domain_path / \"observations\" / \"streamflow\" / \"preprocessed\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'has_observations': obs_dir.exists(),\n",
    "                'observation_path': obs_dir if obs_dir.exists() else None,\n",
    "                'simulation_files': [],\n",
    "                'streamflow_obs_file': None\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                # Look for SUMMA outputs\n",
    "                summa_files = list(simulation_dir.glob(\"**/SUMMA/*.nc\"))\n",
    "                # Look for mizuRoute outputs (streamflow routing)\n",
    "                mizuroute_files = list(simulation_dir.glob(\"**/mizuRoute/*.nc\"))\n",
    "                \n",
    "                domain_info['simulation_files'] = summa_files + mizuroute_files\n",
    "                domain_info['has_results'] = len(domain_info['simulation_files']) > 0\n",
    "                domain_info['has_summa'] = len(summa_files) > 0\n",
    "                domain_info['has_routing'] = len(mizuroute_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "                domain_info['has_summa'] = False\n",
    "                domain_info['has_routing'] = False\n",
    "            \n",
    "            # Find observation files\n",
    "            if obs_dir.exists():\n",
    "                streamflow_files = list(obs_dir.glob(\"*streamflow*.csv\"))\n",
    "                if streamflow_files:\n",
    "                    domain_info['streamflow_obs_file'] = streamflow_files[0]\n",
    "            \n",
    "            # Add continental information\n",
    "            watershed_row = None\n",
    "            for _, row in selected_watersheds.iterrows():\n",
    "                if domain_name.startswith(row['ID']):\n",
    "                    watershed_row = row\n",
    "                    break\n",
    "            \n",
    "            if watershed_row is not None:\n",
    "                domain_info['continent'] = watershed_row.get('continent', 'Unknown')\n",
    "                domain_info['climate_class'] = watershed_row.get('climate_class', 'Unknown')\n",
    "                domain_info['flow_regime'] = watershed_row.get('flow_regime', 'Unknown')\n",
    "                domain_info['watershed_scale'] = watershed_row.get('area_class', 'Unknown')\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   🌍 CARAVAN global domains found: {len(completed_domains)}\")\n",
    "    print(f\"   📊 Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   📈 Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    print(f\"   🌊 Domains with routing outputs: {sum(1 for d in completed_domains if d['has_routing'])}\")\n",
    "    print(f\"   📋 Domains with observations: {sum(1 for d in completed_domains if d['has_observations'])}\")\n",
    "    \n",
    "    # Continental breakdown\n",
    "    if completed_domains:\n",
    "        continental_summary = {}\n",
    "        for domain in completed_domains:\n",
    "            continent = domain.get('continent', 'Unknown')\n",
    "            if continent not in continental_summary:\n",
    "                continental_summary[continent] = {'total': 0, 'with_results': 0, 'with_routing': 0}\n",
    "            continental_summary[continent]['total'] += 1\n",
    "            if domain['has_results']:\n",
    "                continental_summary[continent]['with_results'] += 1\n",
    "            if domain['has_routing']:\n",
    "                continental_summary[continent]['with_routing'] += 1\n",
    "        \n",
    "        print(f\"   🗺️  Continental breakdown:\")\n",
    "        for continent, stats in continental_summary.items():\n",
    "            print(f\"     {continent}: {stats['total']} total, {stats['with_results']} with results, {stats['with_routing']} with routing\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_global_streamflow_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all global streamflow domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\n🗺️  Creating Global Streamflow Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for global overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    if len(selected_watersheds) > 0:\n",
    "        ax1.scatter(selected_watersheds['Lon'], selected_watersheds['Lat'], \n",
    "                   c='lightgray', alpha=0.5, s=40, label='Selected watersheds', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    continent_colors = {'North America': 'red', 'Europe': 'blue', 'Australia': 'green', \n",
    "                       'South America': 'orange', 'Asia': 'purple', 'Africa': 'brown'}\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in selected_watersheds\n",
    "        site_row = None\n",
    "        for _, row in selected_watersheds.iterrows():\n",
    "            if domain_name.startswith(row['ID']):\n",
    "                site_row = row\n",
    "                break\n",
    "        \n",
    "        if site_row is not None:\n",
    "            lat = site_row['Lat']\n",
    "            lon = site_row['Lon']\n",
    "            continent = domain.get('continent', 'Unknown')\n",
    "            base_color = continent_colors.get(continent, 'gray')\n",
    "            \n",
    "            # Marker style based on completion status\n",
    "            if domain['has_routing'] and domain['has_observations']:\n",
    "                marker = 's'\n",
    "                size = 120\n",
    "                alpha = 1.0\n",
    "                label = 'Complete with streamflow validation'\n",
    "            elif domain['has_routing']:\n",
    "                marker = '^'\n",
    "                size = 100\n",
    "                alpha = 0.8\n",
    "                label = 'Routing complete'\n",
    "            elif domain['has_results']:\n",
    "                marker = 'D'\n",
    "                size = 80\n",
    "                alpha = 0.7\n",
    "                label = 'Simulation complete'\n",
    "            else:\n",
    "                marker = 'v'\n",
    "                size = 60\n",
    "                alpha = 0.5\n",
    "                label = 'Processing started'\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=base_color, s=size, marker=marker, alpha=alpha,\n",
    "                       edgecolors='black', linewidth=1, label=f'{continent} - {label}')\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('CARAVAN Global Streamflow Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(-60, 80)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = []\n",
    "    for continent, color in continent_colors.items():\n",
    "        if any(d.get('continent') == continent for d in completed_domains):\n",
    "            legend_elements.append(plt.scatter([], [], c=color, s=60, label=continent))\n",
    "    \n",
    "    # Add completion status legend\n",
    "    legend_elements.extend([\n",
    "        plt.scatter([], [], c='gray', s=120, marker='s', label='Complete with validation'),\n",
    "        plt.scatter([], [], c='gray', s=100, marker='^', label='Routing complete'),\n",
    "        plt.scatter([], [], c='gray', s=80, marker='D', label='Simulation complete'),\n",
    "        plt.scatter([], [], c='gray', s=60, marker='v', label='Processing started')\n",
    "    ])\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Map 2: Continental completion statistics\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Create continental completion analysis\n",
    "    continental_completion = {}\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        continent = domain.get('continent', 'Unknown')\n",
    "        \n",
    "        if continent not in continental_completion:\n",
    "            continental_completion[continent] = {'total': 0, 'complete': 0, 'partial': 0, 'pending': 0}\n",
    "        \n",
    "        continental_completion[continent]['total'] += 1\n",
    "        \n",
    "        if domain['has_routing'] and domain['has_observations']:\n",
    "            continental_completion[continent]['complete'] += 1\n",
    "        elif domain['has_results']:\n",
    "            continental_completion[continent]['partial'] += 1\n",
    "        else:\n",
    "            continental_completion[continent]['pending'] += 1\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    if continental_completion:\n",
    "        continents = list(continental_completion.keys())\n",
    "        complete_counts = [continental_completion[c]['complete'] for c in continents]\n",
    "        partial_counts = [continental_completion[c]['partial'] for c in continents]\n",
    "        pending_counts = [continental_completion[c]['pending'] for c in continents]\n",
    "        \n",
    "        x_pos = range(len(continents))\n",
    "        \n",
    "        ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.8)\n",
    "        ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.8)\n",
    "        ax2.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.8)\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(continents, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Number of Watersheds')\n",
    "        ax2.set_title('Processing Status by Continent')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Climate zone vs completion status\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    climate_completion = {}\n",
    "    for domain in completed_domains:\n",
    "        climate = domain.get('climate_class', 'Unknown')\n",
    "        if climate not in climate_completion:\n",
    "            climate_completion[climate] = {'complete': 0, 'partial': 0, 'pending': 0}\n",
    "        \n",
    "        if domain['has_routing'] and domain['has_observations']:\n",
    "            climate_completion[climate]['complete'] += 1\n",
    "        elif domain['has_results']:\n",
    "            climate_completion[climate]['partial'] += 1\n",
    "        else:\n",
    "            climate_completion[climate]['pending'] += 1\n",
    "    \n",
    "    if climate_completion:\n",
    "        climates = list(climate_completion.keys())\n",
    "        complete_counts = [climate_completion[c]['complete'] for c in climates]\n",
    "        partial_counts = [climate_completion[c]['partial'] for c in climates]\n",
    "        pending_counts = [climate_completion[c]['pending'] for c in climates]\n",
    "        \n",
    "        x_pos = range(len(climates))\n",
    "        \n",
    "        ax3.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.8)\n",
    "        ax3.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.8)\n",
    "        ax3.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.8)\n",
    "        \n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(climates, rotation=45, ha='right')\n",
    "        ax3.set_ylabel('Number of Watersheds')\n",
    "        ax3.set_title('Processing Status by Climate Zone')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 4: Global processing summary statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(selected_watersheds) if len(selected_watersheds) > 0 else 0\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    total_with_routing = sum(1 for d in completed_domains if d['has_routing'])\n",
    "    total_with_obs = sum(1 for d in completed_domains if d['has_observations'])\n",
    "    total_complete = sum(1 for d in completed_domains if d['has_routing'] and d['has_observations'])\n",
    "    \n",
    "    categories = ['Selected\\\\nGlobally', 'Processing\\\\nStarted', 'Simulation\\\\nComplete', \n",
    "                 'Routing\\\\nComplete', 'Observations\\\\nAvailable', 'Ready for\\\\nValidation']\n",
    "    counts = [total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete]\n",
    "    colors = ['lightblue', 'yellow', 'blue', 'orange', 'cyan', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Watersheds')\n",
    "    ax4.set_title('Global Streamflow Modeling Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('CARAVAN Global Large Sample Streamflow Study - Domain Overview', \n",
    "                 fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'global_streamflow_domain_overview_map.png'\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Global streamflow domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete\n",
    "\n",
    "def extract_global_streamflow_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract streamflow simulation results from all completed global domains\n",
    "    \"\"\"\n",
    "    print(f\"\\n🌊 Extracting Global Streamflow Results from Completed Domains...\")\n",
    "    \n",
    "    streamflow_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_routing': 0,\n",
    "        'domains_with_streamflow': 0,\n",
    "        'failed_extractions': 0,\n",
    "        'continental_breakdown': {}\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_routing']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        continent = domain.get('continent', 'Unknown')\n",
    "        processing_summary['domains_with_routing'] += 1\n",
    "        \n",
    "        if continent not in processing_summary['continental_breakdown']:\n",
    "            processing_summary['continental_breakdown'][continent] = {'attempted': 0, 'successful': 0}\n",
    "        processing_summary['continental_breakdown'][continent]['attempted'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   🔄 Processing {domain_name} ({continent})...\")\n",
    "            \n",
    "            # Find routing output files (mizuRoute)\n",
    "            mizuroute_files = [f for f in domain['simulation_files'] if 'mizuRoute' in str(f)]\n",
    "            \n",
    "            if not mizuroute_files:\n",
    "                print(f\"     ❌ No mizuRoute files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Use the first mizuRoute file\n",
    "            output_file = mizuroute_files[0]\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for streamflow variables\n",
    "            streamflow_vars = {}\n",
    "            \n",
    "            # Common mizuRoute streamflow variable names\n",
    "            potential_vars = ['IRFroutedRunoff', 'routedRunoff', 'discharge', 'streamflow']\n",
    "            \n",
    "            for var in potential_vars:\n",
    "                if var in ds.data_vars:\n",
    "                    streamflow_vars['discharge'] = var\n",
    "                    break\n",
    "            \n",
    "            if not streamflow_vars:\n",
    "                print(f\"     ⚠️  No streamflow variables found in {output_file.name}\")\n",
    "                available_vars = list(ds.data_vars.keys())\n",
    "                print(f\"     Available variables: {available_vars[:5]}...\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"     🌊 Using streamflow variable: {streamflow_vars['discharge']}\")\n",
    "            \n",
    "            # Extract streamflow data\n",
    "            streamflow_var = streamflow_vars['discharge']\n",
    "            streamflow_data = ds[streamflow_var]\n",
    "            \n",
    "            # Handle multi-dimensional data (time x reaches)\n",
    "            if len(streamflow_data.dims) > 1:\n",
    "                # Find the time dimension\n",
    "                time_dim = 'time'\n",
    "                reach_dims = [dim for dim in streamflow_data.dims if dim != time_dim]\n",
    "                \n",
    "                if reach_dims:\n",
    "                    reach_dim = reach_dims[0]\n",
    "                    # Use the last reach (often the outlet)\n",
    "                    outlet_idx = streamflow_data.sizes[reach_dim] - 1\n",
    "                    streamflow_data = streamflow_data.isel({reach_dim: outlet_idx})\n",
    "                    print(f\"     📍 Using outlet reach (index {outlet_idx})\")\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            streamflow_series = streamflow_data.to_pandas()\n",
    "            \n",
    "            # Handle unit conversion if needed (assume m³/s is correct)\n",
    "            # Remove any negative values (set to 0)\n",
    "            streamflow_series = streamflow_series.clip(lower=0)\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = None\n",
    "            for _, row in selected_watersheds.iterrows():\n",
    "                if domain_name.startswith(row['ID']):\n",
    "                    site_row = row\n",
    "                    break\n",
    "            \n",
    "            if site_row is None:\n",
    "                print(f\"     ⚠️  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate streamflow statistics\n",
    "            streamflow_stats = {\n",
    "                'mean_flow': streamflow_series.mean(),\n",
    "                'max_flow': streamflow_series.max(),\n",
    "                'min_flow': streamflow_series.min(),\n",
    "                'std_flow': streamflow_series.std(),\n",
    "                'flow_variability': streamflow_series.std() / streamflow_series.mean() if streamflow_series.mean() > 0 else np.nan\n",
    "            }\n",
    "            \n",
    "            # Calculate flow percentiles\n",
    "            percentiles = [5, 25, 50, 75, 95]\n",
    "            for p in percentiles:\n",
    "                streamflow_stats[f'q{p}'] = streamflow_series.quantile(p/100)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'watershed_id': site_row['ID'],\n",
    "                'latitude': site_row['Lat'],\n",
    "                'longitude': site_row['Lon'],\n",
    "                'area_km2': site_row.get('Area_km2', np.nan),\n",
    "                'scale': site_row.get('Scale', 'unknown'),\n",
    "                'continent': continent,\n",
    "                'climate_class': domain.get('climate_class', 'Unknown'),\n",
    "                'flow_regime': domain.get('flow_regime', 'Unknown'),\n",
    "                'streamflow_timeseries': streamflow_series,\n",
    "                'data_period': f\"{streamflow_series.index.min()} to {streamflow_series.index.max()}\",\n",
    "                'data_points': len(streamflow_series),\n",
    "                'streamflow_variable': streamflow_var,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            # Add statistics\n",
    "            result.update(streamflow_stats)\n",
    "            \n",
    "            streamflow_results.append(result)\n",
    "            processing_summary['domains_with_streamflow'] += 1\n",
    "            processing_summary['continental_breakdown'][continent]['successful'] += 1\n",
    "            \n",
    "            print(f\"     ✅ Streamflow extracted: {result['mean_flow']:.2f} m³/s (range: {result['min_flow']:.2f}-{result['max_flow']:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\n🌊 Global Streamflow Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with routing: {processing_summary['domains_with_routing']}\")\n",
    "    print(f\"   Successful extractions: {processing_summary['domains_with_streamflow']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    print(f\"   🗺️  Continental breakdown:\")\n",
    "    for continent, stats in processing_summary['continental_breakdown'].items():\n",
    "        success_rate = (stats['successful'] / stats['attempted'] * 100) if stats['attempted'] > 0 else 0\n",
    "        print(f\"     {continent}: {stats['successful']}/{stats['attempted']} ({success_rate:.0f}% success)\")\n",
    "    \n",
    "    return streamflow_results, processing_summary\n",
    "\n",
    "def load_caravan_global_observations(completed_domains):\n",
    "    \"\"\"\n",
    "    Load CARAVAN observation data for global streamflow validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n📥 Loading CARAVAN Global Streamflow Observation Data...\")\n",
    "    \n",
    "    caravan_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_streamflow': 0,\n",
    "        'total_observations': 0,\n",
    "        'continental_breakdown': {}\n",
    "    }\n",
    "    \n",
    "    # Look for processed CARAVAN observation data in domain directories\n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_observations']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        continent = domain.get('continent', 'Unknown')\n",
    "        \n",
    "        if continent not in obs_summary['continental_breakdown']:\n",
    "            obs_summary['continental_breakdown'][continent] = {'found': 0, 'with_streamflow': 0}\n",
    "        \n",
    "        try:\n",
    "            print(f\"   📊 Loading {domain_name} ({continent})...\")\n",
    "            \n",
    "            obs_summary['sites_found'] += 1\n",
    "            obs_summary['continental_breakdown'][continent]['found'] += 1\n",
    "            \n",
    "            # Load streamflow observations\n",
    "            if domain['streamflow_obs_file']:\n",
    "                obs_df = pd.read_csv(domain['streamflow_obs_file'])\n",
    "                \n",
    "                # Find time and discharge columns\n",
    "                time_col = None\n",
    "                for col in ['datetime', 'date', 'time']:\n",
    "                    if col in obs_df.columns:\n",
    "                        time_col = col\n",
    "                        break\n",
    "                \n",
    "                discharge_col = None\n",
    "                for col in ['discharge_cms', 'streamflow', 'flow', 'Q']:\n",
    "                    if col in obs_df.columns:\n",
    "                        discharge_col = col\n",
    "                        break\n",
    "                \n",
    "                if time_col and discharge_col:\n",
    "                    obs_df[time_col] = pd.to_datetime(obs_df[time_col])\n",
    "                    obs_df.set_index(time_col, inplace=True)\n",
    "                    \n",
    "                    streamflow_obs = obs_df[discharge_col].dropna()\n",
    "                    \n",
    "                    if len(streamflow_obs) > 0:\n",
    "                        # Calculate streamflow statistics\n",
    "                        obs_stats = {\n",
    "                            'mean_flow': streamflow_obs.mean(),\n",
    "                            'max_flow': streamflow_obs.max(),\n",
    "                            'min_flow': streamflow_obs.min(),\n",
    "                            'std_flow': streamflow_obs.std(),\n",
    "                            'flow_variability': streamflow_obs.std() / streamflow_obs.mean() if streamflow_obs.mean() > 0 else np.nan\n",
    "                        }\n",
    "                        \n",
    "                        # Calculate flow percentiles\n",
    "                        percentiles = [5, 25, 50, 75, 95]\n",
    "                        for p in percentiles:\n",
    "                            obs_stats[f'q{p}'] = streamflow_obs.quantile(p/100)\n",
    "                        \n",
    "                        # Store observation data\n",
    "                        site_obs = {\n",
    "                            'streamflow_timeseries': streamflow_obs,\n",
    "                            'data_period': f\"{streamflow_obs.index.min()} to {streamflow_obs.index.max()}\",\n",
    "                            'data_points': len(streamflow_obs),\n",
    "                            'continent': continent,\n",
    "                            'climate_class': domain.get('climate_class', 'Unknown'),\n",
    "                            'flow_regime': domain.get('flow_regime', 'Unknown')\n",
    "                        }\n",
    "                        \n",
    "                        # Add statistics\n",
    "                        site_obs.update(obs_stats)\n",
    "                        \n",
    "                        # Add site metadata\n",
    "                        site_row = None\n",
    "                        for _, row in selected_watersheds.iterrows():\n",
    "                            if domain_name.startswith(row['ID']):\n",
    "                                site_row = row\n",
    "                                break\n",
    "                        \n",
    "                        if site_row is not None:\n",
    "                            site_obs['latitude'] = site_row['Lat']\n",
    "                            site_obs['longitude'] = site_row['Lon']\n",
    "                            site_obs['area_km2'] = site_row.get('Area_km2', np.nan)\n",
    "                            site_obs['scale'] = site_row.get('Scale', 'unknown')\n",
    "                            site_obs['watershed_id'] = site_row['ID']\n",
    "                        \n",
    "                        caravan_obs[domain_name] = site_obs\n",
    "                        \n",
    "                        obs_summary['sites_with_streamflow'] += 1\n",
    "                        obs_summary['continental_breakdown'][continent]['with_streamflow'] += 1\n",
    "                        obs_summary['total_observations'] += len(streamflow_obs)\n",
    "                        \n",
    "                        print(f\"     🌊 Streamflow obs: {streamflow_obs.mean():.2f} m³/s (range: {streamflow_obs.min():.2f}-{streamflow_obs.max():.2f}) ({len(streamflow_obs)} points)\")\n",
    "                else:\n",
    "                    print(f\"     ⚠️ Could not find time/discharge columns in observation file\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n🌊 CARAVAN Global Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with streamflow observations: {obs_summary['sites_with_streamflow']}\")\n",
    "    print(f\"   Total streamflow observations: {obs_summary['total_observations']}\")\n",
    "    \n",
    "    print(f\"   🗺️  Continental breakdown:\")\n",
    "    for continent, stats in obs_summary['continental_breakdown'].items():\n",
    "        print(f\"     {continent}: {stats['with_streamflow']}/{stats['found']} sites with streamflow\")\n",
    "    \n",
    "    return caravan_obs, obs_summary\n",
    "\n",
    "def create_global_streamflow_comparison_analysis(streamflow_results, caravan_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive global streamflow comparison analysis between simulated and observed\n",
    "    \"\"\"\n",
    "    print(f\"\\n🌊 Creating Global Streamflow Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in streamflow_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in caravan_obs:\n",
    "            # Align time periods\n",
    "            sim_flow = sim_result['streamflow_timeseries']\n",
    "            obs_flow = caravan_obs[domain_name]['streamflow_timeseries']\n",
    "            \n",
    "            # Find common time period\n",
    "            common_start = max(sim_flow.index.min(), obs_flow.index.min())\n",
    "            common_end = min(sim_flow.index.max(), obs_flow.index.max())\n",
    "            \n",
    "            if common_start < common_end:\n",
    "                # Resample to daily and align\n",
    "                sim_daily = sim_flow.resample('D').mean().loc[common_start:common_end]\n",
    "                obs_daily = obs_flow.resample('D').mean().loc[common_start:common_end]\n",
    "                \n",
    "                # Remove NaN values\n",
    "                valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                sim_valid = sim_daily[valid_mask]\n",
    "                obs_valid = obs_daily[valid_mask]\n",
    "                \n",
    "                if len(sim_valid) > 50:  # Need minimum data for meaningful comparison\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    def calculate_nse(obs, sim):\n",
    "                        return 1 - ((obs - sim) ** 2).sum() / ((obs - obs.mean()) ** 2).sum()\n",
    "                    \n",
    "                    def calculate_kge(obs, sim):\n",
    "                        # Kling-Gupta Efficiency\n",
    "                        r = np.corrcoef(obs, sim)[0, 1]\n",
    "                        alpha = sim.std() / obs.std()\n",
    "                        beta = sim.mean() / obs.mean()\n",
    "                        kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "                        return kge\n",
    "                    \n",
    "                    # Performance metrics\n",
    "                    nse = calculate_nse(obs_valid, sim_valid)\n",
    "                    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                    bias = (sim_valid - obs_valid).mean()\n",
    "                    pbias = 100 * bias / obs_valid.mean() if obs_valid.mean() > 0 else np.nan\n",
    "                    \n",
    "                    # Correlation\n",
    "                    try:\n",
    "                        correlation = obs_valid.corr(sim_valid)\n",
    "                        if pd.isna(correlation):\n",
    "                            correlation = 0.0\n",
    "                    except:\n",
    "                        correlation = 0.0\n",
    "                    \n",
    "                    # KGE\n",
    "                    try:\n",
    "                        kge = calculate_kge(obs_valid.values, sim_valid.values)\n",
    "                        if pd.isna(kge):\n",
    "                            kge = -999\n",
    "                    except:\n",
    "                        kge = -999\n",
    "                    \n",
    "                    common_site = {\n",
    "                        'domain_name': domain_name,\n",
    "                        'watershed_id': sim_result['watershed_id'],\n",
    "                        'latitude': sim_result['latitude'],\n",
    "                        'longitude': sim_result['longitude'],\n",
    "                        'area_km2': sim_result['area_km2'],\n",
    "                        'scale': sim_result['scale'],\n",
    "                        'continent': sim_result['continent'],\n",
    "                        'climate_class': sim_result['climate_class'],\n",
    "                        'flow_regime': sim_result['flow_regime'],\n",
    "                        'sim_flow': sim_valid,\n",
    "                        'obs_flow': obs_valid,\n",
    "                        'sim_mean': sim_valid.mean(),\n",
    "                        'obs_mean': obs_valid.mean(),\n",
    "                        'nse': nse,\n",
    "                        'kge': kge,\n",
    "                        'rmse': rmse,\n",
    "                        'bias': bias,\n",
    "                        'pbias': pbias,\n",
    "                        'correlation': correlation,\n",
    "                        'n_points': len(sim_valid),\n",
    "                        'common_period': f\"{common_start.date()} to {common_end.date()}\"\n",
    "                    }\n",
    "                    \n",
    "                    common_sites.append(common_site)\n",
    "                    \n",
    "                    print(f\"   ✅ {domain_name} ({sim_result['continent']}): NSE={nse:.3f}, KGE={kge:.3f}, r={correlation:.3f} ({len(sim_valid)} points)\")\n",
    "    \n",
    "    print(f\"\\n🌊 Global Streamflow Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(f\"   ⚠️  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Continental breakdown\n",
    "    continental_performance = {}\n",
    "    for site in common_sites:\n",
    "        continent = site['continent']\n",
    "        if continent not in continental_performance:\n",
    "            continental_performance[continent] = {'count': 0, 'nse_sum': 0, 'kge_sum': 0}\n",
    "        continental_performance[continent]['count'] += 1\n",
    "        continental_performance[continent]['nse_sum'] += site['nse']\n",
    "        if site['kge'] != -999:\n",
    "            continental_performance[continent]['kge_sum'] += site['kge']\n",
    "    \n",
    "    print(f\"   🗺️  Continental performance:\")\n",
    "    for continent, stats in continental_performance.items():\n",
    "        mean_nse = stats['nse_sum'] / stats['count']\n",
    "        mean_kge = stats['kge_sum'] / stats['count'] if stats['count'] > 0 else 0\n",
    "        print(f\"     {continent}: {stats['count']} sites, Mean NSE={mean_nse:.3f}, Mean KGE={mean_kge:.3f}\")\n",
    "    \n",
    "    # Create comprehensive global streamflow comparison visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n",
    "    \n",
    "    # Scatter plot: Observed vs Simulated (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    all_obs = np.concatenate([site['obs_flow'].values for site in common_sites])\n",
    "    all_sim = np.concatenate([site['sim_flow'].values for site in common_sites])\n",
    "    \n",
    "    ax1.scatter(all_obs, all_sim, alpha=0.3, s=8, c='blue')\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(all_obs.min(), all_sim.min())\n",
    "    max_val = max(all_obs.max(), all_sim.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    ax1.set_xlabel('Observed Streamflow (m³/s)')\n",
    "    ax1.set_ylabel('Simulated Streamflow (m³/s)')\n",
    "    ax1.set_title('Global: Simulated vs Observed Streamflow')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add overall statistics\n",
    "    overall_corr = np.corrcoef(all_obs, all_sim)[0,1] if len(all_obs) > 1 else 0\n",
    "    overall_nse = 1 - ((all_obs - all_sim) ** 2).sum() / ((all_obs - all_obs.mean()) ** 2).sum()\n",
    "    overall_bias = np.mean(all_sim - all_obs)\n",
    "    \n",
    "    stats_text = f'r = {overall_corr:.3f}\\\\nNSE = {overall_nse:.3f}\\\\nBias = {overall_bias:+.2f}'\n",
    "    ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance by continent (top middle)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    continental_stats = {}\n",
    "    for site in common_sites:\n",
    "        continent = site['continent']\n",
    "        if continent not in continental_stats:\n",
    "            continental_stats[continent] = {'nse': [], 'kge': [], 'corr': []}\n",
    "        \n",
    "        continental_stats[continent]['nse'].append(site['nse'])\n",
    "        continental_stats[continent]['kge'].append(site['kge'])\n",
    "        continental_stats[continent]['corr'].append(site['correlation'])\n",
    "    \n",
    "    # Plot NSE by continent\n",
    "    continents = list(continental_stats.keys())\n",
    "    nse_means = [np.mean(continental_stats[c]['nse']) for c in continents]\n",
    "    nse_stds = [np.std(continental_stats[c]['nse']) for c in continents]\n",
    "    \n",
    "    continent_colors = {'North America': 'red', 'Europe': 'blue', 'Australia': 'green', \n",
    "                       'South America': 'orange', 'Asia': 'purple', 'Africa': 'brown'}\n",
    "    colors = [continent_colors.get(c, 'gray') for c in continents]\n",
    "    \n",
    "    x_pos = range(len(continents))\n",
    "    bars = ax2.bar(x_pos, nse_means, yerr=nse_stds, capsize=5, alpha=0.7, color=colors)\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(continents, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "    ax2.set_title('Streamflow Performance by Continent')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean_val in zip(bars, nse_means):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                f'{mean_val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Performance by climate zone (top right)\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    climate_stats = {}\n",
    "    for site in common_sites:\n",
    "        climate = site['climate_class']\n",
    "        if climate not in climate_stats:\n",
    "            climate_stats[climate] = {'nse': [], 'kge': []}\n",
    "        \n",
    "        climate_stats[climate]['nse'].append(site['nse'])\n",
    "        climate_stats[climate]['kge'].append(site['kge'])\n",
    "    \n",
    "    if climate_stats:\n",
    "        climates = list(climate_stats.keys())\n",
    "        nse_means = [np.mean(climate_stats[c]['nse']) for c in climates]\n",
    "        nse_stds = [np.std(climate_stats[c]['nse']) for c in climates]\n",
    "        \n",
    "        climate_colors = {'Arid': 'brown', 'Semi-arid': 'orange', 'Sub-humid': 'lightgreen', 'Humid': 'blue'}\n",
    "        colors = [climate_colors.get(c, 'gray') for c in climates]\n",
    "        \n",
    "        x_pos = range(len(climates))\n",
    "        bars = ax3.bar(x_pos, nse_means, yerr=nse_stds, capsize=5, alpha=0.7, color=colors)\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(climates, rotation=45, ha='right')\n",
    "        ax3.set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "        ax3.set_title('Performance by Climate Zone')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Global spatial distribution of NSE (middle left)\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites]\n",
    "    lons = [site['longitude'] for site in common_sites]\n",
    "    nse_values = [site['nse'] for site in common_sites]\n",
    "    \n",
    "    scatter4 = ax4.scatter(lons, lats, c=nse_values, cmap='RdYlGn', s=100, \n",
    "                          vmin=-0.5, vmax=1.0, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax4.set_xlabel('Longitude')\n",
    "    ax4.set_ylabel('Latitude')\n",
    "    ax4.set_title('Global Distribution: NSE Performance')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_xlim(-180, 180)\n",
    "    ax4.set_ylim(-60, 80)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar4 = plt.colorbar(scatter4, ax=ax4)\n",
    "    cbar4.set_label('Nash-Sutcliffe Efficiency')\n",
    "    \n",
    "    # Flow regime performance (middle center)\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    regime_stats = {}\n",
    "    for site in common_sites:\n",
    "        regime = site['flow_regime']\n",
    "        if regime not in regime_stats:\n",
    "            regime_stats[regime] = {'nse': [], 'count': 0}\n",
    "        \n",
    "        regime_stats[regime]['nse'].append(site['nse'])\n",
    "        regime_stats[regime]['count'] += 1\n",
    "    \n",
    "    if regime_stats:\n",
    "        regimes = list(regime_stats.keys())\n",
    "        nse_means = [np.mean(regime_stats[r]['nse']) for r in regimes]\n",
    "        counts = [regime_stats[r]['count'] for r in regimes]\n",
    "        \n",
    "        regime_colors = {'snow_dominated': 'lightblue', 'mixed': 'lightcoral', 'rain_dominated': 'lightgreen'}\n",
    "        colors = [regime_colors.get(r, 'gray') for r in regimes]\n",
    "        \n",
    "        x_pos = range(len(regimes))\n",
    "        bars = ax5.bar(x_pos, nse_means, alpha=0.7, color=colors)\n",
    "        ax5.set_xticks(x_pos)\n",
    "        ax5.set_xticklabels([r.replace('_', '-').title() for r in regimes], rotation=45, ha='right')\n",
    "        ax5.set_ylabel('Mean NSE')\n",
    "        ax5.set_title('Performance by Flow Regime')\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add count labels\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                    f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Bias distribution (middle right)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    biases = [site['bias'] for site in common_sites]\n",
    "    ax6.hist(biases, bins=15, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax6.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "    ax6.set_xlabel('Bias (m³/s)')\n",
    "    ax6.set_ylabel('Number of Watersheds')\n",
    "    ax6.set_title('Global Distribution of Streamflow Bias')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Performance vs watershed area (bottom left)\n",
    "    ax7 = axes[2, 0]\n",
    "    \n",
    "    areas = [site['area_km2'] for site in common_sites if not np.isnan(site['area_km2'])]\n",
    "    nses = [site['nse'] for site in common_sites if not np.isnan(site['area_km2'])]\n",
    "    \n",
    "    if areas and nses:\n",
    "        scatter7 = ax7.scatter(areas, nses, alpha=0.7, s=40, c='green')\n",
    "        ax7.set_xlabel('Watershed Area (km²)')\n",
    "        ax7.set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "        ax7.set_title('Performance vs Watershed Size')\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        ax7.set_xscale('log')\n",
    "        ax7.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax7.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='NSE = 0.5')\n",
    "        ax7.legend()\n",
    "    \n",
    "    # KGE vs NSE comparison (bottom middle)\n",
    "    ax8 = axes[2, 1]\n",
    "    \n",
    "    nse_vals = [site['nse'] for site in common_sites]\n",
    "    kge_vals = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "    \n",
    "    if len(kge_vals) > 0:\n",
    "        ax8.scatter(nse_vals[:len(kge_vals)], kge_vals, alpha=0.7, s=40, c='purple')\n",
    "        ax8.set_xlabel('Nash-Sutcliffe Efficiency')\n",
    "        ax8.set_ylabel('Kling-Gupta Efficiency')\n",
    "        ax8.set_title('NSE vs KGE Performance')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add reference lines\n",
    "        ax8.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax8.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax8.plot([-1, 1], [-1, 1], 'k--', alpha=0.3, label='1:1 line')\n",
    "        ax8.legend()\n",
    "    \n",
    "    # Global performance summary (bottom right)\n",
    "    ax9 = axes[2, 2]\n",
    "    \n",
    "    # Create performance categories\n",
    "    perf_categories = {\n",
    "        'Excellent\\\\n(NSE > 0.75)': len([s for s in common_sites if s['nse'] > 0.75]),\n",
    "        'Good\\\\n(0.5 < NSE ≤ 0.75)': len([s for s in common_sites if 0.5 < s['nse'] <= 0.75]),\n",
    "        'Satisfactory\\\\n(0.2 < NSE ≤ 0.5)': len([s for s in common_sites if 0.2 < s['nse'] <= 0.5]),\n",
    "        'Unsatisfactory\\\\n(NSE ≤ 0.2)': len([s for s in common_sites if s['nse'] <= 0.2])\n",
    "    }\n",
    "    \n",
    "    categories = list(perf_categories.keys())\n",
    "    counts = list(perf_categories.values())\n",
    "    colors = ['darkgreen', 'green', 'yellow', 'red']\n",
    "    \n",
    "    bars = ax9.bar(range(len(categories)), counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax9.set_xticks(range(len(categories)))\n",
    "    ax9.set_xticklabels(categories, rotation=45, ha='right')\n",
    "    ax9.set_ylabel('Number of Watersheds')\n",
    "    ax9.set_title('Global Performance Categories')\n",
    "    ax9.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax9.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('CARAVAN Global Large Sample Streamflow Comparison Analysis', \n",
    "                 fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'global_streamflow_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Global streamflow comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\n🔍 Step 3.1: Global Streamflow Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_global_streamflow_domains()\n",
    "\n",
    "# Create global domain overview map\n",
    "if len(completed_domains) > 0:\n",
    "    total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete = create_global_streamflow_domain_overview_map(completed_domains)\n",
    "else:\n",
    "    print(f\"   ⚠️ No completed domains found for overview map\")\n",
    "    total_selected = len(selected_watersheds) if 'selected_watersheds' in locals() else 0\n",
    "    total_discovered = total_with_results = total_with_routing = total_with_obs = total_complete = 0\n",
    "\n",
    "print(f\"\\n🌊 Step 3.2: Global Streamflow Results Extraction\")\n",
    "\n",
    "# Extract streamflow results from simulations\n",
    "if len(completed_domains) > 0:\n",
    "    streamflow_results, streamflow_processing_summary = extract_global_streamflow_results_from_domains(completed_domains)\n",
    "    \n",
    "    # Load CARAVAN observations\n",
    "    caravan_obs, obs_summary = load_caravan_global_observations(completed_domains)\n",
    "else:\n",
    "    print(f\"   ⚠️ No completed domains available for analysis\")\n",
    "    streamflow_results = []\n",
    "    caravan_obs = {}\n",
    "    streamflow_processing_summary = {'domains_with_streamflow': 0}\n",
    "    obs_summary = {'sites_with_streamflow': 0}\n",
    "\n",
    "print(f\"\\n🌊 Step 3.3: Global Streamflow Comparison Analysis\")\n",
    "\n",
    "# Create global streamflow comparison analysis\n",
    "if streamflow_results and caravan_obs:\n",
    "    common_sites = create_global_streamflow_comparison_analysis(streamflow_results, caravan_obs)\n",
    "else:\n",
    "    print(f\"   ⚠️  Insufficient data for global streamflow comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final global summary report\n",
    "print(f\"\\n📋 Creating Final CARAVAN Global Streamflow Study Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'caravan_global_final_report.txt'\n",
    "summary_report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"CARAVAN Global Large Sample Streamflow Study - Final Analysis Report\\\\n\")\n",
    "    f.write(\"=\"*72 + \"\\\\n\\\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"GLOBAL PROCESSING SUMMARY:\\\\n\")\n",
    "    f.write(f\"  Watersheds selected globally: {total_selected}\\\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\\\n\")\n",
    "    f.write(f\"  Routing outputs available: {total_with_routing}\\\\n\")\n",
    "    f.write(f\"  Observations available: {total_with_obs}\\\\n\")\n",
    "    f.write(f\"  Complete streamflow validation: {total_complete}\\\\n\")\n",
    "    f.write(f\"  Streamflow extractions successful: {streamflow_processing_summary['domains_with_streamflow']}\\\\n\")\n",
    "    f.write(f\"  CARAVAN observations available: {obs_summary['sites_with_streamflow']}\\\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\\\n\\\\n\")\n",
    "        \n",
    "        # Global streamflow performance summary\n",
    "        nse_values = [site['nse'] for site in common_sites]\n",
    "        kge_values = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "        bias_values = [site['bias'] for site in common_sites]\n",
    "        corr_values = [site['correlation'] for site in common_sites]\n",
    "        \n",
    "        f.write(\"GLOBAL STREAMFLOW PERFORMANCE SUMMARY:\\\\n\")\n",
    "        f.write(f\"  Mean NSE: {np.mean(nse_values):.3f} ± {np.std(nse_values):.3f}\\\\n\")\n",
    "        if kge_values:\n",
    "            f.write(f\"  Mean KGE: {np.mean(kge_values):.3f} ± {np.std(kge_values):.3f}\\\\n\")\n",
    "        f.write(f\"  Mean correlation: {np.mean(corr_values):.3f} ± {np.std(corr_values):.3f}\\\\n\")\n",
    "        f.write(f\"  Mean bias: {np.mean(bias_values):+.2f} ± {np.std(bias_values):.2f} m³/s\\\\n\\\\n\")\n",
    "        \n",
    "        # Continental performance breakdown\n",
    "        continental_performance = {}\n",
    "        for site in common_sites:\n",
    "            continent = site['continent']\n",
    "            if continent not in continental_performance:\n",
    "                continental_performance[continent] = {'nse': [], 'kge': []}\n",
    "            continental_performance[continent]['nse'].append(site['nse'])\n",
    "            if site['kge'] != -999:\n",
    "                continental_performance[continent]['kge'].append(site['kge'])\n",
    "        \n",
    "        f.write(\"CONTINENTAL PERFORMANCE BREAKDOWN:\\\\n\")\n",
    "        for continent, performance in continental_performance.items():\n",
    "            mean_nse = np.mean(performance['nse'])\n",
    "            mean_kge = np.mean(performance['kge']) if performance['kge'] else 0\n",
    "            f.write(f\"  {continent}: NSE={mean_nse:.3f}, KGE={mean_kge:.3f} ({len(performance['nse'])} sites)\\\\n\")\n",
    "        \n",
    "        # Performance categories\n",
    "        excellent = len([s for s in common_sites if s['nse'] > 0.75])\n",
    "        good = len([s for s in common_sites if 0.5 < s['nse'] <= 0.75])\n",
    "        satisfactory = len([s for s in common_sites if 0.2 < s['nse'] <= 0.5])\n",
    "        unsatisfactory = len([s for s in common_sites if s['nse'] <= 0.2])\n",
    "        \n",
    "        f.write(\"\\\\nGLOBAL PERFORMANCE CATEGORIES:\\\\n\")\n",
    "        f.write(f\"  Excellent (NSE > 0.75): {excellent} watersheds\\\\n\")\n",
    "        f.write(f\"  Good (0.5 < NSE ≤ 0.75): {good} watersheds\\\\n\")\n",
    "        f.write(f\"  Satisfactory (0.2 < NSE ≤ 0.5): {satisfactory} watersheds\\\\n\")\n",
    "        f.write(f\"  Unsatisfactory (NSE ≤ 0.2): {unsatisfactory} watersheds\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING GLOBAL WATERSHEDS (by NSE):\\\\n\")\n",
    "        sorted_sites = sorted(common_sites, key=lambda x: x['nse'], reverse=True)\n",
    "        for i, site in enumerate(sorted_sites[:10]):\n",
    "            f.write(f\"  {i+1}. {site['watershed_id']} ({site['continent']}): NSE={site['nse']:.3f}, KGE={site['kge']:.3f}, Area={site['area_km2']:.0f} km²\\\\n\")\n",
    "\n",
    "print(f\"✅ Final global summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\\\n🎉 Step 3 Complete: CARAVAN Global Streamflow Validation Analysis\")\n",
    "print(f\"   📁 Results saved to: {experiment_dir}\")\n",
    "print(f\"   🌍 Global scope: {total_complete}/{total_selected} watersheds with complete validation\")\n",
    "\n",
    "if common_sites:\n",
    "    nse_values = [site['nse'] for site in common_sites]\n",
    "    kge_values = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "    \n",
    "    print(f\"   📊 Global analysis: {len(common_sites)} watersheds with sim/obs comparison\")\n",
    "    print(f\"   📈 Global NSE performance: Mean = {np.mean(nse_values):.3f}\")\n",
    "    if kge_values:\n",
    "        print(f\"   📈 Global KGE performance: Mean = {np.mean(kge_values):.3f}\")\n",
    "    \n",
    "    # Continental summary\n",
    "    continental_counts = {}\n",
    "    for site in common_sites:\n",
    "        continent = site['continent']\n",
    "        continental_counts[continent] = continental_counts.get(continent, 0) + 1\n",
    "    \n",
    "    print(f\"   🗺️  Continental validation coverage:\")\n",
    "    for continent, count in continental_counts.items():\n",
    "        print(f\"     {continent}: {count} validated watersheds\")\n",
    "else:\n",
    "    print(f\"   📈 Performance: Awaiting more simulation results for global analysis\")\n",
    "\n",
    "print(f\"\\\\n✅ CARAVAN Global Large Sample Streamflow Analysis Complete!\")\n",
    "print(f\"   🌍 Multi-continental streamflow hydrology validation achieved\")\n",
    "print(f\"   📊 Statistical patterns identified across global watershed gradients\")  \n",
    "print(f\"   🌐 Tutorial series culmination: Regional → Global → Multi-continental analysis!\")\n",
    "print(f\"   🏆 Global hydrological understanding through CONFLUENCE framework!\")\n",
    "\n",
    "print(f\"\\\\n🎯 Tutorial Complete: From Point-Scale to Global-Scale Hydrological Modeling\")\n",
    "print(f\"   📈 CONFLUENCE Tutorial Series: Energy → Snow → Regional Streamflow → Global Streamflow\")\n",
    "print(f\"   🌊 Comprehensive validation across multiple scales and environments\")\n",
    "print(f\"   🔬 Scientific advancement from case studies to universal hydrological principles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
