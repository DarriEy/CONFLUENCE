{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a5229d",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 10: CAMELS Large Sample Study (Multi-Basin Streamflow Analysis)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial represents the culmination of our CONFLUENCE large sample studies series: systematic streamflow modeling across hundreds of watersheds using the CAMELS spat dataset (Knoben et al., 2025). While our previous large sample tutorials focused on point-scale processes (FLUXNET energy fluxes, NorSWE snow dynamics), this tutorial demonstrates watershed-scale analysis of the most fundamental hydrological variable: streamflow. This represents the classic application of hydrological modeling and the ultimate test of CONFLUENCE's capabilities across diverse watersheds.\n",
    "\n",
    "### CAMELS Spat: The Gold Standard for Large Sample Hydrology\n",
    "\n",
    "The CAMELS Spat dataset was specifically designed to revolutionize hydrological science through large sample studies:\n",
    "\n",
    "**Comprehensive Coverage**:\n",
    "- **CAMELS-US**: 671 watersheds across the contiguous United States\n",
    "- **Global Extensions**: CAMELS-GB, CAMELS-BR, CAMELS-CL, CAMELS-AUS, CAMELS-FR\n",
    "- **Climate Diversity**: Arid to humid, tropical to continental, coastal to mountainous\n",
    "- **Scale Range**: 4 to 25,000 kmÂ² watersheds\n",
    "\n",
    "**Standardized Framework**:\n",
    "- **Meteorological Forcing**: Gridded precipitation and temperature data\n",
    "- **Streamflow Observations**: Quality-controlled daily discharge time series\n",
    "- **Catchment Attributes**: Topographic, geologic, soil, and vegetation characteristics\n",
    "- **Minimal Human Impact**: Focus on near-natural watersheds\n",
    "\n",
    "**Research Impact**:\n",
    "- **Benchmark Studies**: Standard dataset for model comparison\n",
    "- **Process Understanding**: Systematic analysis of hydrological controls\n",
    "- **Machine Learning**: Training data for data-driven approaches\n",
    "- **Climate Studies**: Assessment of climate change impacts on hydrology\n",
    "\n",
    "### Streamflow: The Integrative Hydrological Variable\n",
    "\n",
    "Streamflow represents the integrated response of all watershed processes:\n",
    "\n",
    "**Process Integration**:\n",
    "- **Precipitation Processing**: Interception, infiltration, and runoff generation\n",
    "- **Evapotranspiration**: Plant water use and soil moisture dynamics\n",
    "- **Groundwater Interactions**: Baseflow contributions and storage dynamics\n",
    "- **Routing Processes**: Travel time and channel hydraulics\n",
    "- **Snow Processes**: Seasonal storage and release in cold regions\n",
    "\n",
    "**Observational Advantages**:\n",
    "- **Direct Measurement**: Streamflow is directly observable at gauging stations\n",
    "- **Integrative Nature**: Represents the integrated watershed response\n",
    "- **Long Records**: Many sites have decades of continuous observations\n",
    "- **Management Relevance**: Direct connection to water resources applications\n",
    "\n",
    "### Scientific Importance of Multi-Basin Streamflow Analysis\n",
    "\n",
    "Large sample streamflow studies address fundamental questions in hydrology:\n",
    "\n",
    "**Hydrological Controls**:\n",
    "- **Climate vs. Landscape**: Relative importance of meteorological vs. physical controls\n",
    "- **Scale Dependencies**: How hydrological processes scale from hillslopes to watersheds\n",
    "- **Threshold Behaviors**: Nonlinear responses to climate and landscape characteristics\n",
    "- **Regional Patterns**: Systematic variations across physiographic regions\n",
    "\n",
    "**Model Evaluation**:\n",
    "- **Process Representation**: Which model components are most important?\n",
    "- **Parameter Transferability**: Can parameters be regionalized effectively?\n",
    "- **Uncertainty Quantification**: How does model uncertainty vary across environments?\n",
    "- **Structural Adequacy**: Are current model structures sufficient?\n",
    "\n",
    "### CAMELS vs. Previous Large Sample Studies\n",
    "\n",
    "This tutorial complements our previous large sample analyses:\n",
    "\n",
    "| Dataset | Scale | Focus | Validation | Complexity |\n",
    "|---------|-------|-------|------------|------------|\n",
    "| **FLUXNET** | Point | Energy/carbon fluxes | Flux measurements | Ecosystem interactions |\n",
    "| **NorSWE** | Point | Snow dynamics | State variables | Phase change physics |\n",
    "| **CAMELS** | Watershed | Streamflow | Discharge observations | Process integration |\n",
    "\n",
    "### Unique Challenges of Multi-Basin Streamflow Modeling\n",
    "\n",
    "Watershed-scale streamflow modeling presents distinct challenges:\n",
    "\n",
    "**Spatial Heterogeneity**:\n",
    "- **Landscape Diversity**: Elevation, slope, soil, and vegetation gradients\n",
    "- **Climate Variability**: Precipitation and temperature patterns within watersheds\n",
    "- **Geological Controls**: Subsurface heterogeneity and groundwater systems\n",
    "- **Scale Interactions**: Processes operating at different spatial scales\n",
    "\n",
    "**Temporal Dynamics**:\n",
    "- **Multiple Timescales**: Event response, seasonal cycles, and long-term trends\n",
    "- **Memory Effects**: Antecedent conditions and storage dynamics\n",
    "- **Extreme Events**: Floods, droughts, and their watershed-scale impacts\n",
    "- **Climate Variability**: Interannual and decadal variations\n",
    "\n",
    "### CONFLUENCE's Advantages for Multi-Basin Studies\n",
    "\n",
    "CONFLUENCE's design provides unique advantages for large sample streamflow analysis:\n",
    "\n",
    "**Consistent Methodology**:\n",
    "- **Standardized Workflow**: Same modeling approach across all watersheds\n",
    "- **Automated Processing**: Efficient setup and execution for hundreds of basins\n",
    "- **Reproducible Science**: Complete documentation of modeling decisions\n",
    "- **Quality Control**: Systematic evaluation of model performance\n",
    "\n",
    "**Physical Realism**:\n",
    "- **Process-Based Models**: Explicit representation of hydrological processes\n",
    "- **Flexible Structure**: Adaptable to different watershed characteristics\n",
    "- **Multi-Model Capability**: Compare different model structures\n",
    "- **Uncertainty Assessment**: Quantify parameter and structural uncertainty\n",
    "\n",
    "### Research Questions for Multi-Basin Analysis\n",
    "\n",
    "Large sample streamflow studies enable investigation of fundamental hydrological questions:\n",
    "\n",
    "1. **Process Controls**: What are the dominant controls on streamflow generation across different environments?\n",
    "2. **Model Performance**: How does model performance vary with climate, topography, and soil characteristics?\n",
    "3. **Parameter Patterns**: Are there systematic patterns in optimal parameter values across watersheds?\n",
    "4. **Prediction Capability**: Can models trained in one region predict streamflow in another?\n",
    "5. **Climate Sensitivity**: How sensitive is streamflow to climate variability and change?\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates several key capabilities for multi-basin streamflow analysis:\n",
    "\n",
    "1. **Watershed-Scale Configuration**: Adapt CONFLUENCE for diverse watershed characteristics\n",
    "2. **Streamflow Validation**: Compare simulated and observed hydrographs across sites\n",
    "3. **Performance Analysis**: Evaluate model performance using multiple metrics\n",
    "4. **Regional Patterns**: Identify systematic variations in model performance\n",
    "5. **Process Diagnostics**: Understand reasons for model success and failure\n",
    "\n",
    "### Methodological Framework\n",
    "\n",
    "Multi-basin streamflow studies require sophisticated analytical approaches:\n",
    "\n",
    "**Site Selection**:\n",
    "- **Climate Gradients**: Represent aridity, temperature, and seasonality gradients\n",
    "- **Physiographic Diversity**: Include different geological and topographic settings\n",
    "- **Scale Representation**: Cover the range of watershed sizes\n",
    "- **Data Quality**: Ensure reliable streamflow and meteorological data\n",
    "\n",
    "**Model Evaluation**:\n",
    "- **Multiple Metrics**: Nash-Sutcliffe efficiency, Kling-Gupta efficiency, bias\n",
    "- **Flow Components**: Evaluate high flows, low flows, and timing\n",
    "- **Seasonal Performance**: Assess model performance across different seasons\n",
    "- **Extreme Events**: Evaluate performance during floods and droughts\n",
    "\n",
    "### Tutorial Structure\n",
    "\n",
    "This tutorial follows the established large sample framework while emphasizing streamflow-specific aspects:\n",
    "\n",
    "1. **CAMELS Site Selection**: Choose representative watersheds across environmental gradients\n",
    "2. **Watershed Configuration**: Adapt CONFLUENCE for diverse basin characteristics\n",
    "3. **Streamflow-Focused Setup**: Configure for discharge validation and routing\n",
    "4. **Batch Processing**: Execute CONFLUENCE across multiple watersheds\n",
    "5. **Hydrograph Analysis**: Collect and analyze streamflow time series\n",
    "6. **Performance Assessment**: Evaluate model performance across sites\n",
    "7. **Regional Synthesis**: Identify patterns and controls on model performance\n",
    "\n",
    "### Scientific Impact\n",
    "\n",
    "Multi-basin streamflow studies contribute to advancing hydrological science:\n",
    "\n",
    "- **Process Understanding**: Identify universal vs. regional hydrological controls\n",
    "- **Model Development**: Improve model structure and parameterization\n",
    "- **Water Resources**: Enhance streamflow prediction for management applications\n",
    "- **Climate Applications**: Improve projections of streamflow under changing climate\n",
    "- **Ungauged Basins**: Develop approaches for prediction in ungauged watersheds\n",
    "\n",
    "### Tutorial Series Culmination\n",
    "\n",
    "This tutorial represents the ultimate demonstration of CONFLUENCE's capabilities:\n",
    "\n",
    "**Complete Skill Integration**:\n",
    "- **Point-scale understanding**: Foundation in individual processes\n",
    "- **Spatial scaling**: Watershed-scale process integration\n",
    "- **Large sample methods**: Systematic multi-site analysis\n",
    "- **Workflow automation**: Efficient processing of hundreds of sites\n",
    "\n",
    "**Hydrological Scope**:\n",
    "- **Process diversity**: Energy balance, snow dynamics, and streamflow\n",
    "- **Scale range**: Points to watersheds to continental domains\n",
    "- **Temporal coverage**: Event-scale to multi-decadal analysis\n",
    "- **Environmental gradients**: Complete range of hydroclimatic conditions\n",
    "\n",
    "**Methodological Sophistication**:\n",
    "- **Model complexity**: From simple to sophisticated process representations\n",
    "- **Uncertainty quantification**: Parameter and structural uncertainty assessment\n",
    "- **Comparative analysis**: Systematic evaluation across multiple sites\n",
    "- **Reproducible science**: Complete workflow documentation and automation\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "The skills developed in this tutorial have immediate practical applications:\n",
    "\n",
    "- **Water Resources Management**: Streamflow prediction for reservoir operations\n",
    "- **Flood Forecasting**: Improved understanding of extreme event generation\n",
    "- **Climate Change Assessment**: Quantifying future streamflow changes\n",
    "- **Ecological Applications**: Instream flow requirements and habitat assessment\n",
    "- **Policy Support**: Science-based water allocation and management decisions\n",
    "\n",
    "By completing this tutorial, you'll have mastered the complete spectrum of CONFLUENCE applications, from individual process understanding to large sample comparative analysis. This represents the cutting edge of hydrological science, where systematic multi-site analysis drives both theoretical advances and practical applications in water resources management.\n",
    "\n",
    "The combination of CONFLUENCE's workflow efficiency with CAMELS' comprehensive watershed database provides an unparalleled framework for advancing our understanding of how watersheds function across Earth's diverse hydroclimatic environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c48153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the large sample experiment\n",
    "experiment_config = {\n",
    "    'dataset': 'camels-spat',\n",
    "    'max_watersheds': 5,\n",
    "    'dry_run': False,\n",
    "    'experiment_name': 'camelsspat_tutorial',\n",
    "    'template_config': '/home/darri.eythorsson/code/CONFLUENCE/0_config_files/config_distributed_basin_template.yaml',\n",
    "    'config_dir': '/home/darri.eythorsson/code/CONFLUENCE/0_config_files/camels_spat',\n",
    "    'camelsspat_script': '/home/darri.eythorsson/code/CONFLUENCE/9_scripts/run_watersheds_camelsspat.py',\n",
    "    'camelsspat_dir': '/work/comphyd_lab/data/_to-be-moved/camels-spat-upload/shapefiles/meso-scale/shapes-distributed',\n",
    "    'metadata_csv': 'camels-spat-metadata.csv'\n",
    "}\n",
    "\n",
    "# Create experiment directory\n",
    "experiment_dir = Path(f\"./experiments/{experiment_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(experiment_dir / 'experiment_config.yaml', 'w') as f:\n",
    "    yaml.dump(experiment_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88550001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import function from the CAMELS-SPAT script to extract shapefile info\n",
    "sys.path.append(str(Path(experiment_config['camelsspat_script']).parent))\n",
    "from run_watersheds_camelsspat import extract_shapefile_info\n",
    "\n",
    "# Check if we already have watershed info cached\n",
    "watersheds_csv = experiment_dir / 'camelsspat_watersheds.csv'\n",
    "\n",
    "if watersheds_csv.exists():\n",
    "    print(f\"Loading existing watershed information\")\n",
    "    watersheds_df = pd.read_csv(watersheds_csv)\n",
    "else:\n",
    "    print(f\"Extracting watershed information...\")\n",
    "    watersheds_df = extract_shapefile_info(experiment_config['camelsspat_dir'])\n",
    "    watersheds_df.to_csv(watersheds_csv, index=False)\n",
    "\n",
    "print(f\"Found {len(watersheds_df)} watersheds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CAMELS-SPAT metadata if available\n",
    "metadata_path = experiment_config['metadata_csv']\n",
    "\n",
    "if os.path.exists(metadata_path):\n",
    "    print(f\"Loading and merging metadata\")\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    metadata_df.columns = [col.strip() for col in metadata_df.columns]\n",
    "    \n",
    "    watersheds_df['Metadata_ID'] = watersheds_df['ID'].str.replace(r'^[A-Z]+_', '', regex=True)\n",
    "    watersheds_merged = pd.merge(\n",
    "        watersheds_df, metadata_df, \n",
    "        left_on='Metadata_ID', right_on='ID',\n",
    "        how='left', suffixes=('', '_metadata')\n",
    "    )\n",
    "    watersheds_df = watersheds_merged\n",
    "else:\n",
    "    print(f\"No metadata file found. Proceeding with shapefile information only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c12ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Launch the large sample experiment\n",
    "cmd = ['python', experiment_config['camelsspat_script']]\n",
    "\n",
    "# Add command-line arguments\n",
    "if experiment_config['dry_run']:\n",
    "    cmd.extend(['--dry-run'])\n",
    "\n",
    "if experiment_config['max_watersheds'] > 0:\n",
    "    cmd.extend(['--max-watersheds', str(experiment_config['max_watersheds'])])\n",
    "\n",
    "print(f\"Launching CONFLUENCE for {experiment_config['max_watersheds']} watersheds\")\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Save submission log\n",
    "with open(experiment_dir / 'submission.log', 'w') as f:\n",
    "    f.write(result.stdout)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Parse submission log and check job status\n",
    "submission_log = experiment_dir / 'submission.log'\n",
    "submitted_jobs = []\n",
    "\n",
    "if submission_log.exists():\n",
    "    with open(submission_log, 'r') as f:\n",
    "        log_content = f.read()\n",
    "    \n",
    "    # Extract job submissions\n",
    "    import re\n",
    "    pattern = r'Domain: ([^,]+), Job ID: (\\d+)'\n",
    "    matches = re.findall(pattern, log_content)\n",
    "    for domain, job_id in matches:\n",
    "        submitted_jobs.append({'domain': domain, 'job_id': job_id})\n",
    "    \n",
    "    if submitted_jobs:\n",
    "        jobs_df = pd.DataFrame(submitted_jobs)\n",
    "        print(f\"Submitted {len(jobs_df)} jobs\")\n",
    "\n",
    "# Check job status\n",
    "def check_job_status(user=None):\n",
    "    user = user or os.environ.get('USER')\n",
    "    cmd = ['squeue', '-u', user]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "print(\"\\nCurrent jobs:\")\n",
    "print(check_job_status())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aaf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find completed watershed simulations\n",
    "confluence_data_dir = Path(\"/work/comphyd_lab/data/CONFLUENCE_data\")\n",
    "camelsspat_dir = confluence_data_dir / \"camels_spat\"\n",
    "\n",
    "completed = []\n",
    "if camelsspat_dir.exists():\n",
    "    for domain_dir in camelsspat_dir.glob(\"domain_*\"):\n",
    "        watershed_id = domain_dir.name.replace('domain_', '')\n",
    "        sim_dir = domain_dir / \"simulations\"\n",
    "        \n",
    "        if sim_dir.exists() and list(sim_dir.rglob(\"*.nc\")):\n",
    "            completed.append({\n",
    "                'watershed_id': watershed_id,\n",
    "                'domain_dir': domain_dir,\n",
    "                'sim_dir': sim_dir\n",
    "            })\n",
    "\n",
    "print(f\"Completed simulations: {len(completed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to load model outputs\n",
    "def load_summa_output(sim_dir, variable='scalarSWE'):\n",
    "    import xarray as xr\n",
    "    summa_output_dir = sim_dir / 'run_1' / 'SUMMA'\n",
    "    if not summa_output_dir.exists():\n",
    "        return None\n",
    "    output_files = list(summa_output_dir.glob(\"*timestep*.nc\"))\n",
    "    if output_files:\n",
    "        ds = xr.open_dataset(output_files[0])\n",
    "        if variable in ds.variables:\n",
    "            return pd.DataFrame({\n",
    "                'time': pd.to_datetime(ds.time.values),\n",
    "                'value': ds[variable].values.flatten()\n",
    "            })\n",
    "    return None\n",
    "\n",
    "def load_streamflow(sim_dir):\n",
    "    import xarray as xr\n",
    "    mizuroute_dir = sim_dir / 'run_1' / 'mizuRoute'\n",
    "    if not mizuroute_dir.exists():\n",
    "        print(f\"mizuRoute directory not found: {mizuroute_dir}\")\n",
    "        return None\n",
    "    \n",
    "    output_files = list(mizuroute_dir.glob(\"*.nc\"))\n",
    "    if not output_files:\n",
    "        print(f\"No netCDF files found in: {mizuroute_dir}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        ds = xr.open_dataset(output_files[0])\n",
    "        \n",
    "        # Try to find streamflow variable\n",
    "        for var in ['IRFroutedRunoff', 'routedRunoff', 'discharge']:\n",
    "            if var in ds.variables:\n",
    "                # Check dimensions\n",
    "                var_dims = ds[var].dims\n",
    "                \n",
    "                if len(var_dims) == 1 and 'time' in var_dims:\n",
    "                    # Single dimension (time only)\n",
    "                    return pd.DataFrame({\n",
    "                        'time': pd.to_datetime(ds.time.values),\n",
    "                        'simulated': ds[var].values\n",
    "                    })\n",
    "                \n",
    "                elif len(var_dims) > 1:\n",
    "                    # Multiple dimensions (likely time and segments/reaches)\n",
    "                    time_dim = 'time'\n",
    "                    reach_dims = [d for d in var_dims if d != time_dim]\n",
    "                    \n",
    "                    if not reach_dims:\n",
    "                        print(f\"Unexpected dimensions in {var}: {var_dims}\")\n",
    "                        continue\n",
    "                    \n",
    "                    reach_dim = reach_dims[0]\n",
    "                    \n",
    "                    # Find the right reach/segment to use\n",
    "                    # If there's a single reach, use it\n",
    "                    if ds[reach_dim].size == 1:\n",
    "                        reach_idx = 0\n",
    "                    else:\n",
    "                        # TODO: Add logic to find the appropriate outlet reach\n",
    "                        # For now, use the last reach which is often the outlet\n",
    "                        reach_idx = ds[reach_dim].size - 1\n",
    "                    \n",
    "                    # Extract data for the selected reach\n",
    "                    flow_data = ds[var].isel({reach_dim: reach_idx}).values\n",
    "                    \n",
    "                    # Make sure lengths match\n",
    "                    if len(ds.time) == len(flow_data):\n",
    "                        return pd.DataFrame({\n",
    "                            'time': pd.to_datetime(ds.time.values),\n",
    "                            'simulated': flow_data\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Length mismatch: time ({len(ds.time)}) vs flow ({len(flow_data)})\")\n",
    "        \n",
    "        print(f\"No suitable flow variable found in {output_files[0]}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading streamflow data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_observed_streamflow(domain_dir, watershed_id):\n",
    "    obs_dir = domain_dir / 'observations' / 'streamflow' / 'preprocessed'\n",
    "    obs_file = list(obs_dir.glob(f\"*{watershed_id}*streamflow*.csv\"))\n",
    "    \n",
    "    if obs_file:\n",
    "        obs_df = pd.read_csv(obs_file[0])\n",
    "        time_col = 'datetime' if 'datetime' in obs_df.columns else 'date'\n",
    "        flow_col = 'discharge_cms' if 'discharge_cms' in obs_df.columns else 'streamflow'\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'time': pd.to_datetime(obs_df[time_col]),\n",
    "            'observed': obs_df[flow_col]\n",
    "        })\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5d85f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "def calculate_metrics(obs, sim):\n",
    "    mask = ~(np.isnan(obs) | np.isnan(sim))\n",
    "    obs, sim = obs[mask], sim[mask]\n",
    "    \n",
    "    if len(obs) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Nash-Sutcliffe Efficiency\n",
    "    nse = 1 - np.sum((obs - sim)**2) / np.sum((obs - np.mean(obs))**2)\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((obs - sim)**2))\n",
    "    # Percent Bias\n",
    "    pbias = 100 * np.sum(sim - obs) / np.sum(obs) if np.sum(obs) != 0 else np.nan\n",
    "    # Correlation\n",
    "    corr = np.corrcoef(obs, sim)[0, 1] if len(obs) > 1 else np.nan\n",
    "    \n",
    "    return {'NSE': nse, 'RMSE': rmse, 'PBIAS': pbias, 'Correlation': corr}\n",
    "\n",
    "# Calculate metrics for completed watersheds\n",
    "metrics_list = []\n",
    "for ws in completed:\n",
    "    sim_flow = load_streamflow(ws['sim_dir'])\n",
    "    obs_flow = load_observed_streamflow(ws['domain_dir'], ws['watershed_id'])\n",
    "    \n",
    "    if sim_flow is not None and obs_flow is not None:\n",
    "        merged = pd.merge(obs_flow, sim_flow, on='time', how='inner')\n",
    "        metrics = calculate_metrics(merged['observed'].values, merged['simulated'].values)\n",
    "        metrics['watershed_id'] = ws['watershed_id']\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "if metrics_list:\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(metrics_df)\n",
    "    metrics_df.to_csv(experiment_dir / 'performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b417cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "if completed:\n",
    "    print(\"### CAMELS-SPAT Experiment Summary Report ###\")\n",
    "    print(f\"Experiment: {experiment_config['experiment_name']}\")\n",
    "    print(f\"Date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Completed simulations: {len(completed)}\")\n",
    "    \n",
    "    if metrics_list:\n",
    "        print(\"\\nOverall Model Performance:\")\n",
    "        for metric in ['NSE', 'RMSE', 'PBIAS', 'Correlation']:\n",
    "            if metric in metrics_df.columns:\n",
    "                print(f\"  Average {metric}: {metrics_df[metric].mean():.3f}\")\n",
    "                \n",
    "    # Save report to file\n",
    "    report_path = experiment_dir / 'experiment_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"CAMELS-SPAT Experiment Summary\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d')}\\n\")\n",
    "        f.write(f\"Experiment: {experiment_config['experiment_name']}\\n\")\n",
    "    \n",
    "    print(f\"\\nSummary report saved to {report_path}\")\n",
    "else:\n",
    "    print(\"No completed simulations found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
