{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a5229d",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 10: CAMELS Large Sample Study (Multi-Basin Streamflow Analysis)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates systematic streamflow modeling across multiple watersheds using the CAMELS spat dataset (Knoben et al., 2025). Unlike previous tutorials focused on point-scale processes, this analysis addresses watershed-scale streamflow prediction across diverse environmental conditions.\n",
    "\n",
    "### CAMELS Spat Dataset\n",
    "\n",
    "The CAMELS Spat dataset provides standardized data for 671 watersheds across the contiguous United States, with global extensions covering additional regions. The dataset includes gridded meteorological forcing, quality-controlled daily discharge observations, and comprehensive catchment attributes. Watersheds range from 4 to 25,000 kmÂ² and span diverse climate and physiographic conditions while maintaining focus on near-natural basins with minimal human impact.\n",
    "\n",
    "### Streamflow Modeling Challenges\n",
    "\n",
    "Streamflow represents the integrated watershed response to precipitation, evapotranspiration, groundwater interactions, and routing processes. Multi-basin analysis presents challenges including spatial heterogeneity across landscapes, temporal dynamics spanning multiple timescales, and scale interactions between different hydrological processes.\n",
    "\n",
    "### Research Objectives\n",
    "\n",
    "This tutorial addresses fundamental questions about dominant controls on streamflow generation, model performance variations across environmental gradients, parameter transferability between watersheds, and streamflow sensitivity to climate variability. The analysis employs multiple performance metrics including Nash-Sutcliffe efficiency, Kling-Gupta efficiency, and bias assessment.\n",
    "\n",
    "### Methodological Framework\n",
    "\n",
    "The approach involves strategic site selection across environmental gradients, standardized model configuration for diverse basin characteristics, batch processing execution, and systematic performance evaluation. Sites are selected to represent climate diversity, physiographic variation, and multiple watershed scales while ensuring adequate data quality.\n",
    "\n",
    "### CONFLUENCE Advantages\n",
    "\n",
    "CONFLUENCE provides consistent methodology across watersheds, automated processing capabilities, and systematic quality control. The framework emphasizes process-based modeling with flexible structure adaptable to different watershed characteristics and comprehensive uncertainty assessment.\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates watershed-scale configuration, streamflow validation through observed-simulated comparisons, performance analysis across sites, regional pattern identification, and process diagnostics. Results contribute to improved understanding of hydrological controls, enhanced model development, and applications in water resources management and climate impact assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e562-d16c-4257-909b-f6dbe7ff9483",
   "metadata": {},
   "source": [
    "## Step 1: Multi-Basin Streamflow Experimental Design and Site Selection\n",
    "Transitioning from the FLUXNET energy balance and NORSWE snow focus to systematic streamflow hydrology simulations, this step establishes the foundation for large sample hydrological modeling using the comprehensive CAMELS-Spat dataset. We demonstrate how CONFLUENCE's workflow efficiency enables systematic streamflow evaluation across the full spectrum of North American hydroclimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d212f-06e1-49b5-9441-ed14e9c1e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set up plotting style for watershed visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "%matplotlib inline\n",
    "confluence_path = Path('../').resolve()\n",
    "\n",
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/anvil/scratch/x-deythorsson/CONFLUENCE_data')  # Update this path\n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# =============================================================================\n",
    "# CAMELS-SPAT TEMPLATE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Load streamflow configuration template or create from base template\n",
    "streamflow_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "with open(streamflow_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update for CAMELS-SPAT tutorial-specific settings\n",
    "config_updates = {\n",
    "    'CONFLUENCE_CODE_DIR': str(CONFLUENCE_CODE_DIR),\n",
    "    'CONFLUENCE_DATA_DIR': str(CONFLUENCE_DATA_DIR),\n",
    "    'DOMAIN_NAME': 'camelsspat_template',\n",
    "    'EXPERIMENT_ID': 'run_1',\n",
    "    'EXPERIMENT_TIME_START': '2005-01-01 01:00',\n",
    "    'EXPERIMENT_TIME_END': '2015-12-31 23:00',  # 10-year period for streamflow analysis\n",
    "}\n",
    "\n",
    "config_dict.update(config_updates)\n",
    "\n",
    "# Save CAMELS-SPAT configuration template\n",
    "camelsspat_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_camelsspat_template.yaml'\n",
    "with open(camelsspat_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"CAMELS-SPAT template configuration saved: {camelsspat_config_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND EXAMINE CAMELS-SPAT WATERSHED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nLoading CAMELS-SPAT Watershed Database...\")\n",
    "\n",
    "# Load the CAMELS-SPAT watersheds database\n",
    "try:\n",
    "    camelsspat_df = pd.read_csv('camels-spat-metadata.csv')\n",
    "    print(f\"Successfully loaded CAMELS-SPAT database: {len(camelsspat_df)} watersheds available\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"CAMELS-SPAT database not found, creating demonstration dataset...\")\n",
    "    \n",
    "    # Create demonstration CAMELS-SPAT dataset for tutorial\n",
    "    np.random.seed(42)\n",
    "    n_watersheds = 100\n",
    "    \n",
    "    # Generate realistic North American watershed locations\n",
    "    # Focus on major watershed regions with good streamflow data\n",
    "    regions = [\n",
    "        {'name': 'Pacific_Northwest', 'lat_range': (42, 49), 'lon_range': (-125, -117), 'n': 20},\n",
    "        {'name': 'Rocky_Mountains', 'lat_range': (37, 48), 'lon_range': (-115, -105), 'n': 25},\n",
    "        {'name': 'Great_Plains', 'lat_range': (35, 45), 'lon_range': (-105, -95), 'n': 15},\n",
    "        {'name': 'Southeastern_US', 'lat_range': (30, 40), 'lon_range': (-95, -80), 'n': 20},\n",
    "        {'name': 'Northeastern_US', 'lat_range': (40, 47), 'lon_range': (-80, -67), 'n': 15},\n",
    "        {'name': 'California', 'lat_range': (32, 42), 'lon_range': (-125, -114), 'n': 5}\n",
    "    ]\n",
    "    \n",
    "    watersheds_data = []\n",
    "    watershed_id = 1\n",
    "    \n",
    "    for region in regions:\n",
    "        for i in range(region['n']):\n",
    "            lat = np.random.uniform(region['lat_range'][0], region['lat_range'][1])\n",
    "            lon = np.random.uniform(region['lon_range'][0], region['lon_range'][1])\n",
    "            \n",
    "            # Area based on typical CAMELS watersheds (log-normal distribution)\n",
    "            area = np.random.lognormal(np.log(200), 1.2)\n",
    "            area = np.clip(area, 10, 15000)  # Clip to CAMELS range\n",
    "            \n",
    "            # Elevation varies by region and affects streamflow characteristics\n",
    "            if region['name'] == 'Rocky_Mountains':\n",
    "                elevation = np.random.uniform(1200, 3500)\n",
    "            elif region['name'] == 'Pacific_Northwest':\n",
    "                elevation = np.random.uniform(300, 2200)\n",
    "            elif region['name'] == 'California':\n",
    "                elevation = np.random.uniform(200, 2800)\n",
    "            else:\n",
    "                elevation = np.random.uniform(50, 1200)\n",
    "            \n",
    "            # Climate characteristics affecting streamflow\n",
    "            if region['name'] in ['Pacific_Northwest', 'Southeastern_US']:\n",
    "                map_precip = np.random.uniform(800, 2500)  # Wet regions\n",
    "                mat_temp = np.random.uniform(5, 18)\n",
    "            elif region['name'] == 'Great_Plains':\n",
    "                map_precip = np.random.uniform(300, 800)   # Dry regions\n",
    "                mat_temp = np.random.uniform(8, 16)\n",
    "            else:\n",
    "                map_precip = np.random.uniform(400, 1500)  # Mixed\n",
    "                mat_temp = np.random.uniform(-2, 15)\n",
    "            \n",
    "            # Derived characteristics\n",
    "            aridity = map_precip / (max(0.1, mat_temp + 5) * 365)  # Aridity index\n",
    "            seasonality = np.random.uniform(0.2, 0.8)  # Precipitation seasonality\n",
    "            \n",
    "            # Forest fraction varies by region\n",
    "            if region['name'] in ['Pacific_Northwest', 'Southeastern_US', 'Northeastern_US']:\n",
    "                forest_frac = np.random.uniform(0.4, 0.9)\n",
    "            else:\n",
    "                forest_frac = np.random.uniform(0.05, 0.4)\n",
    "            \n",
    "            # Scale classification based on area\n",
    "            if area < 100:\n",
    "                scale = 'headwater'\n",
    "            elif area < 1000:\n",
    "                scale = 'meso'\n",
    "            else:\n",
    "                scale = 'macro'\n",
    "            \n",
    "            # Streamflow characteristics\n",
    "            mean_q = area * map_precip * 0.001 * np.random.uniform(0.2, 0.8)  # Rough runoff coefficient\n",
    "            baseflow_index = np.random.uniform(0.2, 0.8)\n",
    "            \n",
    "            # Create watershed entry\n",
    "            watershed = {\n",
    "                'station_id': f\"CAMELS_{watershed_id:04d}\",\n",
    "                'station_name': f\"{region['name']}_Basin_{i+1:03d}\",\n",
    "                'lat': round(lat, 4),\n",
    "                'lon': round(lon, 4),\n",
    "                'drainage_area': round(area, 1),\n",
    "                'elevation_mean': round(elevation, 0),\n",
    "                'p_mean': round(map_precip, 0),  # Mean annual precipitation\n",
    "                't_mean': round(mat_temp, 1),    # Mean annual temperature\n",
    "                'aridity': round(aridity, 3),\n",
    "                'seasonality': round(seasonality, 3),\n",
    "                'forest_frac': round(forest_frac, 3),\n",
    "                'q_mean': round(mean_q, 2),\n",
    "                'baseflow_index': round(baseflow_index, 3),\n",
    "                'scale': scale,\n",
    "                'region': region['name'],\n",
    "                'data_length': np.random.randint(10, 30),  # Years of data\n",
    "                'data_quality': np.random.choice(['excellent', 'good', 'fair'], p=[0.3, 0.5, 0.2])\n",
    "            }\n",
    "            \n",
    "            # Add CONFLUENCE formatting\n",
    "            buffer = 0.05\n",
    "            watershed['BOUNDING_BOX_COORDS'] = f\"{lat + buffer}/{lon - buffer}/{lat - buffer}/{lon + buffer}\"\n",
    "            watershed['POUR_POINT_COORDS'] = f\"{lat}/{lon}\"\n",
    "            watershed['Watershed_Name'] = watershed['station_id'].replace(' ', '_')\n",
    "            \n",
    "            watersheds_data.append(watershed)\n",
    "            watershed_id += 1\n",
    "    \n",
    "    camelsspat_df = pd.DataFrame(watersheds_data)\n",
    "    \n",
    "    # Save demonstration dataset\n",
    "    camelsspat_df.to_csv('camels-spat-metadata.csv', index=False)\n",
    "    print(f\"Created demonstration CAMELS-SPAT dataset: {len(camelsspat_df)} watersheds\")\n",
    "\n",
    "# Display basic dataset information\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"  Total watersheds: {len(camelsspat_df)}\")\n",
    "print(f\"  Columns: {len(camelsspat_df.columns)}\")\n",
    "print(f\"  Column names: {', '.join(camelsspat_df.columns[:8])}...\")\n",
    "\n",
    "\n",
    "\n",
    "camelsspat_df['latitude'] = camelsspat_df['Station_lat']\n",
    "camelsspat_df['longitude'] = camelsspat_df['Station_lon']\n",
    "camelsspat_df['drainage_area'] = camelsspat_df['Ref_shape_area_km2']\n",
    "\n",
    "print(f\"Coordinate extraction successful\")\n",
    "print(f\"  Latitude range: {camelsspat_df['latitude'].min():.1f}Â° to {camelsspat_df['latitude'].max():.1f}Â°N\")\n",
    "print(f\"  Longitude range: {camelsspat_df['longitude'].min():.1f}Â° to {camelsspat_df['longitude'].max():.1f}Â°W\")\n",
    "print(f\"  Drainage area range: {camelsspat_df['drainage_area'].min():.0f} to {camelsspat_df['drainage_area'].max():.0f} kmÂ²\")\n",
    "\n",
    "# =============================================================================\n",
    "# WATERSHED-SPECIFIC DATASET CHARACTERISTICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nAnalyzing Watershed Dataset Characteristics...\")\n",
    "\n",
    "# Area-based watershed scale zones\n",
    "area_zones = [\n",
    "    (0, 100, 'Headwater'),\n",
    "    (100, 1000, 'Meso-scale'),\n",
    "    (1000, 5000, 'Macro-scale'),\n",
    "    (5000, 50000, 'Large-scale')\n",
    "]\n",
    "\n",
    "camelsspat_df['area_class'] = 'Unknown'\n",
    "for min_area, max_area, zone_name in area_zones:\n",
    "    mask = (camelsspat_df['drainage_area'] >= min_area) & (camelsspat_df['drainage_area'] < max_area)\n",
    "    camelsspat_df.loc[mask, 'area_class'] = zone_name\n",
    "\n",
    "area_counts = camelsspat_df['area_class'].value_counts()\n",
    "print(f\"  Watershed scales: {len(area_counts)}\")\n",
    "print(f\"    Most common: {area_counts.index[0]} ({area_counts.iloc[0]} watersheds)\")\n",
    "\n",
    "# Climate-based zones using aridity\n",
    "if 'aridity' in camelsspat_df.columns:\n",
    "    camelsspat_df['climate_class'] = 'Unknown'\n",
    "    camelsspat_df.loc[camelsspat_df['aridity'] < 0.5, 'climate_class'] = 'Arid'\n",
    "    camelsspat_df.loc[(camelsspat_df['aridity'] >= 0.5) & (camelsspat_df['aridity'] < 1.0), 'climate_class'] = 'Semi-arid'\n",
    "    camelsspat_df.loc[(camelsspat_df['aridity'] >= 1.0) & (camelsspat_df['aridity'] < 2.0), 'climate_class'] = 'Sub-humid'\n",
    "    camelsspat_df.loc[camelsspat_df['aridity'] >= 2.0, 'climate_class'] = 'Humid'\n",
    "    \n",
    "    climate_counts = camelsspat_df['climate_class'].value_counts()\n",
    "    print(f\"  Climate zones: {len(climate_counts)}\")\n",
    "    print(f\"    Most common: {climate_counts.index[0]} ({climate_counts.iloc[0]} watersheds)\")\n",
    "\n",
    "# Regional analysis\n",
    "if 'region' in camelsspat_df.columns:\n",
    "    region_counts = camelsspat_df['region'].value_counts()\n",
    "    print(f\"  Regions: {len(region_counts)}\")\n",
    "    print(f\"    Largest region: {region_counts.index[0]} ({region_counts.iloc[0]} watersheds)\")\n",
    "\n",
    "# Climate characteristics\n",
    "if 'p_mean' in camelsspat_df.columns:\n",
    "    precip_stats = camelsspat_df['p_mean'].describe()\n",
    "    print(f\"  Precipitation range: {precip_stats['min']:.0f} to {precip_stats['max']:.0f} mm/yr\")\n",
    "\n",
    "if 't_mean' in camelsspat_df.columns:\n",
    "    temp_stats = camelsspat_df['t_mean'].describe()\n",
    "    print(f\"  Temperature range: {temp_stats['min']:.1f} to {temp_stats['max']:.1f} Â°C\")\n",
    "\n",
    "# Streamflow characteristics\n",
    "if 'q_mean' in camelsspat_df.columns:\n",
    "    flow_stats = camelsspat_df['q_mean'].describe()\n",
    "    print(f\"  Mean streamflow range: {flow_stats['min']:.1f} to {flow_stats['max']:.1f} mÂ³/s\")\n",
    "\n",
    "# =============================================================================\n",
    "# CAMELS-SPAT DATASET VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nCreating CAMELS-SPAT Dataset Overview Visualization...\")\n",
    "\n",
    "# Create comprehensive watershed dataset overview\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. North American watershed distribution map\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(camelsspat_df['longitude'], camelsspat_df['latitude'], \n",
    "                     c=camelsspat_df['drainage_area'], cmap='viridis', \n",
    "                     alpha=0.7, s=40, edgecolors='black', linewidth=0.5, norm=plt.Normalize(vmin=10, vmax=2000))\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'CAMELS-SPAT Watershed Distribution\\n({len(camelsspat_df)} watersheds)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-130, -65)\n",
    "ax1.set_ylim(25, 50)  # Focus on CONUS\n",
    "\n",
    "# Add colorbar for drainage area\n",
    "cbar = plt.colorbar(scatter, ax=ax1)\n",
    "cbar.set_label('Drainage Area (kmÂ²)')\n",
    "\n",
    "# 2. Watershed scale distribution\n",
    "ax2 = axes[1]\n",
    "if 'scale' in camelsspat_df.columns:\n",
    "    scale_counts = camelsspat_df['scale'].value_counts()\n",
    "else:\n",
    "    scale_counts = camelsspat_df['area_class'].value_counts()\n",
    "\n",
    "bars = ax2.bar(range(len(scale_counts)), scale_counts.values, \n",
    "               color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(scale_counts)))\n",
    "ax2.set_xticklabels(scale_counts.index, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Watersheds')\n",
    "ax2.set_title('Watersheds by Scale')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, scale_counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Climate distribution\n",
    "ax3 = axes[2]\n",
    "if 'climate_class' in camelsspat_df.columns:\n",
    "    climate_counts = camelsspat_df['climate_class'].value_counts()\n",
    "    colors = ['brown', 'orange', 'lightgreen', 'blue']\n",
    "    bars = ax3.bar(range(len(climate_counts)), climate_counts.values, \n",
    "                   color=colors[:len(climate_counts)], alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xticks(range(len(climate_counts)))\n",
    "    ax3.set_xticklabels(climate_counts.index, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Number of Watersheds')\n",
    "    ax3.set_title('Watersheds by Climate')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, climate_counts.values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('CAMELS-SPAT Watershed Dataset - Overview', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328d0f5-3310-4565-bb3c-97a0abc44efd",
   "metadata": {},
   "source": [
    "## Step 2: Automated CONFLUENCE Configuration and Batch Processing\n",
    "\n",
    "Building on the dataset analysis and default configuration from Step 1, this step demonstrates automated large sample processing using the `run_watersheds_camelsspat.py` script. This script performs two key functions:\n",
    "\n",
    "**Configuration Generation**: The script reads the CAMELS-Spat database and automatically creates individual CONFLUENCE configuration files for each site. Each configuration is customized with site-specific parameters including domain coordinates, bounding box definitions, and unique identifiers, while maintaining consistent model settings across all basins.\n",
    "\n",
    "**Batch Job Submission**: The script submits SLURM jobs to execute the complete CONFLUENCE workflow for each basin in parallel. Each job processes geographic data, prepares meteorological forcing, processes FLUXNET observations, runs the hydrological model, and generates standardized output files.\n",
    "\n",
    "This automated approach scales CONFLUENCE from single-domain modeling to systematic multi-site analysis across 1000+ watersheds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089af51-b285-48eb-aed0-40c899454844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camelsspat_script_from_notebook():\n",
    "    \"\"\"\n",
    "    Execute the run_watersheds_camelsspat-3.py script from within the notebook\n",
    "    \"\"\"\n",
    "    print(f\"\\nð Executing CAMELS-SPAT Large Sample Streamflow Processing Script...\")\n",
    "    \n",
    "    script_path = \"./run_watersheds_camelsspat-3.py\"\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"â Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"   ð Script location: {script_path}\")\n",
    "    print(f\"   ð¯ Target watersheds: {len(selected_watersheds)} CAMELS-SPAT basins\")\n",
    "    print(f\"   â° Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a process with interactive input automation\n",
    "        process = subprocess.Popen(\n",
    "            ['python', script_path],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Prepare automated responses for the script prompts\n",
    "        automated_inputs = [\n",
    "            'n',  # Don't reload shapefile information\n",
    "            'all',  # Process all scales\n",
    "            str(streamflow_config['max_watersheds']),  # Number of watersheds to process\n",
    "            'y' if not streamflow_config.get('dry_run_mode', False) else 'dry'  # Submit jobs or dry run\n",
    "        ]\n",
    "        \n",
    "        input_string = '\\n'.join(automated_inputs) + '\\n'\n",
    "        \n",
    "        # Send automated responses\n",
    "        stdout, stderr = process.communicate(input=input_string)\n",
    "        \n",
    "        # Print the output\n",
    "        if stdout:\n",
    "            print(\"ð Script Output:\")\n",
    "            for line in stdout.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"â ï¸  Script Warnings/Errors:\")\n",
    "            for line in stderr.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"â CAMELS-SPAT processing script completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"â Script failed with return code: {process.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"â Error running script: {e}\")\n",
    "        return False\n",
    "        \n",
    "# Execute the CAMELS-SPAT processing script\n",
    "script_success = run_camelsspat_script_from_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17b4e1-e1d7-4606-b66a-d783b1404c40",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Basin Streamflow Validation and Regional Analysis\n",
    "Having executed large sample streamflow modeling, we now demonstrate the analytical power that emerges from systematic multi-basin streamflow validation using CAMELS-SPAT observations. This step showcases comprehensive watershed response evaluation, regional performance assessment, and integrated process validationâthe scientific culmination of our entire CONFLUENCE tutorial series.\n",
    "Streamflow Science Evolution: Case Studies â Systematic Watershed Understanding\n",
    "Traditional Streamflow Validation: Individual basin model evaluation and calibration\n",
    "\n",
    "Basin-specific parameter tuning with limited transferability to other watersheds\n",
    "Difficulty separating universal hydrological principles from local environmental effects\n",
    "Manual comparison across different studies and modeling approaches\n",
    "Limited statistical power for robust hydrological process generalization\n",
    "\n",
    "Large Sample Streamflow Validation: Systematic multi-basin hydrological analysis\n",
    "\n",
    "Continental-scale pattern recognition across climate, topography, and scale gradients\n",
    "Statistical hypothesis testing for hydrological process representations with robust sample sizes\n",
    "Process universality assessment distinguishing general vs. basin-specific hydrological behaviors\n",
    "Model transferability evaluation across diverse continental watershed environments\n",
    "\n",
    "Comprehensive Multi-Basin Analysis Framework\n",
    "Tier 1: Watershed Domain Spatial Overview\n",
    "\n",
    "Automated discovery of completed streamflow modeling domains across environmental gradients\n",
    "Processing status assessment including simulation completion, routing success, and observation availability\n",
    "Continental spatial distribution showing streamflow modeling coverage across physiographic regions\n",
    "Scale-based analysis revealing streamflow modeling performance across watershed size gradients\n",
    "\n",
    "Tier 2: Integrated Streamflow Process Validation\n",
    "\n",
    "Hydrograph comparison: Comprehensive streamflow time series validation across diverse watersheds\n",
    "Multi-metric evaluation: Nash-Sutcliffe efficiency, Kling-Gupta efficiency, bias, and correlation assessment\n",
    "Flow signature analysis: Characteristic watershed response patterns and hydrological behavior\n",
    "Seasonal performance evaluation: Assessment across different hydrological seasons and flow conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74a6ab-b842-459d-b5cf-bae05e4fb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_completed_streamflow_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed CAMELS-SPAT domain directories and their streamflow outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\nð Discovering Completed CAMELS-SPAT Streamflow Modeling Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    base_path = Path(streamflow_config['base_data_path'])\n",
    "    domain_pattern = str(base_path / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(domain_pattern)\n",
    "    \n",
    "    print(f\"   ð Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a CAMELS-SPAT domain (should match our selected watersheds)\n",
    "        if any(domain_name.startswith(ws) for ws in selected_watersheds['ID'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"river_basins\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            obs_dir = domain_path / \"observations\" / \"streamflow\" / \"preprocessed\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'has_observations': obs_dir.exists(),\n",
    "                'observation_path': obs_dir if obs_dir.exists() else None,\n",
    "                'simulation_files': [],\n",
    "                'streamflow_obs_file': None\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                # Look for SUMMA outputs\n",
    "                summa_files = list(simulation_dir.glob(\"**/SUMMA/*.nc\"))\n",
    "                # Look for mizuRoute outputs (streamflow routing)\n",
    "                mizuroute_files = list(simulation_dir.glob(\"**/mizuRoute/*.nc\"))\n",
    "                \n",
    "                domain_info['simulation_files'] = summa_files + mizuroute_files\n",
    "                domain_info['has_results'] = len(domain_info['simulation_files']) > 0\n",
    "                domain_info['has_summa'] = len(summa_files) > 0\n",
    "                domain_info['has_routing'] = len(mizuroute_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "                domain_info['has_summa'] = False\n",
    "                domain_info['has_routing'] = False\n",
    "            \n",
    "            # Find observation files\n",
    "            if obs_dir.exists():\n",
    "                streamflow_files = list(obs_dir.glob(\"*streamflow*.csv\"))\n",
    "                if streamflow_files:\n",
    "                    domain_info['streamflow_obs_file'] = streamflow_files[0]\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   ð CAMELS-SPAT domains found: {len(completed_domains)}\")\n",
    "    print(f\"   ð Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   ð Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    print(f\"   ð Domains with routing outputs: {sum(1 for d in completed_domains if d['has_routing'])}\")\n",
    "    print(f\"   ð Domains with observations: {sum(1 for d in completed_domains if d['has_observations'])}\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_streamflow_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all streamflow domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\nðºï¸  Creating Streamflow Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    if len(selected_watersheds) > 0:\n",
    "        ax1.scatter(selected_watersheds['Lon'], selected_watersheds['Lat'], \n",
    "                   c='lightgray', alpha=0.5, s=30, label='Selected watersheds', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in selected_watersheds\n",
    "        site_row = None\n",
    "        for _, row in selected_watersheds.iterrows():\n",
    "            if domain_name.startswith(row['ID']):\n",
    "                site_row = row\n",
    "                break\n",
    "        \n",
    "        if site_row is not None:\n",
    "            lat = site_row['Lat']\n",
    "            lon = site_row['Lon']\n",
    "            \n",
    "            # Color based on completion status\n",
    "            if domain['has_routing'] and domain['has_observations']:\n",
    "                color = 'green'\n",
    "                label = 'Complete with streamflow validation'\n",
    "                marker = 's'\n",
    "                size = 80\n",
    "            elif domain['has_routing']:\n",
    "                color = 'orange' \n",
    "                label = 'Routing complete'\n",
    "                marker = '^'\n",
    "                size = 60\n",
    "            elif domain['has_results']:\n",
    "                color = 'blue'\n",
    "                label = 'Simulation complete'\n",
    "                marker = 'D'\n",
    "                size = 50\n",
    "            else:\n",
    "                color = 'red'\n",
    "                label = 'Processing started'\n",
    "                marker = 'v'\n",
    "                size = 40\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=color, s=size, marker=marker, alpha=0.8,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('CAMELS-SPAT Streamflow Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-130, -60)  # Focus on North America\n",
    "    ax1.set_ylim(25, 55)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='green', s=80, marker='s', label='Complete with validation'),\n",
    "        plt.scatter([], [], c='orange', s=60, marker='^', label='Routing complete'),\n",
    "        plt.scatter([], [], c='blue', s=50, marker='D', label='Simulation complete'),\n",
    "        plt.scatter([], [], c='red', s=40, marker='v', label='Processing started'),\n",
    "        plt.scatter([], [], c='lightgray', s=30, marker='o', label='Selected watersheds')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    # Map 2: Completion statistics by scale\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if len(selected_watersheds) > 0 and 'Scale' in selected_watersheds.columns:\n",
    "        # Create scale-based completion analysis\n",
    "        scale_completion = {}\n",
    "        \n",
    "        for domain in completed_domains:\n",
    "            domain_name = domain['domain_name']\n",
    "            \n",
    "            # Find corresponding watershed\n",
    "            site_row = None\n",
    "            for _, row in selected_watersheds.iterrows():\n",
    "                if domain_name.startswith(row['ID']):\n",
    "                    site_row = row\n",
    "                    break\n",
    "            \n",
    "            if site_row is not None:\n",
    "                scale = site_row['Scale']\n",
    "                \n",
    "                if scale not in scale_completion:\n",
    "                    scale_completion[scale] = {'total': 0, 'complete': 0, 'partial': 0}\n",
    "                \n",
    "                scale_completion[scale]['total'] += 1\n",
    "                \n",
    "                if domain['has_routing'] and domain['has_observations']:\n",
    "                    scale_completion[scale]['complete'] += 1\n",
    "                elif domain['has_results']:\n",
    "                    scale_completion[scale]['partial'] += 1\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        if scale_completion:\n",
    "            scales = list(scale_completion.keys())\n",
    "            complete_counts = [scale_completion[s]['complete'] for s in scales]\n",
    "            partial_counts = [scale_completion[s]['partial'] for s in scales]\n",
    "            pending_counts = [scale_completion[s]['total'] - \n",
    "                             scale_completion[s]['complete'] - \n",
    "                             scale_completion[s]['partial'] for s in scales]\n",
    "            \n",
    "            x_pos = range(len(scales))\n",
    "            \n",
    "            ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.7)\n",
    "            ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "                   label='Partial', color='orange', alpha=0.7)\n",
    "            ax2.bar(x_pos, pending_counts, \n",
    "                   bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "                   label='Pending', color='red', alpha=0.7)\n",
    "            \n",
    "            ax2.set_xticks(x_pos)\n",
    "            ax2.set_xticklabels([s.capitalize() for s in scales])\n",
    "            ax2.set_ylabel('Number of Watersheds')\n",
    "            ax2.set_title('Processing Status by Watershed Scale')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Watershed area vs simulation status\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    domain_areas = []\n",
    "    completion_status = []\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding watershed\n",
    "        site_row = None\n",
    "        for _, row in selected_watersheds.iterrows():\n",
    "            if domain_name.startswith(row['ID']):\n",
    "                site_row = row\n",
    "                break\n",
    "        \n",
    "        if site_row is not None and 'Area_km2' in site_row:\n",
    "            area = site_row['Area_km2']\n",
    "            domain_areas.append(area)\n",
    "            \n",
    "            # Determine completion status\n",
    "            if domain['has_routing'] and domain['has_observations']:\n",
    "                status = 'Complete'\n",
    "                color = 'green'\n",
    "            elif domain['has_routing']:\n",
    "                status = 'Routing'\n",
    "                color = 'orange'\n",
    "            elif domain['has_results']:\n",
    "                status = 'Simulation'\n",
    "                color = 'blue'\n",
    "            else:\n",
    "                status = 'Started'\n",
    "                color = 'red'\n",
    "            \n",
    "            completion_status.append(status)\n",
    "            ax3.scatter(area, len(completion_status), c=color, alpha=0.7, s=60, \n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax3.set_xlabel('Watershed Area (kmÂ²)')\n",
    "    ax3.set_ylabel('Processing Order')\n",
    "    ax3.set_title('Watershed Size vs Processing Status')\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Map 4: Processing summary statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(selected_watersheds) if len(selected_watersheds) > 0 else 0\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    total_with_routing = sum(1 for d in completed_domains if d['has_routing'])\n",
    "    total_with_obs = sum(1 for d in completed_domains if d['has_observations'])\n",
    "    total_complete = sum(1 for d in completed_domains if d['has_routing'] and d['has_observations'])\n",
    "    \n",
    "    categories = ['Selected', 'Processing\\nStarted', 'Simulation\\nComplete', 'Routing\\nComplete', 'Observations\\nAvailable', 'Ready for\\nValidation']\n",
    "    counts = [total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete]\n",
    "    colors = ['lightblue', 'yellow', 'blue', 'orange', 'cyan', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.2,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Watersheds')\n",
    "    ax4.set_title('Streamflow Modeling Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('CAMELS-SPAT Large Sample Streamflow Study - Domain Overview', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'streamflow_domain_overview_map.png'\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"â Streamflow domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete\n",
    "\n",
    "def extract_streamflow_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract streamflow simulation results from all completed domains\n",
    "    \"\"\"\n",
    "    print(f\"\\nð Extracting Streamflow Results from Completed Domains...\")\n",
    "    \n",
    "    streamflow_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_routing': 0,\n",
    "        'domains_with_streamflow': 0,\n",
    "        'failed_extractions': 0\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_routing']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        processing_summary['domains_with_routing'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   ð Processing {domain_name}...\")\n",
    "            \n",
    "            # Find routing output files (mizuRoute)\n",
    "            mizuroute_files = [f for f in domain['simulation_files'] if 'mizuRoute' in str(f)]\n",
    "            \n",
    "            if not mizuroute_files:\n",
    "                print(f\"     â No mizuRoute files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Use the first mizuRoute file\n",
    "            output_file = mizuroute_files[0]\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for streamflow variables\n",
    "            streamflow_vars = {}\n",
    "            \n",
    "            # Common mizuRoute streamflow variable names\n",
    "            potential_vars = ['IRFroutedRunoff', 'routedRunoff', 'discharge', 'streamflow']\n",
    "            \n",
    "            for var in potential_vars:\n",
    "                if var in ds.data_vars:\n",
    "                    streamflow_vars['discharge'] = var\n",
    "                    break\n",
    "            \n",
    "            if not streamflow_vars:\n",
    "                print(f\"     â ï¸  No streamflow variables found in {output_file.name}\")\n",
    "                available_vars = list(ds.data_vars.keys())\n",
    "                print(f\"     Available variables: {available_vars[:5]}...\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"     ð Using streamflow variable: {streamflow_vars['discharge']}\")\n",
    "            \n",
    "            # Extract streamflow data\n",
    "            streamflow_var = streamflow_vars['discharge']\n",
    "            streamflow_data = ds[streamflow_var]\n",
    "            \n",
    "            # Handle multi-dimensional data (time x reaches)\n",
    "            if len(streamflow_data.dims) > 1:\n",
    "                # Find the time dimension\n",
    "                time_dim = 'time'\n",
    "                reach_dims = [dim for dim in streamflow_data.dims if dim != time_dim]\n",
    "                \n",
    "                if reach_dims:\n",
    "                    reach_dim = reach_dims[0]\n",
    "                    # Use the last reach (often the outlet)\n",
    "                    outlet_idx = streamflow_data.sizes[reach_dim] - 1\n",
    "                    streamflow_data = streamflow_data.isel({reach_dim: outlet_idx})\n",
    "                    print(f\"     ð Using outlet reach (index {outlet_idx})\")\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            streamflow_series = streamflow_data.to_pandas()\n",
    "            \n",
    "            # Handle unit conversion if needed (assume mÂ³/s is correct)\n",
    "            # Remove any negative values (set to 0)\n",
    "            streamflow_series = streamflow_series.clip(lower=0)\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = None\n",
    "            for _, row in selected_watersheds.iterrows():\n",
    "                if domain_name.startswith(row['ID']):\n",
    "                    site_row = row\n",
    "                    break\n",
    "            \n",
    "            if site_row is None:\n",
    "                print(f\"     â ï¸  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate streamflow statistics\n",
    "            streamflow_stats = {\n",
    "                'mean_flow': streamflow_series.mean(),\n",
    "                'max_flow': streamflow_series.max(),\n",
    "                'min_flow': streamflow_series.min(),\n",
    "                'std_flow': streamflow_series.std(),\n",
    "                'flow_variability': streamflow_series.std() / streamflow_series.mean() if streamflow_series.mean() > 0 else np.nan\n",
    "            }\n",
    "            \n",
    "            # Calculate flow percentiles\n",
    "            percentiles = [5, 25, 50, 75, 95]\n",
    "            for p in percentiles:\n",
    "                streamflow_stats[f'q{p}'] = streamflow_series.quantile(p/100)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'watershed_id': site_row['ID'],\n",
    "                'latitude': site_row['Lat'],\n",
    "                'longitude': site_row['Lon'],\n",
    "                'area_km2': site_row.get('Area_km2', np.nan),\n",
    "                'scale': site_row.get('Scale', 'unknown'),\n",
    "                'streamflow_timeseries': streamflow_series,\n",
    "                'data_period': f\"{streamflow_series.index.min()} to {streamflow_series.index.max()}\",\n",
    "                'data_points': len(streamflow_series),\n",
    "                'streamflow_variable': streamflow_var,\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            # Add statistics\n",
    "            result.update(streamflow_stats)\n",
    "            \n",
    "            streamflow_results.append(result)\n",
    "            processing_summary['domains_with_streamflow'] += 1\n",
    "            \n",
    "            print(f\"     â Streamflow extracted: {result['mean_flow']:.2f} mÂ³/s (range: {result['min_flow']:.2f}-{result['max_flow']:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     â Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\nð Streamflow Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with routing: {processing_summary['domains_with_routing']}\")\n",
    "    print(f\"   Successful extractions: {processing_summary['domains_with_streamflow']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    return streamflow_results, processing_summary\n",
    "\n",
    "def load_camelsspat_observations(completed_domains):\n",
    "    \"\"\"\n",
    "    Load CAMELS-SPAT observation data for streamflow validation\n",
    "    \"\"\"\n",
    "    print(f\"\\nð¥ Loading CAMELS-SPAT Streamflow Observation Data...\")\n",
    "    \n",
    "    camelsspat_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_streamflow': 0,\n",
    "        'total_observations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for processed CAMELS-SPAT observation data in domain directories\n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_observations']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        try:\n",
    "            print(f\"   ð Loading {domain_name}...\")\n",
    "            \n",
    "            obs_summary['sites_found'] += 1\n",
    "            \n",
    "            # Load streamflow observations\n",
    "            if domain['streamflow_obs_file']:\n",
    "                obs_df = pd.read_csv(domain['streamflow_obs_file'])\n",
    "                \n",
    "                # Find time and discharge columns\n",
    "                time_col = None\n",
    "                for col in ['datetime', 'date', 'time']:\n",
    "                    if col in obs_df.columns:\n",
    "                        time_col = col\n",
    "                        break\n",
    "                \n",
    "                discharge_col = None\n",
    "                for col in ['discharge_cms', 'streamflow', 'flow', 'Q']:\n",
    "                    if col in obs_df.columns:\n",
    "                        discharge_col = col\n",
    "                        break\n",
    "                \n",
    "                if time_col and discharge_col:\n",
    "                    obs_df[time_col] = pd.to_datetime(obs_df[time_col])\n",
    "                    obs_df.set_index(time_col, inplace=True)\n",
    "                    \n",
    "                    streamflow_obs = obs_df[discharge_col].dropna()\n",
    "                    \n",
    "                    if len(streamflow_obs) > 0:\n",
    "                        # Calculate streamflow statistics\n",
    "                        obs_stats = {\n",
    "                            'mean_flow': streamflow_obs.mean(),\n",
    "                            'max_flow': streamflow_obs.max(),\n",
    "                            'min_flow': streamflow_obs.min(),\n",
    "                            'std_flow': streamflow_obs.std(),\n",
    "                            'flow_variability': streamflow_obs.std() / streamflow_obs.mean() if streamflow_obs.mean() > 0 else np.nan\n",
    "                        }\n",
    "                        \n",
    "                        # Calculate flow percentiles\n",
    "                        percentiles = [5, 25, 50, 75, 95]\n",
    "                        for p in percentiles:\n",
    "                            obs_stats[f'q{p}'] = streamflow_obs.quantile(p/100)\n",
    "                        \n",
    "                        # Store observation data\n",
    "                        site_obs = {\n",
    "                            'streamflow_timeseries': streamflow_obs,\n",
    "                            'data_period': f\"{streamflow_obs.index.min()} to {streamflow_obs.index.max()}\",\n",
    "                            'data_points': len(streamflow_obs)\n",
    "                        }\n",
    "                        \n",
    "                        # Add statistics\n",
    "                        site_obs.update(obs_stats)\n",
    "                        \n",
    "                        # Add site metadata\n",
    "                        site_row = None\n",
    "                        for _, row in selected_watersheds.iterrows():\n",
    "                            if domain_name.startswith(row['ID']):\n",
    "                                site_row = row\n",
    "                                break\n",
    "                        \n",
    "                        if site_row is not None:\n",
    "                            site_obs['latitude'] = site_row['Lat']\n",
    "                            site_obs['longitude'] = site_row['Lon']\n",
    "                            site_obs['area_km2'] = site_row.get('Area_km2', np.nan)\n",
    "                            site_obs['scale'] = site_row.get('Scale', 'unknown')\n",
    "                            site_obs['watershed_id'] = site_row['ID']\n",
    "                        \n",
    "                        camelsspat_obs[domain_name] = site_obs\n",
    "                        \n",
    "                        obs_summary['sites_with_streamflow'] += 1\n",
    "                        obs_summary['total_observations'] += len(streamflow_obs)\n",
    "                        \n",
    "                        print(f\"     ð Streamflow obs: {streamflow_obs.mean():.2f} mÂ³/s (range: {streamflow_obs.min():.2f}-{streamflow_obs.max():.2f}) ({len(streamflow_obs)} points)\")\n",
    "                else:\n",
    "                    print(f\"     â ï¸ Could not find time/discharge columns in observation file\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     â Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nð CAMELS-SPAT Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with streamflow observations: {obs_summary['sites_with_streamflow']}\")\n",
    "    print(f\"   Total streamflow observations: {obs_summary['total_observations']}\")\n",
    "    \n",
    "    return camelsspat_obs, obs_summary\n",
    "\n",
    "def create_streamflow_comparison_analysis(streamflow_results, camelsspat_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive streamflow comparison analysis between simulated and observed\n",
    "    \"\"\"\n",
    "    print(f\"\\nð Creating Streamflow Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in streamflow_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in camelsspat_obs:\n",
    "            # Align time periods\n",
    "            sim_flow = sim_result['streamflow_timeseries']\n",
    "            obs_flow = camelsspat_obs[domain_name]['streamflow_timeseries']\n",
    "            \n",
    "            # Find common time period\n",
    "            common_start = max(sim_flow.index.min(), obs_flow.index.min())\n",
    "            common_end = min(sim_flow.index.max(), obs_flow.index.max())\n",
    "            \n",
    "            if common_start < common_end:\n",
    "                # Resample to daily and align\n",
    "                sim_daily = sim_flow.resample('D').mean().loc[common_start:common_end]\n",
    "                obs_daily = obs_flow.resample('D').mean().loc[common_start:common_end]\n",
    "                \n",
    "                # Remove NaN values\n",
    "                valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                sim_valid = sim_daily[valid_mask]\n",
    "                obs_valid = obs_daily[valid_mask]\n",
    "                \n",
    "                if len(sim_valid) > 50:  # Need minimum data for meaningful comparison\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    def calculate_nse(obs, sim):\n",
    "                        return 1 - ((obs - sim) ** 2).sum() / ((obs - obs.mean()) ** 2).sum()\n",
    "                    \n",
    "                    def calculate_kge(obs, sim):\n",
    "                        # Kling-Gupta Efficiency\n",
    "                        r = np.corrcoef(obs, sim)[0, 1]\n",
    "                        alpha = sim.std() / obs.std()\n",
    "                        beta = sim.mean() / obs.mean()\n",
    "                        kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "                        return kge\n",
    "                    \n",
    "                    # Performance metrics\n",
    "                    nse = calculate_nse(obs_valid, sim_valid)\n",
    "                    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                    bias = (sim_valid - obs_valid).mean()\n",
    "                    pbias = 100 * bias / obs_valid.mean() if obs_valid.mean() > 0 else np.nan\n",
    "                    \n",
    "                    # Correlation\n",
    "                    try:\n",
    "                        correlation = obs_valid.corr(sim_valid)\n",
    "                        if pd.isna(correlation):\n",
    "                            correlation = 0.0\n",
    "                    except:\n",
    "                        correlation = 0.0\n",
    "                    \n",
    "                    # KGE\n",
    "                    try:\n",
    "                        kge = calculate_kge(obs_valid.values, sim_valid.values)\n",
    "                        if pd.isna(kge):\n",
    "                            kge = -999\n",
    "                    except:\n",
    "                        kge = -999\n",
    "                    \n",
    "                    common_site = {\n",
    "                        'domain_name': domain_name,\n",
    "                        'watershed_id': sim_result['watershed_id'],\n",
    "                        'latitude': sim_result['latitude'],\n",
    "                        'longitude': sim_result['longitude'],\n",
    "                        'area_km2': sim_result['area_km2'],\n",
    "                        'scale': sim_result['scale'],\n",
    "                        'sim_flow': sim_valid,\n",
    "                        'obs_flow': obs_valid,\n",
    "                        'sim_mean': sim_valid.mean(),\n",
    "                        'obs_mean': obs_valid.mean(),\n",
    "                        'nse': nse,\n",
    "                        'kge': kge,\n",
    "                        'rmse': rmse,\n",
    "                        'bias': bias,\n",
    "                        'pbias': pbias,\n",
    "                        'correlation': correlation,\n",
    "                        'n_points': len(sim_valid),\n",
    "                        'common_period': f\"{common_start.date()} to {common_end.date()}\"\n",
    "                    }\n",
    "                    \n",
    "                    common_sites.append(common_site)\n",
    "                    \n",
    "                    print(f\"   â {domain_name}: NSE={nse:.3f}, KGE={kge:.3f}, r={correlation:.3f}, Bias={bias:+.2f} ({len(sim_valid)} points)\")\n",
    "    \n",
    "    print(f\"\\nð Streamflow Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(\"   â ï¸  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive streamflow comparison visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Scatter plot: Observed vs Simulated (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    all_obs = np.concatenate([site['obs_flow'].values for site in common_sites])\n",
    "    all_sim = np.concatenate([site['sim_flow'].values for site in common_sites])\n",
    "    \n",
    "    ax1.scatter(all_obs, all_sim, alpha=0.3, s=8, c='blue')\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(all_obs.min(), all_sim.min())\n",
    "    max_val = max(all_obs.max(), all_sim.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    ax1.set_xlabel('Observed Streamflow (mÂ³/s)')\n",
    "    ax1.set_ylabel('Simulated Streamflow (mÂ³/s)')\n",
    "    ax1.set_title('All Sites: Simulated vs Observed Streamflow')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add overall statistics\n",
    "    overall_corr = np.corrcoef(all_obs, all_sim)[0,1] if len(all_obs) > 1 else 0\n",
    "    overall_nse = 1 - ((all_obs - all_sim) ** 2).sum() / ((all_obs - all_obs.mean()) ** 2).sum()\n",
    "    overall_bias = np.mean(all_sim - all_obs)\n",
    "    \n",
    "    stats_text = f'r = {overall_corr:.3f}\\nNSE = {overall_nse:.3f}\\nBias = {overall_bias:+.2f}'\n",
    "    ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance by watershed scale (top middle)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    if any('scale' in site for site in common_sites):\n",
    "        scale_stats = {}\n",
    "        for site in common_sites:\n",
    "            scale = site.get('scale', 'unknown')\n",
    "            if scale not in scale_stats:\n",
    "                scale_stats[scale] = {'nse': [], 'kge': [], 'corr': []}\n",
    "            \n",
    "            scale_stats[scale]['nse'].append(site['nse'])\n",
    "            scale_stats[scale]['kge'].append(site['kge'])\n",
    "            scale_stats[scale]['corr'].append(site['correlation'])\n",
    "        \n",
    "        # Plot NSE by scale\n",
    "        scales = list(scale_stats.keys())\n",
    "        nse_means = [np.mean(scale_stats[s]['nse']) for s in scales]\n",
    "        nse_stds = [np.std(scale_stats[s]['nse']) for s in scales]\n",
    "        \n",
    "        x_pos = range(len(scales))\n",
    "        bars = ax2.bar(x_pos, nse_means, yerr=nse_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels([s.capitalize() for s in scales])\n",
    "        ax2.set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "        ax2.set_title('Streamflow Performance by Watershed Scale')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, mean_val in zip(bars, nse_means):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                    f'{mean_val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Performance vs watershed area (top right)\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    areas = [site['area_km2'] for site in common_sites if not np.isnan(site['area_km2'])]\n",
    "    nses = [site['nse'] for site in common_sites if not np.isnan(site['area_km2'])]\n",
    "    \n",
    "    if areas and nses:\n",
    "        scatter3 = ax3.scatter(areas, nses, alpha=0.7, s=40, c='green')\n",
    "        ax3.set_xlabel('Watershed Area (kmÂ²)')\n",
    "        ax3.set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "        ax3.set_title('Performance vs Watershed Size')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xscale('log')\n",
    "        ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax3.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='NSE = 0.5')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # Bias distribution (bottom left)\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    biases = [site['bias'] for site in common_sites]\n",
    "    ax4.hist(biases, bins=15, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "    ax4.set_xlabel('Bias (mÂ³/s)')\n",
    "    ax4.set_ylabel('Number of Watersheds')\n",
    "    ax4.set_title('Distribution of Streamflow Bias')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # KGE vs NSE comparison (bottom middle)\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    nse_vals = [site['nse'] for site in common_sites]\n",
    "    kge_vals = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "    \n",
    "    if len(kge_vals) > 0:\n",
    "        ax5.scatter(nse_vals[:len(kge_vals)], kge_vals, alpha=0.7, s=40, c='purple')\n",
    "        ax5.set_xlabel('Nash-Sutcliffe Efficiency')\n",
    "        ax5.set_ylabel('Kling-Gupta Efficiency')\n",
    "        ax5.set_title('NSE vs KGE Performance')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add reference lines\n",
    "        ax5.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax5.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax5.plot([-1, 1], [-1, 1], 'k--', alpha=0.3, label='1:1 line')\n",
    "        ax5.legend()\n",
    "    \n",
    "    # Performance summary (bottom right)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Create performance categories\n",
    "    perf_categories = {\n",
    "        'Excellent (NSE > 0.75)': len([s for s in common_sites if s['nse'] > 0.75]),\n",
    "        'Good (0.5 < NSE â¤ 0.75)': len([s for s in common_sites if 0.5 < s['nse'] <= 0.75]),\n",
    "        'Satisfactory (0.2 < NSE â¤ 0.5)': len([s for s in common_sites if 0.2 < s['nse'] <= 0.5]),\n",
    "        'Unsatisfactory (NSE â¤ 0.2)': len([s for s in common_sites if s['nse'] <= 0.2])\n",
    "    }\n",
    "    \n",
    "    categories = list(perf_categories.keys())\n",
    "    counts = list(perf_categories.values())\n",
    "    colors = ['darkgreen', 'green', 'yellow', 'red']\n",
    "    \n",
    "    bars = ax6.bar(range(len(categories)), counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax6.set_xticks(range(len(categories)))\n",
    "    ax6.set_xticklabels([c.split('(')[0].strip() for c in categories], rotation=45, ha='right')\n",
    "    ax6.set_ylabel('Number of Watersheds')\n",
    "    ax6.set_title('Performance Category Distribution')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('CAMELS-SPAT Large Sample Streamflow Comparison Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'streamflow_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"â Streamflow comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    # Create spatial performance map\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Map 1: NSE spatial distribution\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites]\n",
    "    lons = [site['longitude'] for site in common_sites]\n",
    "    nse_values = [site['nse'] for site in common_sites]\n",
    "    \n",
    "    scatter1 = ax1.scatter(lons, lats, c=nse_values, cmap='RdYlGn', s=100, \n",
    "                          vmin=-0.5, vmax=1.0, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Streamflow Model Performance: NSE')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-130, -60)\n",
    "    ax1.set_ylim(25, 55)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "    cbar1.set_label('Nash-Sutcliffe Efficiency')\n",
    "    \n",
    "    # Map 2: Bias spatial distribution\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    bias_values = [site['bias'] for site in common_sites]\n",
    "    max_abs_bias = max(abs(min(bias_values)), abs(max(bias_values)))\n",
    "    \n",
    "    scatter2 = ax2.scatter(lons, lats, c=bias_values, cmap='RdBu_r', s=100,\n",
    "                          vmin=-max_abs_bias, vmax=max_abs_bias, \n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title('Streamflow Model Performance: Bias (Sim - Obs)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-130, -60)\n",
    "    ax2.set_ylim(25, 55)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "    cbar2.set_label('Bias (mÂ³/s)')\n",
    "    \n",
    "    plt.suptitle('CAMELS-SPAT Large Sample Streamflow Performance - Spatial Distribution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save spatial analysis\n",
    "    spatial_path = experiment_dir / 'plots' / 'streamflow_spatial_performance.png'\n",
    "    plt.savefig(spatial_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"â Streamflow spatial performance map saved: {spatial_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\nð Step 3.1: Streamflow Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_streamflow_domains()\n",
    "\n",
    "# Create domain overview map\n",
    "if len(completed_domains) > 0:\n",
    "    total_selected, total_discovered, total_with_results, total_with_routing, total_with_obs, total_complete = create_streamflow_domain_overview_map(completed_domains)\n",
    "else:\n",
    "    print(\"   â ï¸ No completed domains found for overview map\")\n",
    "    total_selected = len(selected_watersheds) if 'selected_watersheds' in locals() else 0\n",
    "    total_discovered = total_with_results = total_with_routing = total_with_obs = total_complete = 0\n",
    "\n",
    "print(f\"\\nð Step 3.2: Streamflow Results Extraction\")\n",
    "\n",
    "# Extract streamflow results from simulations\n",
    "if len(completed_domains) > 0:\n",
    "    streamflow_results, streamflow_processing_summary = extract_streamflow_results_from_domains(completed_domains)\n",
    "    \n",
    "    # Load CAMELS-SPAT observations\n",
    "    camelsspat_obs, obs_summary = load_camelsspat_observations(completed_domains)\n",
    "else:\n",
    "    print(\"   â ï¸ No completed domains available for analysis\")\n",
    "    streamflow_results = []\n",
    "    camelsspat_obs = {}\n",
    "    streamflow_processing_summary = {'domains_with_streamflow': 0}\n",
    "    obs_summary = {'sites_with_streamflow': 0}\n",
    "\n",
    "print(f\"\\nð Step 3.3: Streamflow Comparison Analysis\")\n",
    "\n",
    "# Create streamflow comparison analysis\n",
    "if streamflow_results and camelsspat_obs:\n",
    "    common_sites = create_streamflow_comparison_analysis(streamflow_results, camelsspat_obs)\n",
    "else:\n",
    "    print(\"   â ï¸  Insufficient data for streamflow comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final summary report\n",
    "print(f\"\\nð Creating Final CAMELS-SPAT Streamflow Study Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'camelsspat_final_report.txt'\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"CAMELS-SPAT Large Sample Streamflow Study - Final Analysis Report\\n\")\n",
    "    f.write(\"=\" * 68 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PROCESSING SUMMARY:\\n\")\n",
    "    f.write(f\"  Watersheds selected for analysis: {total_selected}\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\n\")\n",
    "    f.write(f\"  Routing outputs available: {total_with_routing}\\n\")\n",
    "    f.write(f\"  Observations available: {total_with_obs}\\n\")\n",
    "    f.write(f\"  Complete streamflow validation: {total_complete}\\n\")\n",
    "    f.write(f\"  Streamflow extractions successful: {streamflow_processing_summary['domains_with_streamflow']}\\n\")\n",
    "    f.write(f\"  CAMELS-SPAT observations available: {obs_summary['sites_with_streamflow']}\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\n\\n\")\n",
    "        \n",
    "        # Streamflow performance summary\n",
    "        nse_values = [site['nse'] for site in common_sites]\n",
    "        kge_values = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "        bias_values = [site['bias'] for site in common_sites]\n",
    "        corr_values = [site['correlation'] for site in common_sites]\n",
    "        \n",
    "        f.write(\"STREAMFLOW PERFORMANCE SUMMARY:\\n\")\n",
    "        f.write(f\"  Mean NSE: {np.mean(nse_values):.3f} Â± {np.std(nse_values):.3f}\\n\")\n",
    "        if kge_values:\n",
    "            f.write(f\"  Mean KGE: {np.mean(kge_values):.3f} Â± {np.std(kge_values):.3f}\\n\")\n",
    "        f.write(f\"  Mean correlation: {np.mean(corr_values):.3f} Â± {np.std(corr_values):.3f}\\n\")\n",
    "        f.write(f\"  Mean bias: {np.mean(bias_values):+.2f} Â± {np.std(bias_values):.2f} mÂ³/s\\n\\n\")\n",
    "        \n",
    "        # Performance categories\n",
    "        excellent = len([s for s in common_sites if s['nse'] > 0.75])\n",
    "        good = len([s for s in common_sites if 0.5 < s['nse'] <= 0.75])\n",
    "        satisfactory = len([s for s in common_sites if 0.2 < s['nse'] <= 0.5])\n",
    "        unsatisfactory = len([s for s in common_sites if s['nse'] <= 0.2])\n",
    "        \n",
    "        f.write(\"PERFORMANCE CATEGORIES:\\n\")\n",
    "        f.write(f\"  Excellent (NSE > 0.75): {excellent} watersheds\\n\")\n",
    "        f.write(f\"  Good (0.5 < NSE â¤ 0.75): {good} watersheds\\n\")\n",
    "        f.write(f\"  Satisfactory (0.2 < NSE â¤ 0.5): {satisfactory} watersheds\\n\")\n",
    "        f.write(f\"  Unsatisfactory (NSE â¤ 0.2): {unsatisfactory} watersheds\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING WATERSHEDS (by NSE):\\n\")\n",
    "        sorted_sites = sorted(common_sites, key=lambda x: x['nse'], reverse=True)\n",
    "        for i, site in enumerate(sorted_sites[:5]):\n",
    "            f.write(f\"  {i+1}. {site['watershed_id']}: NSE={site['nse']:.3f}, KGE={site['kge']:.3f}, Area={site['area_km2']:.0f} kmÂ²\\n\")\n",
    "\n",
    "print(f\"â Final summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\nð Step 3 Complete: CAMELS-SPAT Streamflow Validation Analysis\")\n",
    "print(f\"   ð Results saved to: {experiment_dir}\")\n",
    "print(f\"   ð Streamflow domain overview: {total_complete}/{total_selected} watersheds with complete validation\")\n",
    "\n",
    "if common_sites:\n",
    "    nse_values = [site['nse'] for site in common_sites]\n",
    "    kge_values = [site['kge'] for site in common_sites if site['kge'] != -999]\n",
    "    \n",
    "    print(f\"   ð Streamflow analysis: {len(common_sites)} watersheds with sim/obs comparison\")\n",
    "    print(f\"   ð NSE performance: Mean = {np.mean(nse_values):.3f}\")\n",
    "    if kge_values:\n",
    "        print(f\"   ð KGE performance: Mean = {np.mean(kge_values):.3f}\")\n",
    "else:\n",
    "    print(f\"   ð Performance: Awaiting more simulation results\")\n",
    "\n",
    "print(f\"\\nâ Large Sample CAMELS-SPAT Streamflow Analysis Complete!\")\n",
    "print(f\"   ð Multi-basin streamflow hydrology validation achieved\")\n",
    "print(f\"   ð Statistical patterns identified across continental watershed gradients\")\n",
    "print(f\"   ðï¸ Tutorial series culmination: Point â Watershed â Continental â Multi-site analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041626d9-f819-456a-a159-b8fed3e9ce0b",
   "metadata": {},
   "source": [
    "# Tutorial 04c Summary: CAMELS Large Sample Streamflow Study\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates large sample streamflow modeling across multiple watersheds using the CAMELS-SPAT dataset. It represents the culmination of the CONFLUENCE tutorial series, advancing from point-scale process validation to integrated watershed-scale hydrological analysis across diverse environmental gradients.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will configure CONFLUENCE for multi-basin streamflow analysis using standardized watershed data, execute systematic streamflow modeling across environmental gradients, validate simulated streamflow against observed discharge using multiple performance metrics, analyze regional patterns in model performance, and demonstrate workflow automation for large sample hydrological studies.\n",
    "\n",
    "## Tutorial Structure\n",
    "\n",
    "The tutorial begins with analysis of the CAMELS-SPAT watershed database covering diverse North American basins and creation of standardized CONFLUENCE configuration templates. The second component involves automated multi-basin processing execution across selected watersheds, demonstrating batch processing capabilities and monitoring systems. The final section focuses on streamflow validation through extraction and comparison of simulated versus observed time series, calculation of performance metrics including Nash-Sutcliffe efficiency and Kling-Gupta efficiency, spatial performance mapping, and comprehensive analysis reporting.\n",
    "\n",
    "## Scientific Significance\n",
    "\n",
    "This tutorial addresses fundamental questions in watershed hydrology including controls on streamflow generation, model transferability across environmental gradients, and systematic evaluation of hydrological process representations. The large sample approach enables robust statistical analysis and identification of universal versus site-specific hydrological behaviors, contributing to improved understanding of continental-scale watershed function and enhanced predictive capabilities for water resources management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
