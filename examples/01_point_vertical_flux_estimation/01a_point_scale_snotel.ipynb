{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial 1a â€” Point-Scale Workflow (Paradise SNOTEL)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the point-scale modeling workflow in **CONFLUENCE**, a framework for reproducible and modular computational hydrology. At the point scale, we simulate vertical energy and water fluxes at a single site, independent of routing or lateral flow, to isolate and evaluate model process representations.\n",
    "\n",
    "Here, we focus on the **Paradise SNOTEL station (ID 602)**, located at 1,630 m elevation in Washingtonâ€™s Cascade Range. This site represents a transitional snow climate and provides long-term observations of snow water equivalent (SWE) and soil moisture across multiple depths. By reproducing the observed seasonal snow and soil moisture dynamics, this tutorial demonstrates how CONFLUENCE structures a controlled, transparent, and fully reproducible point-scale experiment.\n",
    "\n",
    "Through this example, you will see how configuration-driven workflows manage experiment setup, geospatial definition, input data preprocessing, model instantiation, and performance evaluationâ€”building a foundation for more complex distributed modeling studies later in the series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 â€” Configuration (pick or generate)\n",
    "\n",
    "We begin by selecting (or programmatically generating) a single configuration file that fully specifies the experiment. This keeps the workflow reproducible and makes initialization a one-liner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 â€” Create a site-specific configuration for the Paradise SNOTEL example\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Path to the default template configuration\n",
    "config_template = Path(\"../0_config_files/config_point_template.yaml\")\n",
    "\n",
    "# Load the base configuration\n",
    "with open(config_template, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify key entries for the Paradise SNOTEL point-scale case ===\n",
    "\n",
    "# Define code directory â€” ensures relative paths resolve correctly\n",
    "config[\"CONFLUENCE_CODE_DIR\"] = str(Path(\"../\").resolve())\n",
    "\n",
    "# Define data directory â€” location of required input and observational data\n",
    "config[\"CONFLUENCE_DATA_DIR\"] = str(Path(\"/path/to/CONFLUENCE_data\").resolve())\n",
    "\n",
    "# Restrict the spatial domain to a single site using latitude/longitude bounds\n",
    "# This ensures domain setup treats it as a point-scale (non-routed) experiment\n",
    "config[\"DOMAIN_DEFINITION_METHOD\"] = \"point\"\n",
    "config[\"BOUNDING_BOX_COORDS\"] = \"46.781/-121.751/46.779/-121.749\"\n",
    "config[\"POUR_POINT_COORDS\"] = \"46.78/-121.75\"\n",
    "\n",
    "# Enable automatic download of SNOTEL data for this station\n",
    "config[\"DOWNLOAD_SNOTEL\"] = True\n",
    "\n",
    "# Specify model and forcing dataset used in this example\n",
    "config[\"HYDROLOGICAL_MODEL\"] = \"SUMMA\"     # SUMMA is the process-based model used here\n",
    "config[\"FORCING_DATASET\"] = \"ERA5\"         # ERA5 reanalysis for meteorological inputs\n",
    "\n",
    "# Define the temporal extent of the experiment\n",
    "config[\"EXPERIMENT_TIME_START\"] = \"2000-01-01 01:00\"\n",
    "config[\"EXPERIMENT_TIME_END\"] = \"2002-12-31 23:00\"\n",
    "\n",
    "# Assign a descriptive domain name and experiment ID\n",
    "config[\"DOMAIN_NAME\"] = \"paradise\"\n",
    "\n",
    "# === Save the customized configuration ===\n",
    "out_config = Path(\"../0_config_files/config_paradise.yaml\")\n",
    "with open(out_config, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ… New configuration written to: {out_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b â€” Initialize CONFLUENCE\n",
    "\n",
    "With the configuration prepared, we now initialize **CONFLUENCE**.  \n",
    "This step reads the configuration file, sets up the project directory, and registers all workflow managers (data, domain, model, and evaluation).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b â€” Initialize CONFLUENCE\n",
    "\n",
    "from confluence import CONFLUENCE  # adjust if your import path differs\n",
    "\n",
    "config_path = \"../0_config_files/config_paradise.yaml\"\n",
    "confluence = CONFLUENCE(config_path)\n",
    "\n",
    "print(\"âœ… CONFLUENCE initialized successfully.\")\n",
    "print(f\"Configuration loaded from: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c â€” Project structure setup\n",
    "\n",
    "We now create the standardized project directory and a pour-point feature for the site.  \n",
    "This anchors the experiment in a clear, reproducible file layout and records the site location for downstream domain and data steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c â€” Project structure setup\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Create the standardized project layout (logs, config link, data/output folders, etc.)\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# 2) Create a pour-point feature (the site reference geometry for point-scale workflows)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(\"âœ… Project structure created.\")\n",
    "print(f\"Project root: {project_dir}\")\n",
    "print(f\"Pour point:   {pour_point_path}\")\n",
    "\n",
    "# 3) Brief top-level directory preview\n",
    "print(\"\\nTop-level structure:\")\n",
    "for p in sorted(Path(project_dir).iterdir()):\n",
    "    if p.is_dir():\n",
    "        print(f\"â”œâ”€â”€ {p.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure Creation and Organization\n",
    "We can now use CONFLUENCE to setup a project directory for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project structure\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "print(f\"\\nðŸ“ Project root created: {project_dir}\")\n",
    "\n",
    "# Create spatial reference point (SNOTEL station location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"ðŸ“ Pour point created: {pour_point_path}\")\n",
    "print(f\"   â†’ Location: {config_dict['POUR_POINT_COORDS']} (Paradise SNOTEL)\")\n",
    "\n",
    "# Display the created directory structure\n",
    "print(f\"\\n=== Standardized Directory Structure ===\")\n",
    "\n",
    "def display_directory_tree(path, prefix=\"\", max_depth=2, current_depth=0):\n",
    "    \"\"\"Display directory tree with scientific context\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted([item for item in path.iterdir() if item.is_dir()])\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        # Add scientific context for key directories\n",
    "        if item.name == \"forcing\":\n",
    "            print(f\"{prefix}{'    ' if is_last else 'â”‚   '}    â†’ Meteorological input data\")\n",
    "        elif item.name == \"observations\": \n",
    "            print(f\"{prefix}{'    ' if is_last else 'â”‚   '}    â†’ Validation datasets (SNOTEL, streamflow)\")\n",
    "        elif item.name == \"simulations\":\n",
    "            print(f\"{prefix}{'    ' if is_last else 'â”‚   '}    â†’ Model output organized by experiment\")\n",
    "        elif item.name == \"attributes\":\n",
    "            print(f\"{prefix}{'    ' if is_last else 'â”‚   '}    â†’ Geospatial characteristics (elevation, soil, land cover)\")\n",
    "        elif item.name == \"shapefiles\":\n",
    "            print(f\"{prefix}{'    ' if is_last else 'â”‚   '}    â†’ Spatial domains and discretization\")\n",
    "            \n",
    "        if current_depth < max_depth - 1:\n",
    "            extension = \"    \" if is_last else \"â”‚   \"\n",
    "            display_directory_tree(item, prefix + extension, max_depth, current_depth + 1)\n",
    "\n",
    "display_directory_tree(project_dir, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Geospatial Domain Definition and Spatial Discretization\n",
    "\n",
    "## Scientific Context\n",
    "Spatial representation is fundamental to hydrological modeling, determining how we conceptualize the landscape and partition it into computational units. The choice of spatial discretization profoundly affects:\n",
    "\n",
    "- Process Representation: How we capture spatial heterogeneity in climate, topography, vegetation, and soils\n",
    "- Model Complexity: The trade-off between process detail and computational efficiency\n",
    "- Scale Dependencies: How processes manifest differently at point, hillslope, and watershed scales\n",
    "- Validation Strategy: What observations are appropriate for model evaluation\n",
    "\n",
    "For point-scale modeling, we deliberately minimize spatial complexity to isolate vertical processes. This creates a controlled environment where energy and water balance physics can be evaluated without the confounding effects of lateral flow, spatial heterogeneity, or routing processes.\n",
    "\n",
    "The spatial representation we establish here contrasts with the distributed watersheds we'll explore in Tutorial 2, where complex topography drives spatial patterns in precipitation, radiation, and runoff generation.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE handles spatial domain definition through three components:\n",
    "\n",
    "- Attribute Acquisition: Systematic collection of the geospatial characteristics we need to configure our hydrological models (elevation, soil properties, land cover) using standardized datasets\n",
    "- Domain Delineation: Creation of the primary computational boundary (Grouped Response Units - GRUs)\n",
    "- Domain Discretization: Subdivision into Hydrologic Response Units (HRUs) based on landscape similarity\n",
    "\n",
    "For point-scale studies, this process creates a minimal spatial representation:\n",
    "\n",
    "- Bounding Box: 0.001Â° Ã— 0.001Â° square centered on station coordinates\n",
    "- Single GRU: One computational unit representing the station footprint\n",
    "- Single HRU: No further subdivision needed for point-scale physics\n",
    "\n",
    "The same framework scales seamlessly from this minimal representation to complex distributed watersheds with hundreds of HRUs as we'll explore in Tutorials 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Geospatial Attribute Acquisition\n",
    "Attributes provide the physical characteristics needed to parameterize model physics. Even for point-scale modeling, we need elevation, soil properties, and vegetation characteristics to constrain energy and water balance processes.\n",
    "\n",
    "CONFLUENCE uses [gistool (Keshavaraz et al., 2025](https://github.com/CH-Earth/gistool) to subset and aquire the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire geospatial attributes\n",
    "print(f\"\\nâ¬‡ï¸  Acquiring attributes through gistool (Model Agnostic Framework)...\")\n",
    "print(\"   â†’ This may take several minutes \")\n",
    "\n",
    "# confluence.managers['data'].acquire_attributes()\n",
    "\n",
    "print(\"âœ… Attribute acquisition complete\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Domain Delineation (GRU Creation)\n",
    "Domain delineation creates the primary computational boundary. For point-scale modeling, this is simply a geometric square around our station location, but the same process scales to complex watershed delineation for distributed modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating primary computational boundary...\")\n",
    "\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "print(f\"âœ… Domain delineation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c: Domain Discretization (HRU Creation)\n",
    "Discretization subdivides GRUs into Hydrologic Response Units based on landscape similarity. For point-scale modeling, we maintain a 1:1 relationship (1 GRU = 1 HRU), but this step demonstrates the framework that enables more complex spatial representations which we will encounter in later notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Creating Hydrologic Response Units for model execution...\")\n",
    "\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "print(f\"âœ… Domain discretization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "hru_path = confluence.project_dir / \"shapefiles\" / \"catchment\" / f\"{config_dict['DOMAIN_NAME']}_HRUs_{config_dict['DOMAIN_DISCRETIZATION']}.shp\"\n",
    "hru_gdf = gpd.read_file(hru_path)\n",
    "\n",
    "# Create simple plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot HRU\n",
    "hru_gdf.plot(ax=ax, facecolor='lightblue', edgecolor='blue', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add station point\n",
    "station_coords = config_dict['POUR_POINT_COORDS'].split('/')\n",
    "station_lat, station_lon = float(station_coords[0]), float(station_coords[1])\n",
    "ax.scatter(station_lon, station_lat, c='red', s=100, marker='*', \n",
    "          label=f'Paradise SNOTEL\\n({station_lat:.4f}, {station_lon:.4f})', zorder=5)\n",
    "\n",
    "# Styling\n",
    "ax.set_title(f'Point-Scale Domain: {config_dict[\"DOMAIN_NAME\"].title()}', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Longitude (Â°)', fontsize=12)\n",
    "ax.set_ylabel('Latitude (Â°)', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add annotation\n",
    "ax.text(0.02, 0.98, f\"Area: ~0.012 kmÂ²\\n1 GRU, 1 HRU\\nPoint-scale representation\", \n",
    "       transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "       bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Spatial visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Input Data Preprocessing and Model-Agnostic Framework\n",
    "## Scientific Context\n",
    "Input data preprocessing represents a critical but often overlooked component of hydrological modeling that profoundly affects model performance and scientific conclusions. Traditional approaches tightly couple data preprocessing with specific models, creating several scientific and practical challenges:\n",
    "\n",
    "- Model Comparison Barriers: Different preprocessing approaches make it difficult to determine whether performance differences arise from model physics or data preparation\n",
    "- Reproducibility Issues: Model-specific preprocessing pipelines are often poorly documented and difficult to reproduce\n",
    "- Research Inefficiency: Duplicated preprocessing effort across modeling studies\n",
    "- Benchmarking Limitations: Inability to evaluate models against consistent baselines\n",
    "\n",
    "The Model-Agnostic Framework (MAF) philosophy addresses these challenges by separating data preparation from model execution, creating a standardized pipeline that serves multiple modeling applications.\n",
    "\n",
    "## CONFLUENCE Implementation Philosophy\n",
    "CONFLUENCE implements a two-stage preprocessing architecture:\n",
    "Stage 1: Model-Agnostic Preprocessing\n",
    "\n",
    "- Standardized Data Sources: Consistent meteorological and geospatial datasets\n",
    "- Unified Spatial Framework: Common geospatial operations across all models\n",
    "- Quality-Controlled Outputs: Standardized formats with documented provenance\n",
    "- Reusable Products: Same preprocessed data serves multiple models and analyses\n",
    "\n",
    "Stage 2: Model-Specific Preprocessing\n",
    "\n",
    "- Format Translation: Convert standardized outputs to model-required formats\n",
    "- Model Configuration: Apply model-specific parameter assignments and settings\n",
    "- Initialization: Prepare model-specific initial conditions and control files\n",
    "\n",
    "This separation enables true model intercomparison studies, automated benchmarking, and scalable research workflows that maintain scientific rigor while maximizing efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Meteorological Forcing Data Acquisition\n",
    "Meteorological forcing drives all hydrological models, making standardized acquisition critical for reproducible research. CONFLUENCE leverages the Model-Agnostic Framework's [datatool (Keshavarz et al., 2025)](https://github.com/CH-Earth/datatool) to access quality-controlled, globally-consistent datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acquiring standardized meteorological forcing through datatool...\")\n",
    "\n",
    "# confluence.managers['data'].acquire_forcings()\n",
    "\n",
    "print(\"âœ… Forcing data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Observational Data Processing\n",
    "Observational data provides the ground truth for model evaluation. CONFLUENCE systematically acquires and processes multiple observation types, creating standardized validation datasets that support comprehensive model assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute observational data processing\n",
    "print(f\"\\nðŸ“¥ Processing observational datasets...\")\n",
    "confluence.managers['data'].process_observed_data()\n",
    "\n",
    "print(\"âœ… Observational data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Model-Agnostic Preprocessing Pipeline\n",
    "The model-agnostic preprocessing represents the core innovation of CONFLUENCE's data management philosophy. This stage creates standardized, model-independent data products that serve as the foundation for all subsequent modeling activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nâš™ï¸  Executing model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"âœ… Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: Model-Specific Preprocessing\n",
    "Model-specific preprocessing translates the standardized model-agnostic products into the formats and configurations required by individual models. This stage maintains the scientific benefits of standardized inputs while accommodating diverse model requirements.\n",
    "\n",
    "Remapping of the forcing data and zonal statistics calculations for the geospatial attributes is performed in one model-agnostic pre-processing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nðŸ”§ Executing SUMMA-specific preprocessing...\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"âœ… Model-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Instantiation and Process-Based Simulation\n",
    "## Scientific Context\n",
    "Model instantiation represents the critical transition from static data preparation to dynamic process simulation. This step transforms spatially-distributed inputs and temporally-varying forcing into evolving hydrological states through the explicit representation of physical processes.\n",
    "In process-based hydrological modeling, we solve coupled differential equations representing:\n",
    "\n",
    "- Energy Balance: Net radiation partitioning between sensible, latent, and ground heat fluxes\n",
    "- Water Balance: Precipitation partitioning among interception, infiltration, evapotranspiration, and runoff\n",
    "- Snow Physics: Accumulation, metamorphism, and melt processes with explicit energy considerations\n",
    "- Soil Hydrology: Infiltration, redistribution, and drainage through layered soil profiles\n",
    "- Vegetation Dynamics: Canopy interception, transpiration, and phenological controls\n",
    "\n",
    "The SUMMA (Structure for Unifying Multiple Modeling Alternatives) framework enables systematic evaluation of process representations, making it ideal for scientific hypothesis testing and model physics assessment.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE manages model execution through several integrated components:\n",
    "\n",
    "- Workflow Orchestration: Automated sequencing of model initialization, spinup, and main simulation\n",
    "- Configuration Management: Translation of scientific decisions into model-specific control files\n",
    "- Execution Monitoring: Real-time tracking of model progress and error detection\n",
    "- Output Organization: Systematic storage and cataloging of simulation results\n",
    "- Quality Assurance: Automated checks for mass balance closure and physical realism\n",
    "\n",
    "This framework ensures that model execution is reproducible, traceable, and scientifically rigorous, while handling the computational complexity behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation and Process Validation\n",
    "## Scientific Context\n",
    "Model evaluation represents the critical test of whether our process-based simulation captures the essential physics of the real-world system. Effective evaluation requires:\n",
    "\n",
    "- Multi-Variable Assessment: Testing multiple aspects of the hydrological system to avoid equifinality and ensure robust process representation\n",
    "- Temporal Pattern Analysis: Evaluating both magnitude and timing of hydrological responses across seasonal cycles\n",
    "- Process-Specific Metrics: Using evaluation criteria that reflect the underlying physics being tested\n",
    "- Uncertainty Quantification: Understanding both observational and model uncertainty in performance assessment\n",
    "\n",
    "For point-scale modeling, we focus on direct process validation where observations closely match the spatial and temporal scales of model representation. The Paradise SNOTEL station provides co-located snow water equivalent and multi-depth soil moisture observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "sim_dir = confluence.project_dir / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\"\n",
    "daily_output_path = sim_dir / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Load and prepare the evaluation dataset\n",
    "ds = xr.open_dataset(daily_output_path)\n",
    "\n",
    "# Skip spinup period\n",
    "start_year = ds.time.dt.year.min().values + 1\n",
    "spinup_end = f\"{start_year}-01-01\"\n",
    "time_mask = ds.time >= pd.to_datetime(spinup_end)\n",
    "evaluation_data = ds.isel(time=time_mask)\n",
    "\n",
    "# Load observed SWE data\n",
    "obs_swe_path = confluence.project_dir / \"observations\" / \"snow\" / \"snotel\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'], dayfirst=True)\n",
    "obs_swe.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure proper datetime index\n",
    "if not isinstance(obs_swe.index, pd.DatetimeIndex):\n",
    "    obs_swe.index = pd.to_datetime(obs_swe.index)\n",
    "\n",
    "# Extract simulated SWE\n",
    "sim_swe = evaluation_data['scalarSWE'].to_pandas()\n",
    "\n",
    "# Find common period and align data\n",
    "start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "\n",
    "# Resample to daily and filter to common period\n",
    "obs_daily = obs_swe.resample('D').mean().loc[start_date:end_date]\n",
    "sim_daily = sim_swe.resample('D').mean().loc[start_date:end_date]\n",
    "\n",
    "# Handle different column names for SWE\n",
    "if 'SWE' in obs_daily.columns:\n",
    "    obs_values = obs_daily['SWE']\n",
    "elif 'swe' in obs_daily.columns:\n",
    "    obs_values = obs_daily['swe']\n",
    "else:\n",
    "    # Use first column\n",
    "    obs_values = obs_daily.iloc[:, 0]\n",
    "\n",
    "# Convert sim_daily to Series if it's a DataFrame\n",
    "if isinstance(sim_daily, pd.DataFrame):\n",
    "    if len(sim_daily.columns) == 1:\n",
    "        sim_daily = sim_daily.iloc[:, 0]  # Extract the single column as Series\n",
    "\n",
    "# Remove NaN values for metrics calculation\n",
    "valid_mask = ~(obs_values.isna() | sim_daily.isna())\n",
    "obs_valid = obs_values[valid_mask]\n",
    "sim_valid = sim_daily[valid_mask]\n",
    "\n",
    "print(f\"\\nðŸ“Š Snow Water Equivalent Performance Metrics:\")\n",
    "\n",
    "# Basic statistics\n",
    "rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "bias = (sim_valid - obs_valid).mean()\n",
    "mae = np.abs(obs_valid - sim_valid).mean()\n",
    "corr = obs_valid.corr(sim_valid)\n",
    "\n",
    "# Percent bias\n",
    "pbias = 100 * bias / obs_valid.mean()\n",
    "\n",
    "# Nash-Sutcliffe Efficiency\n",
    "nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "\n",
    "# Kling-Gupta Efficiency\n",
    "kge_corr = obs_valid.corr(sim_valid)\n",
    "kge_bias = sim_valid.mean() / obs_valid.mean()\n",
    "kge_var = (sim_valid.std() / obs_valid.std())\n",
    "kge = 1 - np.sqrt((kge_corr - 1)**2 + (kge_bias - 1)**2 + (kge_var - 1)**2)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"   Root Mean Square Error (RMSE): {rmse:.2f} mm\")\n",
    "print(f\"   Mean Absolute Error (MAE): {mae:.2f} mm\")\n",
    "print(f\"   Bias: {bias:+.2f} mm ({pbias:+.1f}%)\")\n",
    "print(f\"   Correlation: {corr:.3f}\")\n",
    "print(f\"   Nash-Sutcliffe Efficiency: {nse:.3f}\")\n",
    "print(f\"   Kling-Gupta Efficiency: {kge:.3f}\")\n",
    "\n",
    "# Snow-specific metrics\n",
    "print(f\"\\nâ„ï¸  Snow-Specific Performance Assessment:\")\n",
    "\n",
    "# Peak SWE analysis\n",
    "obs_peak = obs_valid.max()\n",
    "sim_peak = sim_valid.max()\n",
    "peak_bias = sim_peak - obs_peak\n",
    "peak_pbias = 100 * peak_bias / obs_peak\n",
    "\n",
    "print(f\"       Peak SWE:\")\n",
    "print(f\"       Observed: {obs_peak:.1f} mm\")\n",
    "print(f\"       Simulated: {sim_peak:.1f} mm\")\n",
    "print(f\"       Bias: {peak_bias:+.1f} mm ({peak_pbias:+.1f}%)\")\n",
    "\n",
    "# Snow season timing\n",
    "obs_peak_date = obs_valid.idxmax()\n",
    "sim_peak_date = sim_valid.idxmax()\n",
    "timing_diff = (sim_peak_date - obs_peak_date).days\n",
    "\n",
    "print(f\"       Peak Timing:\")\n",
    "print(f\"       Observed peak: {obs_peak_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"       Simulated peak: {sim_peak_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"       Timing difference: {timing_diff:+d} days\")\n",
    "\n",
    "# Snow season length\n",
    "snow_threshold = 10  # mm\n",
    "obs_snow_days = (obs_valid > snow_threshold).sum()\n",
    "sim_snow_days = (sim_valid > snow_threshold).sum()\n",
    "\n",
    "print(f\"       Snow Season (SWE > {snow_threshold} mm):\")\n",
    "print(f\"       Observed: {obs_snow_days} days\")\n",
    "print(f\"       Simulated: {sim_snow_days} days\")\n",
    "print(f\"       Difference: {sim_snow_days - obs_snow_days:+d} days\")\n",
    "\n",
    "# Create  visualization\n",
    "print(f\"\\n Creating SWE comparison visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(obs_daily.index, obs_values, 'o-', label='Observed', \n",
    "         color='black', alpha=0.7, markersize=3, linewidth=1)\n",
    "ax1.plot(sim_daily.index, sim_daily, '-', label='Simulated', \n",
    "         color='blue', linewidth=2)\n",
    "ax1.set_title('Snow Water Equivalent Time Series', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('SWE (mm)', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add performance metrics as text\n",
    "metrics_text = f'RMSE: {rmse:.1f} mm\\nBias: {bias:+.1f} mm\\nCorr: {corr:.3f}\\nNSE: {nse:.3f}'\n",
    "ax1.text(0.02, 0.95, metrics_text, transform=ax1.transAxes, \n",
    "         bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "\n",
    "# Scatter plot\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(obs_valid, sim_valid, alpha=0.6, c='blue', s=20)\n",
    "max_val = max(obs_valid.max(), sim_valid.max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "ax2.set_xlabel('Observed SWE (mm)', fontsize=11)\n",
    "ax2.set_ylabel('Simulated SWE (mm)', fontsize=11)\n",
    "ax2.set_title('Observed vs. Simulated SWE', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Seasonal cycle\n",
    "ax3 = axes[1, 0]\n",
    "obs_monthly = obs_values.groupby(obs_values.index.month).mean()\n",
    "sim_monthly = sim_daily.groupby(sim_daily.index.month).mean()\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax3.plot(range(1, 13), obs_monthly, 'o-', label='Observed', color='black', linewidth=2)\n",
    "ax3.plot(range(1, 13), sim_monthly, 'o-', label='Simulated', color='blue', linewidth=2)\n",
    "ax3.set_xticks(range(1, 13))\n",
    "ax3.set_xticklabels(months, rotation=45)\n",
    "ax3.set_ylabel('Mean SWE (mm)', fontsize=11)\n",
    "ax3.set_title('Seasonal Cycle', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Residuals over time\n",
    "ax4 = axes[1, 1]\n",
    "residuals = sim_valid - obs_valid\n",
    "ax4.scatter(obs_valid.index, residuals, alpha=0.6, c='red', s=15)\n",
    "ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.axhline(y=residuals.std(), color='red', linestyle='--', alpha=0.5, label=f'Â±1Ïƒ ({residuals.std():.1f} mm)')\n",
    "ax4.axhline(y=-residuals.std(), color='red', linestyle='--', alpha=0.5)\n",
    "ax4.set_ylabel('Residuals (Sim - Obs) [mm]', fontsize=11)\n",
    "ax4.set_title('Model Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle(f'Snow Water Equivalent Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b: Soil Moisture Profile Evaluation\n",
    "Soil moisture evaluation tests the model's representation of vadose zone processes, including infiltration, drainage, and vertical redistribution. Multi-depth observations provide unprecedented validation opportunities for soil physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observed soil moisture data\n",
    "obs_sm_path = confluence.project_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_sm_processed.csv\"\n",
    "\n",
    "# Load observed data\n",
    "obs_sm = pd.read_csv(obs_sm_path, parse_dates=['timestamp'])\n",
    "obs_sm.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Ensure proper datetime index\n",
    "if not isinstance(obs_sm.index, pd.DatetimeIndex):\n",
    "    obs_sm.index = pd.to_datetime(obs_sm.index)\n",
    "\n",
    "# Identify observed depth columns\n",
    "obs_depth_cols = [col for col in obs_sm.columns if col.startswith('sm_')]\n",
    "\n",
    "# Extract observed depth values\n",
    "obs_depths = []\n",
    "for depth_col in obs_depth_cols:\n",
    "    # Extract depth from column name (e.g., 'sm_0.0508_0.0508' -> 0.0508)\n",
    "    depth_str = depth_col.split('_')[1]\n",
    "    obs_depths.append(float(depth_str))\n",
    "\n",
    "# Extract simulated soil moisture\n",
    "\n",
    "sim_sm = evaluation_data['mLayerVolFracLiq']\n",
    "sim_depths = evaluation_data['mLayerDepth']\n",
    "    \n",
    "# Calculate representative layer depths\n",
    "mean_layer_depths = sim_depths.mean(dim='time')\n",
    "valid_layers = mean_layer_depths > 0  # Filter out invalid layers\n",
    "    \n",
    "# Find common period\n",
    "start_date = max(obs_sm.index.min(), pd.to_datetime(sim_sm.time.min().values))\n",
    "end_date = min(obs_sm.index.max(), pd.to_datetime(sim_sm.time.max().values))\n",
    "    \n",
    "# Filter to common period\n",
    "obs_period = obs_sm.loc[start_date:end_date]\n",
    "sim_time_mask = (sim_sm.time >= start_date) & (sim_sm.time <= end_date)\n",
    "sim_period = sim_sm.isel(time=sim_time_mask)\n",
    "sim_depths_period = sim_depths.isel(time=sim_time_mask)\n",
    "    \n",
    "n_depths = len(obs_depth_cols)\n",
    "depth_results = {}\n",
    "\n",
    "# Analyze each observed depth\n",
    "for i, (depth_col, obs_depth) in enumerate(zip(obs_depth_cols, obs_depths)):\n",
    "    print(f\"\\n     Depth {i+1}: {obs_depth:.4f}m ({depth_col})\")\n",
    "    \n",
    "    # Find closest simulated layer\n",
    "    mean_depths = sim_depths_period.mean(dim='time')\n",
    "    valid_mask = mean_depths > 0\n",
    "    \n",
    "    if valid_mask.sum() > 0:\n",
    "        valid_mean_depths = mean_depths.where(valid_mask)\n",
    "        depth_differences = np.abs(valid_mean_depths - obs_depth)\n",
    "        closest_layer_idx = depth_differences.argmin().values\n",
    "        closest_layer_depth = valid_mean_depths[closest_layer_idx].values\n",
    "        \n",
    "        \n",
    "        # Extract data for this layer\n",
    "        obs_layer = obs_period[depth_col]\n",
    "        sim_layer = sim_period.isel(midToto=closest_layer_idx, hru=0)\n",
    "        \n",
    "        # Convert to pandas for easier handling\n",
    "        sim_layer_ts = sim_layer.to_pandas()\n",
    "        \n",
    "        # Resample to daily and align\n",
    "        obs_daily = obs_layer.resample('D').mean()\n",
    "        sim_daily = sim_layer_ts.resample('D').mean()\n",
    "        \n",
    "        # Remove invalid values (negative soil moisture indicates missing data)\n",
    "        sim_daily = sim_daily.where(sim_daily > -100)\n",
    "        \n",
    "        # Find valid paired data\n",
    "        valid_data_mask = ~(obs_daily.isna() | sim_daily.isna())\n",
    "        obs_valid = obs_daily[valid_data_mask]\n",
    "        sim_valid = sim_daily[valid_data_mask]\n",
    "        \n",
    "        if len(obs_valid) > 10:  # Require minimum data for meaningful evaluation\n",
    "            # Calculate performance metrics\n",
    "            rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "            bias = (sim_valid - obs_valid).mean()\n",
    "            mae = np.abs(obs_valid - sim_valid).mean()\n",
    "            corr = obs_valid.corr(sim_valid)\n",
    "            \n",
    "            # Store results\n",
    "            depth_results[obs_depth] = {\n",
    "                'obs_valid': obs_valid,\n",
    "                'sim_valid': sim_valid,\n",
    "                'rmse': rmse,\n",
    "                'bias': bias,\n",
    "                'mae': mae,\n",
    "                'corr': corr,\n",
    "                'layer_idx': closest_layer_idx,\n",
    "                'sim_depth': closest_layer_depth\n",
    "            }\n",
    "            \n",
    "            print(f\"       RMSE: {rmse:.3f} mÂ³/mÂ³\")\n",
    "            print(f\"       Bias: {bias:+.3f} mÂ³/mÂ³\")\n",
    "            print(f\"       Correlation: {corr:.3f}\")\n",
    "            print(f\"       Valid pairs: {len(obs_valid)}\")\n",
    "            \n",
    "# Create  visualization\n",
    "print(f\"\\n  Creating soil moisture profile evaluation...\")\n",
    "\n",
    "n_depths = len(depth_results)\n",
    "fig, axes = plt.subplots(n_depths, 2, figsize=(15, 4*n_depths))\n",
    "\n",
    "if n_depths == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (obs_depth, results) in enumerate(depth_results.items()):\n",
    "    # Time series plot\n",
    "    ax_ts = axes[i, 0]\n",
    "    ax_ts.plot(results['obs_valid'].index, results['obs_valid'], 'o-', \n",
    "              label=f'Observed ({obs_depth:.4f}m)', \n",
    "              color='black', alpha=0.7, markersize=2, linewidth=1)\n",
    "    ax_ts.plot(results['sim_valid'].index, results['sim_valid'], '-', \n",
    "              label=f'Simulated (L{results[\"layer_idx\"]}, {results[\"sim_depth\"]}m)', \n",
    "              color='blue', linewidth=2)\n",
    "    \n",
    "    ax_ts.set_title(f'Soil Moisture at {obs_depth:.4f}m depth', fontsize=11, fontweight='bold')\n",
    "    ax_ts.set_ylabel('Soil Moisture (mÂ³/mÂ³)', fontsize=10)\n",
    "    ax_ts.grid(True, alpha=0.3)\n",
    "    ax_ts.legend(fontsize=9)\n",
    "    \n",
    "    # Add metrics\n",
    "    metrics_text = (f\"RMSE: {results['rmse']:.3f}\\n\"\n",
    "                   f\"Bias: {results['bias']:+.3f}\\n\"\n",
    "                   f\"Corr: {results['corr']:.3f}\")\n",
    "    ax_ts.text(0.02, 0.95, metrics_text, transform=ax_ts.transAxes,\n",
    "              bbox=dict(facecolor='white', alpha=0.8), fontsize=9, verticalalignment='top')\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax_scatter = axes[i, 1]\n",
    "    ax_scatter.scatter(results['obs_valid'], results['sim_valid'], \n",
    "                      alpha=0.6, c='blue', s=15)\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(results['obs_valid'].min(), results['sim_valid'].min())\n",
    "    max_val = max(results['obs_valid'].max(), results['sim_valid'].max())\n",
    "    ax_scatter.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                   label='1:1 line', alpha=0.7)\n",
    "    \n",
    "    ax_scatter.set_xlabel('Observed SM (mÂ³/mÂ³)', fontsize=10)\n",
    "    ax_scatter.set_ylabel('Simulated SM (mÂ³/mÂ³)', fontsize=10)\n",
    "    ax_scatter.set_title(f'Obs vs Sim at {obs_depth:.4f}m', fontsize=11, fontweight='bold')\n",
    "    ax_scatter.grid(True, alpha=0.3)\n",
    "    ax_scatter.legend(fontsize=9)\n",
    "    ax_scatter.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.suptitle(f'Soil Moisture Profile Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary performance assessment\n",
    "print(f\"\\n  Soil Moisture Profile Performance Summary:\")\n",
    "\n",
    "avg_rmse = np.mean([r['rmse'] for r in depth_results.values()])\n",
    "avg_corr = np.mean([r['corr'] for r in depth_results.values()])\n",
    "avg_bias = np.mean([r['bias'] for r in depth_results.values()])\n",
    "\n",
    "print(f\"       Profile-averaged metrics:\")\n",
    "print(f\"       Average RMSE: {avg_rmse:.3f} mÂ³/mÂ³\")\n",
    "print(f\"       Average correlation: {avg_corr:.3f}\")\n",
    "print(f\"       Average bias: {avg_bias:+.3f} mÂ³/mÂ³\")\n",
    "\n",
    "# Close evaluation dataset\n",
    "evaluation_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tutorial Summary and Next Steps\n",
    "\n",
    "## Summary: Point-Scale Snow and Soil Process Evaluation\n",
    "\n",
    "This tutorial demonstrated the complete CONFLUENCE workflow for point-scale hydrological modeling, establishing fundamental principles for reproducible computational hydrology research. Through the Paradise SNOTEL case study, we illustrated how standardized workflows can bridge the gap between complex modeling frameworks and practical scientific applications while maintaining scientific rigor and reproducibility.\n",
    "\n",
    "### Key Methods\n",
    "The tutorial successfully demonstrated CONFLUENCE's core capabilities through five integrated workflow components. Reproducible workflow management was achieved through configuration-driven experiments that provide complete provenance tracking and ensure experimental transparency. The model-agnostic preprocessing pipeline creates standardized data products that enable true model physics comparisons by separating data preparation from model-specific requirements. Process-based simulation was implemented using SUMMA's modular physics to explicitly represent energy and water balance processes at the point scale. Multi-variable validation leveraged co-located snow water equivalent and multi-depth soil moisture observations to provide comprehensive model evaluation. Finally, scientific interpretation linked quantitative performance metrics to underlying physical processes, moving beyond statistical assessment to process understanding.\n",
    "\n",
    "### Scientific Process Validation\n",
    "The tutorial validated key hydrological processes across multiple temporal scales and physical domains. Snow physics evaluation included accumulation, metamorphism, and energy-balance driven melt processes, with assessment of both magnitude and timing accuracy. Soil hydrology validation examined multi-depth moisture dynamics and vertical water redistribution through four soil layers, testing infiltration and drainage representations. Energy balance processes were implicitly evaluated through successful simulation of temperature-driven phase changes and moisture dynamics. Temporal dynamics were assessed from daily to seasonal time scales, capturing both rapid response and longer-term memory effects. Physical realism was maintained through mass and energy conservation checks and realistic state variable bounds.\n",
    "\n",
    "### CONFLUENCE Framework Demonstration\n",
    "This tutorial showcased CONFLUENCE's modular architecture through specialized managers that handle distinct workflow components while maintaining system integration. Workflow orchestration capabilities were demonstrated through automated step sequencing. The scalable design principle was established, showing how the same framework structure supports applications from point-scale studies through continental-scale analyses. Research continuity was ensured by creating a foundation that directly enables progression to distributed modeling and large-sample comparative studies in subsequent tutorials.\n",
    "\n",
    "### Next Focus: Evapotranspiration and Energy Balance Processes\n",
    "\n",
    "**Ready to explore energy flux validation?** â†’ **[Tutorial 01b: Point-Scale Energy Balance Validation](./01b_point_scale_fluxnet.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scienv)",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
