{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 1a — Point-Scale Workflow (Paradise SNOTEL)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the point-scale modeling workflow in **SYMFLUENCE**, a framework for reproducible and modular computational hydrology. At the point scale, we simulate vertical energy and water fluxes at a single site, independent of routing or lateral flow, to isolate and evaluate model process representations.\n",
    "\n",
    "Here, we focus on the **Paradise SNOTEL station (ID 602)**, located at 1,630 m elevation in Washington’s Cascade Range. This site represents a transitional snow climate and provides long-term observations of snow water equivalent (SWE) and soil moisture across multiple depths. By reproducing the observed seasonal snow and soil moisture dynamics, this tutorial demonstrates how SYMFLUENCE structures a controlled, transparent, and fully reproducible point-scale experiment.\n",
    "\n",
    "Through this example, you will see how configuration-driven workflows manage experiment setup, geospatial definition, input data preprocessing, model instantiation, and performance evaluation—building a foundation for more complex distributed modeling studies later in the series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 — Configuration (pick or generate)\n",
    "\n",
    "We begin by selecting (or programmatically generating) a single configuration file that fully specifies the experiment. This keeps the workflow reproducible and makes initialization a one-liner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Create a site-specific configuration for the Paradise SNOTEL example\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Path to the default template configuration\n",
    "config_template = Path(\"../../0_config_files/config_template.yaml\")\n",
    "\n",
    "# Load the base configuration\n",
    "with open(config_template, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify key entries for the Paradise SNOTEL point-scale case ===\n",
    "\n",
    "# Define code directory — ensures relative paths resolve correctly\n",
    "config[\"SYMFLUENCE_CODE_DIR\"] = str(Path(\"../../\").resolve())\n",
    "\n",
    "# Define data directory — location of required input and observational data\n",
    "#config[\"SYMFLUENCE_DATA_DIR\"] = str(Path(\"/path/to/SYMFLUENCE_data\").resolve())\n",
    "\n",
    "# Restrict the spatial domain to a single site using latitude/longitude bounds\n",
    "# This ensures domain setup treats it as a point-scale (non-routed) experiment\n",
    "config[\"DOMAIN_DEFINITION_METHOD\"] = \"point\"\n",
    "config[\"DOMAIN_DISCRETIZATION\"] = \"GRUs\"\n",
    "config[\"BOUNDING_BOX_COORDS\"] = \"46.781/-121.751/46.779/-121.749\"\n",
    "config[\"POUR_POINT_COORDS\"] = \"46.78/-121.75\"\n",
    "\n",
    "# Enable automatic download of SNOTEL data for this station\n",
    "config[\"DOWNLOAD_SNOTEL\"] = True\n",
    "\n",
    "# Specify model and forcing dataset used in this example\n",
    "config[\"HYDROLOGICAL_MODEL\"] = \"SUMMA\"     # SUMMA is the process-based model used here\n",
    "config[\"FORCING_DATASET\"] = \"ERA5\"         # ERA5 reanalysis for meteorological inputs\n",
    "\n",
    "# Define the temporal extent of the experiment\n",
    "config[\"EXPERIMENT_TIME_START\"] = \"2000-01-01 01:00\"\n",
    "config[\"EXPERIMENT_TIME_END\"] = \"2002-12-31 23:00\"\n",
    "config['CALIBRATION_PERIOD'] = \"2000-10-01, 2001-09-30\"\n",
    "config['EVALUATION_PERIOD'] = \"2001-10-01, 2002-09-30\"\n",
    "config['SPINUP_PERIOD'] = \"2000-01-01, 2000-09-30\"\n",
    "\n",
    "# Assign a descriptive domain name and experiment ID\n",
    "config[\"DOMAIN_NAME\"] = \"paradise\"\n",
    "config[\"EXPERIMENT_ID\"] = \"run_1\"\n",
    "\n",
    "# MAF paths and settings - if you have access to MAF data stored on DRAC, Access Anvil or U calgary ARC \n",
    "config['DATATOOL_DATASET_ROOT'] = '/path/to/meteorological-data/'           # Path to datatool datasets root directory\n",
    "config['GISTOOL_DATASET_ROOT'] = '/path/to/geospatial-data/'                # Path to gistool datasets root directory\n",
    "config['TOOL_CACHE'] = '/path/to/cache/dir'                                 # Path to gistool cache directory\n",
    "config['CLUSTER_JSON']: '/path/to/cluster.json'                             # Path to cluster json config\n",
    "config['SNOW_DATA_SOURCE'] = 'SNOTEL'                                # Snow data source: SNOTEL, manual\n",
    "config['SNOW_STATIONS'] = '679'                                        # Snow station IDs\n",
    "config['ISMN_NETWORK'] = 'SCAN'                                        # ISMN network name\n",
    "config['ISMN_STATIONS'] = '679'                                        # ISMN station IDs\n",
    "\n",
    "# Optimisation settings\n",
    "config['OPTIMISATION_TARGET'] = 'swe'\n",
    "config['PARAMS_TO_CALIBRATE'] = 'tempCritRain,tempRangeTimestep,frozenPrecipMultip,albedoMax,albedoMinWinter,albedoDecayRate,constSnowDen,mw_exp,k_snow,z0Snow'\n",
    "config['ITERATIVE_OPTIMIZATION_ALGORITHM'] = 'DDS'\n",
    "config['OPTIMIZATION_METRIC'] = 'RMSE'\n",
    "config['CALIBRATION_TIMESTEP'] = 'daily'  \n",
    "\n",
    "\n",
    "\n",
    "# === Save the customized configuration ===\n",
    "out_config = Path(\"../../0_config_files/config_paradise.yaml\")\n",
    "with open(out_config, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✅ New configuration written to: {out_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b — Initialize SYMFLUENCE\n",
    "\n",
    "With the configuration prepared, we now initialize **SYMFLUENCE**.  \n",
    "This step reads the configuration file, sets up the project directory, and registers all workflow managers (data, domain, model, and evaluation).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b — Initialize SYMFLUENCE\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "from symfluence import SYMFLUENCE  # adjust if your import path differs\n",
    "\n",
    "config_path = \"../../0_config_files/config_paradise.yaml\"\n",
    "symfluence = SYMFLUENCE(config_path)\n",
    "\n",
    "print(\"✅ SYMFLUENCE initialized successfully.\")\n",
    "print(f\"Configuration loaded from: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c — Project structure setup\n",
    "\n",
    "We now create the standardized project directory and a pour-point feature for the site.  \n",
    "This anchors the experiment in a clear, reproducible file layout and records the site location for downstream domain and data steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c — Project structure setup\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Create the standardized project layout (logs, config link, data/output folders, etc.)\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "\n",
    "# 2) Create a pour-point feature (the site reference geometry for point-scale workflows)\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(\"✅ Project structure created.\")\n",
    "print(f\"Project root: {project_dir}\")\n",
    "print(f\"Pour point:   {pour_point_path}\")\n",
    "\n",
    "# 3) Brief top-level directory preview\n",
    "print(\"\\nTop-level structure:\")\n",
    "for p in sorted(Path(project_dir).iterdir()):\n",
    "    if p.is_dir():\n",
    "        print(f\"├── {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Domain definition (point-scale GRU)\n",
    "\n",
    "For the Paradise SNOTEL example, the domain is a **single GRU** representing the site footprint.  \n",
    "This keeps the workflow strictly point-scale (no routing), aligning the geometry with the pour point created in Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a — Geospatial attribute acquisition - **Only available through MAF supported HPCs**\n",
    "\n",
    "We first acquire site attributes (elevation, land cover, soils, etc.).  \n",
    "These are model-agnostic inputs used to parameterize vertical energy and water balance at the site.\n",
    "\n",
    "- If you are using the downloaded example data. Copy the attributes, forcing and observation directories into the newly created domain directory from step 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2a — Acquire attributes (model-agnostic)\n",
    "# If you are using MAF supported HPC, uncomment the below line\n",
    "#symfluence.managers['data'].acquire_attributes()\n",
    "print(\"✅ Attribute acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b — Domain definition (point-scale)\n",
    "\n",
    "With attributes prepared, we define a point-scale domain consistent with the pour point.  \n",
    "For this example, the domain is a minimal footprint around the Paradise SNOTEL site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b — Define the point-scale domain\n",
    "watershed_path = symfluence.managers['domain'].define_domain()\n",
    "print(\"✅ Domain definition complete\")\n",
    "print(f\"Domain file: {watershed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c — Discretization (required even for 1 GRU = 1 HRU)\n",
    "\n",
    "Discretization writes the **catchment HRU shapefile** and related artifacts required by downstream steps.  \n",
    "For the point-scale case we set `DOMAIN_DISCRETIZATION: GRUs`, which creates a **single HRU** identical to the GRU while still generating the standardized outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c — Discretization (GRUs → HRUs 1:1, but files are still created)\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(\"✅ Domain discretization complete\")\n",
    "print(f\"HRU file: {hru_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2d — Verification & inspection (Paradise SNOTEL)\n",
    "\n",
    "We verify that discretization produced the expected shapefiles in the standardized locations, then plot a minimal GRU–HRU overlay.\n",
    "\n",
    "**Expected files**\n",
    "- `domain_dir/shapefiles/river_basins/paradise_riverBasins_point.shp` (GRU)\n",
    "- `domain_dir/shapefiles/catchment/paradise_HRUs_GRUs.shp` (HRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2d — Verify domain outputs and inspect geometry\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "# 1) Read config to get data dir and domain name\n",
    "with open(\"../../0_config_files/config_paradise.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "shp_dir    = domain_dir / \"shapefiles\"\n",
    "\n",
    "# 2) Explicit expected shapefiles for Paradise\n",
    "gru_fp = shp_dir / \"river_basins\" / \"paradise_riverBasins_point.shp\"\n",
    "hru_fp = shp_dir / \"catchment\"     / \"paradise_HRUs_GRUs.shp\"\n",
    "\n",
    "# 3) Verify presence\n",
    "for label, path in [(\"GRU\", gru_fp), (\"HRU\", hru_fp)]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"❌ Expected {label} file not found: {path}\")\n",
    "    print(f\"✅ {label} file found: {path}\")\n",
    "\n",
    "# 4) Minimal overlay plot\n",
    "gru = gpd.read_file(gru_fp)\n",
    "hru = gpd.read_file(hru_fp)\n",
    "if hru.crs != gru.crs:\n",
    "    hru = hru.to_crs(gru.crs)\n",
    "\n",
    "ax = gru.plot(figsize=(6, 6))\n",
    "hru.plot(ax=ax, facecolor=\"none\")\n",
    "ax.set_title(\"Paradise SNOTEL — GRU vs HRU\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 — Input preprocessing (model-agnostic)\n",
    "\n",
    "We prepare inputs in three small moves:\n",
    "1) acquire **meteorological forcings**,  \n",
    "2) process **observations** (SNOTEL), and  \n",
    "3) run **model-agnostic preprocessing** to standardize time steps, variables, and units for downstream use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a — Acquire meteorological forcings (ERA5)\n",
    "\n",
    "Downloads/subsets the forcings for the Paradise domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a — Forcings\n",
    "# If you are using MAF supported HPC, uncomment the below line\n",
    "# symfluence.managers['data'].acquire_forcings()\n",
    "print(\"✅ Forcing data acquisition complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b — Process observations (SNOTEL)\n",
    "\n",
    "Parses site observations (e.g., SWE, soil moisture), applies basic QA/QC, and stores standardized outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b — Observations\n",
    "# If you are using MAF supported HPC, uncomment the below line\n",
    "#symfluence.managers['data'].process_observed_data()\n",
    "print(\"✅ Observational data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c — Model-agnostic preprocessing\n",
    "\n",
    "Standardizes variable names, units, and time steps (and fills required diagnostics) so multiple models can consume the same inputs consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c — Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"✅ Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d — Quick verification\n",
    "\n",
    "We confirm the expected folders exist and contain files:\n",
    "\n",
    "- `forcing/raw_data/`\n",
    "- `forcing/basin_averaged_data/`\n",
    "- `observations/snow/{raw,processed}/`\n",
    "- `observations/soil_moisture/{raw,processed}/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Derive paths from the config (no hard-coding)\n",
    "with open(\"../../0_config_files/config_paradise.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "targets = {\n",
    "    \"forcing/raw_data\":                  domain_dir / \"forcing\" / \"raw_data\",\n",
    "    \"forcing/basin_averaged_data\":       domain_dir / \"forcing\" / \"basin_averaged_data\",\n",
    "    \"observations/snow/raw\":             domain_dir / \"observations\" / \"snow\" / \"swe\" / \"raw\",\n",
    "    \"observations/snow/processed\":       domain_dir / \"observations\" / \"snow\" / \"swe\" / \"processed\",\n",
    "    \"observations/soil_moisture/raw\":    domain_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"raw\",\n",
    "    \"observations/soil_moisture/processed\": domain_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"processed\",\n",
    "}\n",
    "\n",
    "def count_files(p: Path) -> int:\n",
    "    return sum(1 for x in p.iterdir() if x.is_file()) if p.exists() else 0\n",
    "\n",
    "for label, path in targets.items():\n",
    "    exists = path.exists()\n",
    "    n = count_files(path)\n",
    "    status = \"✅\" if exists and n > 0 else (\"⚠️ empty\" if exists else \"❌ missing\")\n",
    "    suffix = f\"({n} files)\" if exists else \"\"\n",
    "    print(f\"{status} {label}  {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 — Model-specific preprocessing & model run (SUMMA)\n",
    "\n",
    "We now convert the model-agnostic inputs into **SUMMA-ready inputs**, then instantiate and run the model for the Paradise point-scale case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4a — SUMMA-specific preprocessing\n",
    "\n",
    "Creates the SUMMA input bundle (metadata, parameter tables, forcing links) from the standardized inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a — SUMMA-specific preprocessing\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"✅ Model-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b — Instantiate & run the model\n",
    "\n",
    "Instantiates the model using the prepared inputs and executes the point-scale simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4b — Instantiate & run SUMMA\n",
    "print(f\"Running {symfluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation…\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"✅ Point-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4c - Quick verification\n",
    "\n",
    "Print where SUMMA inputs and run outputs were written (paths are derived from the configuration).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "with open(\"../../0_config_files/config_paradise.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "# Common locations used by the model manager\n",
    "summa_in   = domain_dir / \"forcing\" / \"SUMMA_input\"\n",
    "results    = domain_dir / \"simulations\" / cfg['EXPERIMENT_ID'] / 'SUMMA' \n",
    "\n",
    "print(\"SUMMA input dir:\", summa_in if summa_in.exists() else \"(not found)\")\n",
    "print(\"Results dir:\",    results if results.exists()    else \"(not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c — SWE only (obs vs sim) with robust NetCDF open + unit auto-detect\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml, pandas as pd, numpy as np, xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# --- Paths from config ---\n",
    "with open(\"../../0_config_files/config_paradise.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "# Find a daily SUMMA output (e.g., *_day.nc) under the domain folder\n",
    "nc_files = list(domain_dir.rglob(\"*_day.nc\"))\n",
    "if not nc_files:\n",
    "    # fallback: any .nc under results\n",
    "    nc_files = list((domain_dir / \"simulations\" / cfg['EXPERIMENT_ID'] / \"SUMMA\").rglob(\"_day.nc\"))\n",
    "if not nc_files:\n",
    "    raise FileNotFoundError(f\"No netCDF files found under {domain_dir}\")\n",
    "nc_path = nc_files[0]\n",
    "\n",
    "def open_dataset_safe(path: Path) -> xr.Dataset:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Path does not exist: {path}\")\n",
    "    if path.is_dir() or path.suffix.lower() == \".zarr\":\n",
    "        # Zarr store\n",
    "        return xr.open_zarr(path)\n",
    "    errs = []\n",
    "    for eng in (\"netcdf4\", \"scipy\"):\n",
    "        try:\n",
    "            return xr.open_dataset(path, engine=eng)\n",
    "        except Exception as e:\n",
    "            errs.append(f\"{eng}: {e}\")\n",
    "    raise ValueError(f\"Could not open {path} with engines netcdf4/scipy.\\n\" + \"\\n\".join(errs))\n",
    "\n",
    "# --- Helpers ---\n",
    "def rmse(a, b): \n",
    "    d = (a - b).to_numpy(dtype=float)\n",
    "    return float(np.sqrt(np.nanmean(d**2)))\n",
    "\n",
    "def bias(a, b):\n",
    "    return float((a - b).mean())\n",
    "\n",
    "def first_numeric_col(df):\n",
    "    cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return cols[0] if cols else None\n",
    "\n",
    "def reduce_to_time_series(da: xr.DataArray) -> pd.Series:\n",
    "    if \"time\" not in da.dims:\n",
    "        if \"time\" in da.coords:\n",
    "            da = da.swap_dims({list(da.dims)[0]: \"time\"})\n",
    "        else:\n",
    "            raise ValueError(\"DataArray has no 'time' dimension or coordinate.\")\n",
    "    # pick first layer if present; average any spatial dims (hru/gru)\n",
    "    for d in [d for d in da.dims if d != \"time\"]:\n",
    "        if re.search(\"layer|soil\", d, re.I):\n",
    "            da = da.isel({d: 0})\n",
    "        else:\n",
    "            da = da.mean(d)\n",
    "    s = da.to_series()\n",
    "    s.index = pd.to_datetime(s.index)\n",
    "    return s.sort_index()\n",
    "\n",
    "def read_obs_csv(path: Path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = pd.read_csv(path)\n",
    "        date_col = next(c for c in df.columns if re.search(\"date|time\", str(c), re.I))\n",
    "        df[date_col] = pd.to_datetime(df[date_col].astype(str).str.strip(), dayfirst=True, errors=\"raise\")\n",
    "        df = df.set_index(date_col)\n",
    "    df = df.sort_index()\n",
    "    col = first_numeric_col(df)\n",
    "    if not col:\n",
    "        raise ValueError(f\"No numeric SWE column found in {path}\")\n",
    "    return df[col].astype(float), str(col)\n",
    "\n",
    "def align(a, b):\n",
    "    idx = a.index.intersection(b.index)\n",
    "    return a.loc[idx].astype(float), b.loc[idx].astype(float)\n",
    "\n",
    "# --- Open dataset robustly & handle spinup safely ---\n",
    "ds = open_dataset_safe(nc_path)\n",
    "\n",
    "# robust time extraction\n",
    "time_vals = ds.get(\"time\")\n",
    "if time_vals is None or np.size(time_vals) == 0:\n",
    "    raise ValueError(\"Dataset has no usable 'time' coordinate.\")\n",
    "time_pd = pd.to_datetime(np.array(time_vals.values), errors=\"coerce\")\n",
    "if pd.isna(time_pd).all():\n",
    "    # cftime fallback\n",
    "    time_pd = pd.to_datetime([f\"{t.year:04d}-{t.month:02d}-{t.day:02d}\" for t in time_vals.values])\n",
    "start_year = int(time_pd.min().year) + 1\n",
    "\n",
    "ds_eval = ds.sel(time=slice(f\"{start_year}-01-01\", None))\n",
    "if ds_eval.sizes.get(\"time\", 0) == 0:\n",
    "    ds_eval = ds  # if skipping a year empties it, use full record\n",
    "\n",
    "# SWE (simulation) — expected in mm\n",
    "swe_var_candidates = [\"scalarSWE\", \"scalarSnowWaterEquivalent\", \"SWE\"]\n",
    "swe_var = next((v for v in swe_var_candidates if v in ds_eval.data_vars), None)\n",
    "if not swe_var:\n",
    "    raise KeyError(f\"SWE variable not found. Tried: {swe_var_candidates}\")\n",
    "swe_sim = reduce_to_time_series(ds_eval[swe_var])\n",
    "\n",
    "# --- Load SWE observations (processed; unknown units) ---\n",
    "swe_obs_csv1 = domain_dir / \"observations\" / \"snow\" / \"processed\" / f\"{cfg['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "swe_obs_csv2 = domain_dir / \"observations\" / \"snow\" / \"swe\" / \"processed\" / f\"{cfg['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "swe_obs_csv  = swe_obs_csv1 if swe_obs_csv1.exists() else swe_obs_csv2\n",
    "swe_obs_raw, swe_obs_col = read_obs_csv(swe_obs_csv)\n",
    "\n",
    "# --- Unit auto-detection: assume sim is mm; try mm/cm/in for obs ---\n",
    "candidates = {\"mm\": 1.0, \"cm\": 10.0, \"in\": 25.4}\n",
    "scores, aligned_examples = {}, {}\n",
    "for unit, factor in candidates.items():\n",
    "    obs_scaled = swe_obs_raw * factor\n",
    "    sim_a, obs_a = align(swe_sim, obs_scaled)\n",
    "    if len(sim_a) == 0:\n",
    "        scores[unit] = np.inf\n",
    "    else:\n",
    "        scores[unit] = rmse(sim_a, obs_a)\n",
    "        aligned_examples[unit] = (sim_a, obs_a)\n",
    "\n",
    "best_unit = min(scores, key=scores.get)\n",
    "scale = candidates[best_unit]\n",
    "if not np.isfinite(scores[best_unit]):\n",
    "    raise ValueError(\"No overlapping dates between SWE sim and obs.\")\n",
    "swe_sim_a, swe_obs_a = aligned_examples[best_unit]\n",
    "\n",
    "# --- Metrics & diagnostics ---\n",
    "def corr(a, b): return float(pd.Series(a).corr(pd.Series(b)))\n",
    "\n",
    "swe_metrics = dict(RMSE=rmse(swe_sim_a, swe_obs_a),\n",
    "                   Bias=bias(swe_sim_a, swe_obs_a),\n",
    "                   r=corr(swe_sim_a, swe_obs_a))\n",
    "\n",
    "print(f\"Opened: {nc_path}\")\n",
    "print(f\"SWE sim range: {swe_sim.index.min()} → {swe_sim.index.max()}\")\n",
    "print(f\"SWE obs range: {swe_obs_raw.index.min()} → {swe_obs_raw.index.max()}\")\n",
    "print(f\"SWE overlap days: {len(swe_sim_a)}\")\n",
    "print(f\"Detected obs SWE units: {best_unit} (×{scale})\")\n",
    "print(\"SWE metrics:\", {k: round(v,3) for k,v in swe_metrics.items()})\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(swe_obs_a.index, swe_obs_a.values, label=f\"Obs (→ mm from {best_unit})\")\n",
    "plt.plot(swe_sim_a.index, swe_sim_a.values, label=\"Sim (mm)\")\n",
    "plt.title(\"SWE (obs vs sim)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 — Calibration (SUMMA, Differential Evolution)\n",
    "\n",
    "We enable **iterative calibration** for SUMMA, set the **calibration/evaluation periods**, choose **parameters**, and pick a **single objective** (KGE).  \n",
    "SYMFLUENCE exposes a one-liner to run calibration once config is set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5a — Minimal config (what matters)\n",
    "\n",
    "Add/confirm these in `config_paradise.yaml`:\n",
    "\n",
    "```yaml\n",
    "# Enable iterative calibration with DE, use KGE on the calibration window\n",
    "OPTIMISATION_METHODS: [iteration]\n",
    "ITERATIVE_OPTIMIZATION_ALGORITHM: DE      # DE, DDS, PSO, SCE-UA, NSGA-II\n",
    "OPTIMIZATION_METRIC: KGE                  # KGE, NSE, RMSE, MAE, KGEp\n",
    "\n",
    "# Parameters to calibrate (point-scale set)\n",
    "PARAMS_TO_CALIBRATE: tempCritRain,k_soil,vGn_n,theta_sat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b — Run calibration (DE + KGE)\n",
    "\n",
    "results_file = symfluence.managers['optimization'].calibrate_model()  \n",
    "print(\"Calibration results file:\", results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SYMFLUENCE root venv)",
   "language": "python",
   "name": "symfluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
