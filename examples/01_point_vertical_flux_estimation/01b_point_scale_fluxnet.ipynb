{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89326bfc",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 1b — Point-Scale Workflow (FLUXNET CA-NS7)\n",
    "\n",
    "## Introduction\n",
    "This notebook mirrors the concise, configuration-first style established in **Tutorial 01a** and adapts it for **energy-balance validation at a FLUXNET tower (CA-NS7)**. We simulate point-scale land–atmosphere exchanges and evaluate **evapotranspiration (LE)** and **sensible heat (H)** using FLUXNET observations.\n",
    "\n",
    "The workflow is strictly configuration-driven and fully reproducible:\n",
    "1) write a minimal config, 2) initialize SYMFLUENCE and standard project layout, 3) define the point-scale domain, 4) acquire & preprocess inputs, 5) run **SUMMA**, and 6) evaluate fluxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f278971",
   "metadata": {},
   "source": [
    "# Step 1 — Configuration (pick or generate)\n",
    "\n",
    "We start by generating a compact configuration for the **CA-NS7** FLUXNET site using the same pattern as 01a. This keeps initialization a one-liner and the workflow fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Create a site-specific configuration for the CA-NS7 FLUXNET example\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Path to the default template configuration (same pattern as 01a)\n",
    "config_template = Path(\"../../0_config_files/config_template.yaml\")\n",
    "\n",
    "# Load the base configuration\n",
    "with open(config_template, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify key entries for the CA-NS7 point-scale case ===\n",
    "# Code & data directories\n",
    "config[\"SYMFLUENCE_CODE_DIR\"] = str(Path(\"../../\").resolve())\n",
    "#config[\"SYMFLUENCE_DATA_DIR\"] = str(Path(\"/path/to/SYMFLUENCE_data\").resolve())\n",
    "\n",
    "# Point-scale domain settings\n",
    "config[\"DOMAIN_DEFINITION_METHOD\"] = \"point\"\n",
    "config[\"DOMAIN_DISCRETIZATION\"] = \"GRUs\"  # 1 GRU => 1 HRU\n",
    "config[\"DOMAIN_NAME\"] = \"CA-NS7\"\n",
    "config[\"POUR_POINT_COORDS\"] = \"56.6358/-99.9483\"  # CA-NS7 coordinates\n",
    "config[\"BOUNDING_BOX_COORDS\"] = \"56.6858/-99.9983/56.585800000000006/-99.8983\"\n",
    "\n",
    "\n",
    "# Data/forcing & model\n",
    "config[\"HYDROLOGICAL_MODEL\"] = \"SUMMA\"\n",
    "config[\"FORCING_DATASET\"] = \"ERA5\"  # Used for meteorological inputs\n",
    "config[\"DOWNLOAD_FLUXNET\"] = True\n",
    "config[\"FLUXNET_STATION\"] = \"CA-NS7\"\n",
    "\n",
    "# Define the temporal extent of the experiment\n",
    "config[\"EXPERIMENT_TIME_START\"] = \"2001-01-01 01:00\"\n",
    "config[\"EXPERIMENT_TIME_END\"] = \"2005-12-31 23:00\"\n",
    "config['CALIBRATION_PERIOD'] = \"2002-10-01, 2003-09-30\"\n",
    "config['EVALUATION_PERIOD'] = \"2003-10-01, 2004-09-30\"\n",
    "config['SPINUP_PERIOD'] = \"2001-01-01, 2002-09-30\"\n",
    "\n",
    "# (Optional) Paths to institutional data roots — customize if using shared infra\n",
    "config['DATATOOL_DATASET_ROOT'] = '/path/to/meteorological-data/'\n",
    "config['GISTOOL_DATASET_ROOT']  = '/path/to/geospatial-data/'\n",
    "config['TOOL_CACHE']            = '/path/to/cache/dir'\n",
    "config['CLUSTER_JSON']          = '/path/to/cluster.json'\n",
    "\n",
    "# Basic optimization knobs if desired (example only)\n",
    "config['PARAMS_TO_CALIBRATE'] = 'minStomatalResistance,cond2photo_slope,vcmax25_canopyTop,jmax25_scale,summerLAI,rootingDepth,soilStressParam,z0Canopy,windReductionParam'\n",
    "config['OPTIMISATION_TARGET'] = 'et'\n",
    "config['ITERATIVE_OPTIMIZATION_ALGORITHM'] = 'DDS'\n",
    "config['OPTIMIZATION_METRIC'] = 'RMSE'\n",
    "config['CALIBRATION_TIMESTEP'] = 'daily'  \n",
    "\n",
    "# Unique experiment ID for outputs\n",
    "config[\"EXPERIMENT_ID\"] = \"run_fluxnet_1\"\n",
    "\n",
    "# === Save the customized configuration ===\n",
    "out_config = Path(\"../../0_config_files/config_fluxnet_CA-NS7.yaml\")\n",
    "with open(out_config, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✅ New configuration written to: {out_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97564401",
   "metadata": {},
   "source": [
    "## Step 1b — Initialize SYMFLUENCE\n",
    "Initialize the framework using the configuration prepared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ed0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b — Initialize SYMFLUENCE\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "from SYMFLUENCE import SYMFLUENCE  # adjust if your import path differs\n",
    "\n",
    "config_path = \"../../0_config_files/config_fluxnet_CA-NS7.yaml\"\n",
    "symfluence = SYMFLUENCE(config_path)\n",
    "\n",
    "print(\"✅ SYMFLUENCE initialized successfully.\")\n",
    "print(f\"Configuration loaded from: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83254e40",
   "metadata": {},
   "source": [
    "## Step 1c — Project structure setup\n",
    "Create the standardized project directory and a pour-point feature for the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ed2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c — Project structure setup\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Create the standardized project layout (logs, config link, data/output folders, etc.)\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "\n",
    "# 2) Create a pour-point feature (site reference geometry for point-scale workflows)\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(\"✅ Project structure created.\")\n",
    "print(f\"Project root: {project_dir}\")\n",
    "print(f\"Pour point:   {pour_point_path}\")\n",
    "\n",
    "# 3) Brief top-level directory preview\n",
    "print(\"\\nTop-level structure:\")\n",
    "for p in sorted(Path(project_dir).iterdir()):\n",
    "    if p.is_dir():\n",
    "        print(f\"├── {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a157a",
   "metadata": {},
   "source": [
    "# Step 2 — Domain definition (point-scale GRU)\n",
    "The domain is a **single GRU** around the flux tower footprint, ensuring a strictly point-scale (non-routed) experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2a4dc",
   "metadata": {},
   "source": [
    "### Step 2a — Geospatial attribute acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c829a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a — Acquire attributes (model-agnostic)\n",
    "#symfluence.managers['data'].acquire_attributes()\n",
    "print(\"✅ Attribute acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444445cc",
   "metadata": {},
   "source": [
    "### Step 2b — Domain definition (point-scale)\n",
    "Define a minimal footprint around **CA-NS7** consistent with the pour point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b — Define the point-scale domain\n",
    "watershed_path = symfluence.managers['domain'].define_domain()\n",
    "print(\"✅ Domain definition complete\")\n",
    "print(f\"Domain file: {watershed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6170281",
   "metadata": {},
   "source": [
    "### Step 2c — Discretization (required even for 1 GRU = 1 HRU)\n",
    "Creates the **catchment HRU** artifacts required by downstream steps (still 1:1 with the GRU for point scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c — Discretization (GRUs → HRUs 1:1)\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(\"✅ Domain discretization complete\")\n",
    "print(f\"HRU file: {hru_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041ca15",
   "metadata": {},
   "source": [
    "## Step 2d — Verification & inspection (CA-NS7)\n",
    "We verify the expected shapefiles in standardized locations, then draw a minimal GRU–HRU overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ba946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2d — Verify domain outputs and inspect geometry\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "# 1) Read config to derive data & domain paths\n",
    "with open(\"../../0_config_files/config_fluxnet_CA-NS7.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "shp_dir    = domain_dir / \"shapefiles\"\n",
    "\n",
    "# 2) Explicit expected shapefiles for CA-NS7\n",
    "gru_fp = shp_dir / \"river_basins\" / f\"{cfg['DOMAIN_NAME']}_riverBasins_point.shp\"\n",
    "hru_fp = shp_dir / \"catchment\"     / f\"{cfg['DOMAIN_NAME']}_HRUs_GRUs.shp\"\n",
    "\n",
    "# 3) Verify presence\n",
    "for label, path in [(\"GRU\", gru_fp), (\"HRU\", hru_fp)]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"❌ Expected {label} file not found: {path}\")\n",
    "    print(f\"✅ {label} file found: {path}\")\n",
    "\n",
    "# 4) Minimal overlay plot\n",
    "gru = gpd.read_file(gru_fp)\n",
    "hru = gpd.read_file(hru_fp)\n",
    "if hru.crs != gru.crs:\n",
    "    hru = hru.to_crs(gru.crs)\n",
    "ax = gru.plot(figsize=(6, 6))\n",
    "hru.plot(ax=ax, facecolor=\"none\")\n",
    "ax.set_title(\"CA-NS7 — GRU vs HRU\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9f424",
   "metadata": {},
   "source": [
    "# Step 3 — Input preprocessing (model-agnostic)\n",
    "We prepare inputs in three small moves: 1) acquire **meteorological forcings**, 2) process **FLUXNET observations**, and 3) run **model-agnostic preprocessing** to standardize variables and time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed865902",
   "metadata": {},
   "source": [
    "### Step 3a — Acquire meteorological forcings (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a — Forcings\n",
    "#symfluence.managers['data'].acquire_forcings()\n",
    "print(\"✅ Forcing data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027de87",
   "metadata": {},
   "source": [
    "### Step 3b — Process observations (FLUXNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866eb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b — Observations\n",
    "#symfluence.managers['data'].process_observed_data()\n",
    "print(\"✅ FLUXNET observational data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce1cab",
   "metadata": {},
   "source": [
    "### Step 3c — Model-agnostic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c — Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"✅ Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cbc82",
   "metadata": {},
   "source": [
    "### Step 3d — Quick verification\n",
    "Confirm the expected folders exist and contain files (derived from configuration; no hard-coded paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "with open(\"../../0_config_files/config_fluxnet_CA-NS7.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "targets = {\n",
    "    \"forcing/raw_data\":                        domain_dir / \"forcing\" / \"raw_data\",\n",
    "    \"forcing/basin_averaged_data\":             domain_dir / \"forcing\" / \"basin_averaged_data\",\n",
    "}\n",
    "\n",
    "def count_files(p: Path) -> int:\n",
    "    return sum(1 for x in p.iterdir() if x.is_file()) if p.exists() else 0\n",
    "\n",
    "for label, path in targets.items():\n",
    "    exists = path.exists()\n",
    "    n = count_files(path)\n",
    "    status = \"✅\" if exists and n > 0 else (\"⚠️ empty\" if exists else \"❌ missing\")\n",
    "    suffix = f\"({n} files)\" if exists else \"\"\n",
    "    print(f\"{status} {label}  {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9aced",
   "metadata": {},
   "source": [
    "# Step 4 — Model-specific preprocessing & model run (SUMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092289aa",
   "metadata": {},
   "source": [
    "### Step 4a — SUMMA-specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a — SUMMA-specific preprocessing\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"✅ Model-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bedaa6",
   "metadata": {},
   "source": [
    "## Step 4b — Instantiate & run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d32fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4b — Instantiate & run SUMMA\n",
    "print(f\"Running {symfluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation…\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"✅ Point-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fbcd7",
   "metadata": {},
   "source": [
    "### Step 4c — Quick verification\n",
    "Print where SUMMA inputs and run outputs were written (paths are derived from the configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "with open(\"../../0_config_files/config_fluxnet_CA-NS7.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "# Common locations used by the model manager\n",
    "summa_in   = domain_dir / \"forcing\" / \"SUMMA_input\"\n",
    "results    = domain_dir / \"simulations\" / cfg['EXPERIMENT_ID'] / 'SUMMA'\n",
    "\n",
    "print(\"SUMMA input dir:\", summa_in if summa_in.exists() else \"(not found)\")\n",
    "print(\"Results dir:\",    results if results.exists()    else \"(not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c950c",
   "metadata": {},
   "source": [
    "# Step 5 — ET & H Validation (FLUXNET vs Simulation)\n",
    "We compute basic metrics and draw quick comparisons between **observed** and **simulated** latent heat/ET and sensible heat. The code is resilient to different variable names in SUMMA outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d98cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5 — ET & H Validation (FLUXNET vs Simulation) — robust time handling + 1D-safe\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml, pandas as pd, numpy as np, xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Paths from config ---\n",
    "with open(\"../../0_config_files/config_fluxnet_CA-NS7.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "data_dir   = Path(cfg[\"SYMFLUENCE_DATA_DIR\"])\n",
    "domain_dir = data_dir / f\"domain_{cfg['DOMAIN_NAME']}\"\n",
    "\n",
    "# ===== Helpers =================================================================\n",
    "\n",
    "def find_time_coord(ds: xr.Dataset) -> str:\n",
    "    \"\"\"Find a time-like coordinate/dimension name in ds.\"\"\"\n",
    "    # exact match first\n",
    "    if \"time\" in ds.coords or \"time\" in ds.dims:\n",
    "        return \"time\"\n",
    "    # any coord containing 'time'\n",
    "    for c in list(ds.coords) + list(ds.dims):\n",
    "        if re.search(r\"time\", c, re.I):\n",
    "            return c\n",
    "    # look for 1D datetime-like variables\n",
    "    for name, var in ds.variables.items():\n",
    "        if var.ndim == 1 and re.search(r\"time\", name, re.I):\n",
    "            return name\n",
    "    raise KeyError(\"No time-like coordinate found in dataset.\")\n",
    "\n",
    "def parse_spinup_end(cfg_dict) -> datetime | None:\n",
    "    \"\"\"Parse SPINUP_PERIOD like 'YYYY-MM-DD, YYYY-MM-DD' and return the end date (+1 day).\"\"\"\n",
    "    s = cfg_dict.get(\"SPINUP_PERIOD\", \"\")\n",
    "    if not s or \",\" not in s:\n",
    "        return None\n",
    "    try:\n",
    "        start_s, end_s = [x.strip() for x in s.split(\",\")]\n",
    "        end_dt = pd.to_datetime(end_s).to_pydatetime()\n",
    "        return end_dt + timedelta(days=1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def reduce_to_time_series(da: xr.DataArray, time_dim: str) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Ensure da -> 1D over time_dim:\n",
    "      1) .squeeze(drop=True) to remove size-1 dims,\n",
    "      2) if still has extra dims besides time_dim, average over them.\n",
    "    Returns an xarray DataArray with dims (time_dim,).\n",
    "    \"\"\"\n",
    "    da = da.squeeze(drop=True)\n",
    "    # rename time-like to time_dim if needed\n",
    "    if time_dim not in da.dims:\n",
    "        # try swapping a coordinate that equals time_dim\n",
    "        for d in da.dims:\n",
    "            if d.lower() == time_dim.lower():\n",
    "                da = da.rename({d: time_dim})\n",
    "                break\n",
    "    if time_dim not in da.dims:\n",
    "        # Final fallback if time is a coord not a dim\n",
    "        if time_dim in da.coords and da.dims:\n",
    "            da = da.swap_dims({da.dims[0]: time_dim})\n",
    "        else:\n",
    "            raise ValueError(f\"Variable lacks a '{time_dim}' dimension: dims={da.dims}\")\n",
    "    other_dims = [d for d in da.dims if d != time_dim]\n",
    "    if other_dims:\n",
    "        da = da.mean(dim=other_dims, skipna=True)\n",
    "    return da\n",
    "\n",
    "def to_series_time1d(da: xr.DataArray, time_dim: str) -> pd.Series:\n",
    "    \"\"\"Convert to 1D time series safely.\"\"\"\n",
    "    da1 = reduce_to_time_series(da, time_dim)\n",
    "    # direct to_series may create multiindex if coords linger; rebuild index explicitly\n",
    "    idx = pd.to_datetime(da1[time_dim].values)\n",
    "    return pd.Series(da1.values, index=idx)\n",
    "\n",
    "def pick_var(candidates, ds_vars):\n",
    "    \"\"\"Return the first candidate present in ds_vars (set/list).\"\"\"\n",
    "    return next((v for v in candidates if v in ds_vars), None)\n",
    "\n",
    "# ===== Open dataset & choose evaluation window =================================\n",
    "\n",
    "# Find a daily SUMMA output (e.g., *_day.nc) under the domain folder\n",
    "nc_files = list(domain_dir.rglob(\"*_day.nc\"))\n",
    "if not nc_files:\n",
    "    raise FileNotFoundError(\"No daily netCDF found (pattern '*_day.nc').\")\n",
    "nc_path = nc_files[0]\n",
    "\n",
    "ds = xr.open_dataset(nc_path)\n",
    "time_dim = find_time_coord(ds)\n",
    "\n",
    "# Prefer config SPINUP_PERIOD end as the slice start; fallback to min-year + 1\n",
    "start_dt_cfg = parse_spinup_end(cfg)\n",
    "if start_dt_cfg is None:\n",
    "    # fallback: use first valid time + 1 year\n",
    "    if ds[time_dim].size == 0:\n",
    "        raise ValueError(\"Dataset has zero-length time dimension.\")\n",
    "    t0 = pd.to_datetime(ds[time_dim].values[0]).to_pydatetime()\n",
    "    start_dt_cfg = datetime(t0.year + 1, 1, 1)\n",
    "\n",
    "ds_eval = ds\n",
    "try:\n",
    "    ds_eval = ds.sel({time_dim: slice(start_dt_cfg, None)})\n",
    "    if ds_eval[time_dim].size == 0:\n",
    "        # if slice produced empty set, keep the full ds\n",
    "        print(\"⚠️ Time slice after SPINUP produced no data; using full dataset.\")\n",
    "        ds_eval = ds\n",
    "except Exception:\n",
    "    print(\"⚠️ Could not slice by time; using full dataset.\")\n",
    "    ds_eval = ds\n",
    "\n",
    "# ===== Candidate lists =========================================================\n",
    "\n",
    "# ET or LE (LE will be converted to ET)\n",
    "le_candidates = [\"Qle\", \"qle\", \"latentHeatFlux\", \"scalarLatHeatTotal\", \"LE_sim\", \"LE\"]\n",
    "et_candidates = [\"ET\", \"evapotranspiration\", \"evspsbl\", \"tEvap\", \"et_mm_day\"]\n",
    "\n",
    "# Sensible heat H\n",
    "h_candidates  = [\"Qh\", \"qh\", \"sensibleHeatFlux\", \"H_sim\", \"H\"]\n",
    "\n",
    "# For energy balance fallback: Rn - G - LE\n",
    "rn_candidates = [\n",
    "    \"scalarNetRadiation\", \"netRadiation\", \"Qnet\", \"RNET_sim\", \"Rnet_sim\", \"Rn\", \"Rn_surface\"\n",
    "]\n",
    "g_candidates  = [\"groundHeatFlux\", \"Qg\", \"qg\", \"G_sim\", \"G\"]\n",
    "\n",
    "ds_vars = set(list(ds_eval.data_vars))\n",
    "\n",
    "# ===== ET/LE from simulation ===================================================\n",
    "\n",
    "le_var = pick_var(le_candidates, ds_vars)\n",
    "et_var = pick_var(et_candidates, ds_vars)\n",
    "\n",
    "if et_var is None and le_var is None:\n",
    "    # regex sweep for odd names\n",
    "    le_var = next((v for v in ds_vars if re.search(r\"\\b(le|latent)\\b\", v, re.I)), None)\n",
    "    et_var = next((v for v in ds_vars if re.search(r\"\\bet\\b|evap\", v, re.I)), None)\n",
    "\n",
    "if et_var is None and le_var is not None:\n",
    "    # Convert LE (W/m^2) -> ET (mm/day)\n",
    "    et_series_sim = -to_series_time1d(ds_eval[le_var], time_dim) * 0.0353\n",
    "elif et_var is not None:\n",
    "    et_series_sim = -to_series_time1d(ds_eval[et_var], time_dim)\n",
    "else:\n",
    "    raise KeyError(\n",
    "        f\"Could not find ET or LE variables. \"\n",
    "        f\"Tried ET {et_candidates} and LE {le_candidates} (and regex fallbacks).\"\n",
    "    )\n",
    "\n",
    "# ===== H from simulation: direct var OR fallback from energy balance ===========\n",
    "h_var = pick_var(h_candidates, ds_vars)\n",
    "if h_var is not None:\n",
    "    h_series_sim = to_series_time1d(ds_eval[h_var], time_dim)\n",
    "else:\n",
    "    # Fallback: H ≈ Rn - G - LE   (use LE in W/m^2; if we only have ET, convert back)\n",
    "    rn_var = pick_var(rn_candidates, ds_vars)\n",
    "    g_var  = pick_var(g_candidates,  ds_vars)\n",
    "\n",
    "    rn_ok = rn_var is not None\n",
    "    g_ok  = g_var  is not None\n",
    "\n",
    "    # Need a W/m^2 latent heat series for fallback\n",
    "    if le_var is not None:\n",
    "        le_wm2_series = to_series_time1d(ds_eval[le_var], time_dim)\n",
    "    else:\n",
    "        # Convert ET back to LE (mm/day -> W/m^2) using inverse of 0.0353\n",
    "        le_wm2_series = et_series_sim / 0.0353\n",
    "\n",
    "    if rn_ok and g_ok:\n",
    "        rn_series = to_series_time1d(ds_eval[rn_var], time_dim)\n",
    "        g_series  = to_series_time1d(ds_eval[g_var],  time_dim)\n",
    "\n",
    "        # Align to common index\n",
    "        idx = rn_series.index\n",
    "        common_idx = idx.intersection(g_series.index).intersection(le_wm2_series.index)\n",
    "        if len(common_idx) == 0:\n",
    "            h_series_sim = None\n",
    "        else:\n",
    "            rn_a = rn_series.loc[common_idx].astype(float)\n",
    "            g_a  = g_series.loc[common_idx].astype(float)\n",
    "            le_a = le_wm2_series.loc[common_idx].astype(float)\n",
    "            h_series_sim = (rn_a - g_a - le_a)\n",
    "    else:\n",
    "        h_series_sim = None  # No sensible heat estimate possible\n",
    "\n",
    "# ===== Load FLUXNET observations (processed) ==================================\n",
    "\n",
    "proc_dir = domain_dir / \"observations\" / \"energy_fluxes\" / \"processed\"\n",
    "cand = sorted(proc_dir.rglob(\"*.csv\"))\n",
    "if not cand:\n",
    "    raise FileNotFoundError(f\"No processed FLUXNET CSV found under: {proc_dir}\")\n",
    "obs_path = cand[0]\n",
    "obs = pd.read_csv(obs_path)\n",
    "\n",
    "# Make a timestamp; try common names or TIMESTAMP_START\n",
    "ts_col = next((c for c in obs.columns if re.search(r\"timestamp|time|date\", c, re.I)), None)\n",
    "if ts_col is None and \"TIMESTAMP_START\" in obs.columns:\n",
    "    obs[\"timestamp\"] = pd.to_datetime(\n",
    "        obs[\"TIMESTAMP_START\"].astype(str), format=\"%Y%m%d%H%M\", errors=\"coerce\"\n",
    "    )\n",
    "    ts_col = \"timestamp\"\n",
    "elif ts_col is not None:\n",
    "    obs[ts_col] = pd.to_datetime(obs[ts_col])\n",
    "else:\n",
    "    raise KeyError(\"Could not find a timestamp column in processed FLUXNET data.\")\n",
    "\n",
    "obs = obs.dropna(subset=[ts_col]).set_index(ts_col).sort_index()\n",
    "\n",
    "# Observation variables: LE and H\n",
    "le_obs_candidates = [\"LE_F_MDS\", \"LE\", \"ET_from_LE_mm_per_day\"]\n",
    "h_obs_candidates  = [\"H_F_MDS\", \"H\"]\n",
    "\n",
    "def first_available(cols, df):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "le_obs_col = first_available(le_obs_candidates, obs)\n",
    "h_obs_col  = first_available(h_obs_candidates,  obs)\n",
    "\n",
    "if le_obs_col is None:\n",
    "    raise KeyError(f\"Missing expected FLUXNET LE column. Tried {le_obs_candidates}\")\n",
    "\n",
    "# Convert LE (W/m^2) to ET (mm/day) if needed\n",
    "if le_obs_col in (\"LE_F_MDS\", \"LE\"):\n",
    "    obs[\"ET_obs_mm_day\"] = obs[le_obs_col] * 0.0353\n",
    "elif le_obs_col == \"ET_from_LE_mm_per_day\":\n",
    "    obs[\"ET_obs_mm_day\"] = obs[le_obs_col]\n",
    "else:\n",
    "    obs[\"ET_obs_mm_day\"] = obs[le_obs_col] * 0.0353\n",
    "\n",
    "# Daily aggregation for obs (if higher frequency)\n",
    "obs_daily = obs.resample(\"1D\").mean(numeric_only=True)\n",
    "et_obs_series = obs_daily[\"ET_obs_mm_day\"].dropna()\n",
    "h_obs_series  = obs_daily[h_obs_col].dropna() if h_obs_col in obs_daily.columns else None\n",
    "\n",
    "# ===== Metrics & plots =========================================================\n",
    "\n",
    "def align(a: pd.Series, b: pd.Series):\n",
    "    idx = a.index.intersection(b.index)\n",
    "    a2 = pd.Series(a, index=idx).astype(float)\n",
    "    b2 = pd.Series(b, index=idx).astype(float)\n",
    "    return a2, b2\n",
    "\n",
    "def rmse(a, b):\n",
    "    d = (a - b).to_numpy(dtype=float)\n",
    "    return float(np.sqrt(np.nanmean(d**2)))\n",
    "\n",
    "def bias(a, b):\n",
    "    return float((a - b).mean())\n",
    "\n",
    "def corr(a, b):\n",
    "    return float(pd.Series(a).corr(pd.Series(b)))\n",
    "\n",
    "# Compute ET metrics\n",
    "et_sim_a, et_obs_a = align(et_series_sim, et_obs_series)\n",
    "et_metrics = dict(RMSE=round(rmse(et_sim_a, et_obs_a), 3),\n",
    "                  Bias=round(bias(et_sim_a, et_obs_a), 3),\n",
    "                  r=round(corr(et_sim_a, et_obs_a), 3))\n",
    "print(\"ET metrics:\", et_metrics)\n",
    "\n",
    "# Compute H metrics only if we have both obs and sim\n",
    "h_metrics = None\n",
    "h_sim_a = h_obs_a = None\n",
    "if h_series_sim is not None and h_obs_series is not None:\n",
    "    h_sim_a, h_obs_a = align(h_series_sim, h_obs_series)\n",
    "    if len(h_sim_a) > 0:\n",
    "        h_metrics = dict(RMSE=round(rmse(h_sim_a, h_obs_a), 3),\n",
    "                         Bias=round(bias(h_sim_a, h_obs_a), 3),\n",
    "                         r=round(corr(h_sim_a, h_obs_a), 3))\n",
    "        print(\"H  metrics:\", h_metrics)\n",
    "    else:\n",
    "        print(\"H metrics: (no overlapping dates)\")\n",
    "\n",
    "# --- Plots ---\n",
    "if h_metrics is None:\n",
    "    # ET-only figure if H is unavailable\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.plot(et_obs_a.index, et_obs_a.values, label=\"ET obs (mm/d)\")\n",
    "    plt.plot(et_sim_a.index, et_sim_a.values, label=\"ET sim (mm/d)\")\n",
    "    plt.ylabel(\"mm/day\")\n",
    "    plt.title(\"Evapotranspiration\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ET scatter\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(et_obs_a, et_sim_a, s=6)\n",
    "    plt.xlabel(\"ET obs (mm/d)\")\n",
    "    plt.ylabel(\"ET sim (mm/d)\")\n",
    "    plt.title(f\"ET (r={et_metrics['r']})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Time series (ET & H)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9, 6), sharex=True)\n",
    "    axes[0].plot(et_obs_a.index, et_obs_a.values, label=\"ET obs (mm/d)\")\n",
    "    axes[0].plot(et_sim_a.index, et_sim_a.values, label=\"ET sim (mm/d)\")\n",
    "    axes[0].set_ylabel(\"mm/day\")\n",
    "    axes[0].set_title(\"Evapotranspiration\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(h_obs_a.index, h_obs_a.values, label=\"H obs (W/m²)\")\n",
    "    axes[1].plot(h_sim_a.index, h_sim_a.values, label=\"H sim (W/m²)\")\n",
    "    axes[1].set_ylabel(\"W m$^{-2}$\")\n",
    "    axes[1].set_title(\"Sensible Heat Flux\")\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter quick skill view\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes2[0].scatter(et_obs_a, et_sim_a, s=6)\n",
    "    axes2[0].set_xlabel(\"ET obs (mm/d)\")\n",
    "    axes2[0].set_ylabel(\"ET sim (mm/d)\")\n",
    "    axes2[0].set_title(f\"ET (r={et_metrics['r']})\")\n",
    "\n",
    "    axes2[1].scatter(h_obs_a, h_sim_a, s=6)\n",
    "    axes2[1].set_xlabel(\"H obs (W/m²)\")\n",
    "    axes2[1].set_ylabel(\"H sim (W/m²)\")\n",
    "    axes2[1].set_title(f\"H (r={h_metrics['r']})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2808fd0-99b1-46de-a7a1-741b7dc9e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b — Run calibration \n",
    "\n",
    "results_file = symfluence.managers['optimization'].calibrate_model()  \n",
    "print(\"Calibration results file:\", results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CONFLUENCE root venv)",
   "language": "python",
   "name": "confluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
