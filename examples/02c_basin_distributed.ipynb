{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Elevation-Based HRU Discretization\n",
    "\n",
    "This notebook demonstrates elevation-based HRU discretization, building on the distributed domain from Tutorial 3. We'll:\n",
    "\n",
    "1. Use the existing GRUs from Tutorial 3\n",
    "2. Apply elevation-based discretization\n",
    "3. Run the model\n",
    "4. Compare results with lumped and GRU-based approaches\n",
    "\n",
    "**Prerequisites**: Tutorial 3 must be completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import shutil\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize CONFLUENCE\n",
    "First, let's set up our directories and load the configuration. We'll modify the configuration from Tutorial 3 to use elevation-based discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # â† User should modify this path\n",
    "\n",
    "# Verify paths exist\n",
    "if not CONFLUENCE_CODE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"CONFLUENCE code directory not found: {CONFLUENCE_CODE_DIR}\")\n",
    "\n",
    "if not CONFLUENCE_DATA_DIR.exists():\n",
    "    print(f\"Data directory doesn't exist. Creating: {CONFLUENCE_DATA_DIR}\")\n",
    "    CONFLUENCE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the distributed configuration from Tutorial 3\n",
    "distributed_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_distributed.yaml'\n",
    "if not distributed_config_path.exists():\n",
    "    raise FileNotFoundError(\"Tutorial 3 must be run first! Distributed config file not found.\")\n",
    "\n",
    "# Read config file\n",
    "with open(distributed_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update core paths\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Modify for elevation-based discretization\n",
    "config_dict['DOMAIN_NAME'] = 'Bow_at_Banff_elevation'\n",
    "config_dict['EXPERIMENT_ID'] = 'elevation_tutorial'\n",
    "config_dict['DOMAIN_DISCRETIZATION'] = 'elevation'  # Key change!\n",
    "config_dict['ELEVATION_BAND_SIZE'] = 200  # 200m bands\n",
    "config_dict['MIN_HRU_SIZE'] = 4  # 4 kmÂ² minimum\n",
    "# Keep SPATIAL_MODE as 'Distributed'\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "elevation_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_elevation.yaml'\n",
    "with open(elevation_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(elevation_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Discretization Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"Elevation Band Size: {confluence.config['ELEVATION_BAND_SIZE']} m\")\n",
    "print(f\"Minimum HRU Size: {confluence.config['MIN_HRU_SIZE']} kmÂ²\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Setup - Organizing the Modeling Workflow\n",
    "\n",
    "First, we'll establish a well-organized project structure, similar to what we did in Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  ðŸ“ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Copy Domain Data from Tutorial 3\n",
    "\n",
    "We'll reuse the existing domain and GRUs from Tutorial 3 (distributed model) instead of starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source domain from Tutorial 3\n",
    "source_domain = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed'\n",
    "\n",
    "# Check if Tutorial 3 domain exists\n",
    "if not source_domain.exists():\n",
    "    raise FileNotFoundError(\"Tutorial 3 domain not found! Please run Tutorial 3 first.\")\n",
    "\n",
    "# Copy necessary directories from Tutorial 3\n",
    "print(\"=== Step 2: Copying Domain Data from Tutorial 3 ===\")\n",
    "dirs_to_copy = ['shapefiles', 'attributes']\n",
    "\n",
    "for dir_name in dirs_to_copy:\n",
    "    source_dir = source_domain / dir_name\n",
    "    target_dir = project_dir / dir_name\n",
    "    \n",
    "    if source_dir.exists():\n",
    "        print(f\"Copying {dir_name} from Tutorial 3...\")\n",
    "        # Create target directory if it doesn't exist\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Copy files\n",
    "        for src_file in source_dir.glob('**/*'):\n",
    "            if src_file.is_file():\n",
    "                # Calculate relative path\n",
    "                rel_path = src_file.relative_to(source_dir)\n",
    "                dest_file = target_dir / rel_path\n",
    "                \n",
    "                # Create parent directories if needed\n",
    "                dest_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Copy the file, replacing 'distributed' with 'elevation' in filename\n",
    "                shutil.copy2(src_file, dest_file)\n",
    "                if 'distributed' in dest_file.name:\n",
    "                    new_name = dest_file.parent / dest_file.name.replace('distributed', 'elevation')\n",
    "                    dest_file.rename(new_name)\n",
    "    else:\n",
    "        print(f\"Warning: {source_dir} not found in Tutorial 3 domain.\")\n",
    "\n",
    "print(\"\\nâœ“ Domain data copied from Tutorial 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify GRU Boundaries\n",
    "\n",
    "Let's check the existing GRU boundaries that we'll use as a basis for our elevation-based HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check river basins (GRUs) and network\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "basin_files = list(basin_path.glob('*.shp'))\n",
    "network_files = list(network_path.glob('*.shp'))\n",
    "\n",
    "if basin_files and network_files:\n",
    "    # Load data\n",
    "    basins = gpd.read_file(basin_files[0])\n",
    "    rivers = gpd.read_file(network_files[0])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot basins (GRUs)\n",
    "    basins.plot(ax=ax, column='GRU_ID', cmap='viridis', \n",
    "               alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Plot river network\n",
    "    rivers.plot(ax=ax, color='blue', linewidth=1.5)\n",
    "    \n",
    "    # Format plot\n",
    "    ax.set_title(f'GRU Boundaries for Elevation-Based Discretization\\n({len(basins)} Sub-basins)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Add colorbar for GRU IDs\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                             norm=plt.Normalize(vmin=basins['GRU_ID'].min(), \n",
    "                                               vmax=basins['GRU_ID'].max()))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('GRU ID', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of GRUs: {len(basins)}\")\n",
    "else:\n",
    "    print(\"GRU files not found. Make sure Tutorial 3 was completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Elevation-Based HRUs\n",
    "\n",
    "Now we'll apply elevation-based discretization to create HRUs within each GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Create Elevation-Based HRUs\n",
    "print(\"=== Step 3: Creating Elevation-Based HRUs ===\")\n",
    "print(f\"Discretization Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"Elevation Band Size: {confluence.config['ELEVATION_BAND_SIZE']} m\")\n",
    "print(f\"Minimum HRU Size: {confluence.config['MIN_HRU_SIZE']} kmÂ²\")\n",
    "\n",
    "# Apply discretization\n",
    "print(\"\\nApplying elevation-based discretization...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Load and analyze the resulting HRU shapefile\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "if catchment_path.exists():\n",
    "    hru_files = list(catchment_path.glob('*.shp'))\n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        \n",
    "        # Store the GRU GeoDataFrame for later use\n",
    "        gru_gdf = gpd.read_file(basin_files[0])\n",
    "        \n",
    "        print(f\"\\nâœ“ Created elevation-based HRUs\")\n",
    "        print(f\"Number of HRUs: {len(hru_gdf)}\")\n",
    "        print(f\"Number of GRUs: {hru_gdf['GRU_ID'].nunique()}\")\n",
    "        \n",
    "        # Calculate HRUs per GRU\n",
    "        hru_counts = hru_gdf.groupby('GRU_ID').size()\n",
    "        avg_hrus_per_gru = hru_counts.mean()\n",
    "        print(f\"Average HRUs per GRU: {avg_hrus_per_gru:.1f}\")\n",
    "        \n",
    "        # Show the first few GRUs with their HRU counts\n",
    "        print(\"\\nHRUs per GRU (first 10):\")\n",
    "        for gru_id, count in hru_counts.head(10).items():\n",
    "            print(f\"  GRU {gru_id}: {count} HRUs\")\n",
    "else:\n",
    "    print(\"Failed to create elevation-based HRUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Elevation-Based HRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the HRUs\n",
    "if 'hru_gdf' in locals() and len(hru_gdf) > 0:\n",
    "    # Check if mean elevation is available in the data\n",
    "    has_elevation = 'mean_elev' in hru_gdf.columns\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Plot HRUs colored by elevation if available, otherwise by HRU ID\n",
    "    if has_elevation:\n",
    "        hru_gdf.plot(ax=ax, column='mean_elev', cmap='terrain', \n",
    "                   edgecolor='gray', linewidth=0.5, alpha=0.7,\n",
    "                   legend=True, legend_kwds={'label': 'Mean Elevation (m)'})\n",
    "    else:\n",
    "        hru_gdf.plot(ax=ax, column='HRU_ID', cmap='viridis', \n",
    "                   edgecolor='gray', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Overlay GRU boundaries\n",
    "    gru_gdf.boundary.plot(ax=ax, color='red', linewidth=1)\n",
    "    \n",
    "    # Add river network for context\n",
    "    if 'rivers' in locals():\n",
    "        rivers.plot(ax=ax, color='blue', linewidth=1)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(f'Elevation-Based HRUs\\n{len(hru_gdf)} HRUs in {hru_gdf[\"GRU_ID\"].nunique()} GRUs',\n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No HRUs available to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete the Modeling Workflow\n",
    "\n",
    "Now we'll complete the model setup and run the simulation using our elevation-based HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Process Observed Data\n",
    "print(\"=== Step 4: Processing Observed Data ===\")\n",
    "\n",
    "# Check if we can reuse observed data from previous tutorials\n",
    "obs_dir = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed' / 'observations' / 'streamflow' / 'preprocessed'\n",
    "if obs_dir.exists() and list(obs_dir.glob('*.csv')):\n",
    "    print(\"Reusing observed data from Tutorial 3...\")\n",
    "    \n",
    "    # Copy observed data\n",
    "    target_dir = project_dir / 'observations' / 'streamflow' / 'preprocessed'\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for src_file in obs_dir.glob('*.csv'):\n",
    "        dest_file = target_dir / src_file.name.replace('distributed', 'elevation')\n",
    "        shutil.copy2(src_file, dest_file)\n",
    "        print(f\"Copied {src_file.name} to {dest_file.name}\")\n",
    "else:\n",
    "    print(\"Processing observed streamflow data from scratch...\")\n",
    "    confluence.managers['data'].process_observed_data()\n",
    "\n",
    "print(\"\\nâœ“ Observed data processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Acquire and Process Forcing Data\n",
    "print(\"=== Step 5: Acquiring and Processing Forcing Data ===\")\n",
    "\n",
    "# Check if we can reuse forcing data from previous tutorials\n",
    "forcing_dir = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed' / 'forcing'\n",
    "if forcing_dir.exists():\n",
    "    print(\"Reusing forcing data from Tutorial 3...\")\n",
    "    \n",
    "    # Copy forcing data\n",
    "    target_dir = project_dir / 'forcing'\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy all subdirectories\n",
    "    for src_dir in forcing_dir.iterdir():\n",
    "        if src_dir.is_dir():\n",
    "            dest_dir = target_dir / src_dir.name\n",
    "            if not dest_dir.exists():\n",
    "                shutil.copytree(src_dir, dest_dir)\n",
    "            print(f\"Copied {src_dir.name} forcing data\")\n",
    "    \n",
    "    # Run model-agnostic preprocessing to handle the new HRU structure\n",
    "    print(\"\\nRunning model-agnostic preprocessing for new HRU structure...\")\n",
    "    confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "else:\n",
    "    print(\"Acquiring forcing data from scratch...\")\n",
    "    confluence.managers['data'].acquire_forcings()\n",
    "    print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "    confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nâœ“ Forcing data processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Prepare Model-Specific Files\n",
    "print(\"=== Step 6: Preparing Model-Specific Files ===\")\n",
    "\n",
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "print(f\"This step will generate configuration files for {len(hru_gdf)} HRUs across {hru_gdf['GRU_ID'].nunique()} GRUs\")\n",
    "\n",
    "# Preprocess models\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nâœ“ Model-specific preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Run the Model\n",
    "print(\"=== Step 7: Running the Model ===\")\n",
    "\n",
    "# Run the model\n",
    "print(f\"Running {confluence.config['HYDROLOGICAL_MODEL']} with elevation-based HRUs...\")\n",
    "print(\"Note: This may take some time depending on the number of HRUs.\")\n",
    "print(f\"Number of HRUs: {len(hru_gdf)}\")\n",
    "\n",
    "# You can comment this out if you want to skip the actual model run\n",
    "confluence.managers['model'].run_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONFLUENCE (HPC Modules)",
   "language": "python",
   "name": "conf-env-hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
