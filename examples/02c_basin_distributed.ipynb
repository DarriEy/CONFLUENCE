{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Elevation-Based HRU Discretization\n",
    "\n",
    "This notebook demonstrates elevation-based HRU discretization, building on the distributed domain from Tutorial 3. We'll:\n",
    "\n",
    "1. Use the existing GRUs from Tutorial 3\n",
    "2. Apply elevation-based discretization\n",
    "3. Run the model\n",
    "4. Compare results with lumped and GRU-based approaches\n",
    "\n",
    "**Prerequisites**: Tutorial 3 must be completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import shutil\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize CONFLUENCE\n",
    "First, let's set up our directories and load the configuration. We'll modify the configuration from Tutorial 3 to use elevation-based discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # â† User should modify this path\n",
    "\n",
    "# Verify paths exist\n",
    "if not CONFLUENCE_CODE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"CONFLUENCE code directory not found: {CONFLUENCE_CODE_DIR}\")\n",
    "\n",
    "if not CONFLUENCE_DATA_DIR.exists():\n",
    "    print(f\"Data directory doesn't exist. Creating: {CONFLUENCE_DATA_DIR}\")\n",
    "    CONFLUENCE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the distributed configuration from Tutorial 3\n",
    "distributed_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_distributed.yaml'\n",
    "if not distributed_config_path.exists():\n",
    "    raise FileNotFoundError(\"Tutorial 3 must be run first! Distributed config file not found.\")\n",
    "\n",
    "# Read config file\n",
    "with open(distributed_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update core paths\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Modify for elevation-based discretization\n",
    "config_dict['DOMAIN_NAME'] = 'Bow_at_Banff_elevation'\n",
    "config_dict['EXPERIMENT_ID'] = 'elevation_tutorial'\n",
    "config_dict['DOMAIN_DISCRETIZATION'] = 'elevation'  # Key change!\n",
    "config_dict['ELEVATION_BAND_SIZE'] = 200  # 200m bands\n",
    "config_dict['MIN_HRU_SIZE'] = 4  # 4 kmÂ² minimum\n",
    "# Keep SPATIAL_MODE as 'Distributed'\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "elevation_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_elevation.yaml'\n",
    "with open(elevation_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(elevation_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Discretization Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"Elevation Band Size: {confluence.config['ELEVATION_BAND_SIZE']} m\")\n",
    "print(f\"Minimum HRU Size: {confluence.config['MIN_HRU_SIZE']} kmÂ²\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup - Organizing the Modeling Workflow\n",
    "\n",
    "First, we'll establish a well-organized project structure, similar to what we did in Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  ðŸ“ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Domain Data from Tutorial 3\n",
    "\n",
    "We'll reuse the existing domain and GRUs from Tutorial 3 (distributed model) instead of starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source domain from Tutorial 3\n",
    "source_domain = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed'\n",
    "\n",
    "# Check if Tutorial 3 domain exists\n",
    "if not source_domain.exists():\n",
    "    raise FileNotFoundError(\"Tutorial 3 domain not found! Please run Tutorial 3 first.\")\n",
    "\n",
    "# Copy necessary directories from Tutorial 3\n",
    "print(\"=== Step 2: Copying Domain Data from Tutorial 3 ===\")\n",
    "dirs_to_copy = ['shapefiles', 'attributes']\n",
    "\n",
    "for dir_name in dirs_to_copy:\n",
    "    source_dir = source_domain / dir_name\n",
    "    target_dir = project_dir / dir_name\n",
    "    \n",
    "    if source_dir.exists():\n",
    "        print(f\"Copying {dir_name} from Tutorial 3...\")\n",
    "        # Create target directory if it doesn't exist\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Copy files\n",
    "        for src_file in source_dir.glob('**/*'):\n",
    "            if src_file.is_file():\n",
    "                # Calculate relative path\n",
    "                rel_path = src_file.relative_to(source_dir)\n",
    "                dest_file = target_dir / rel_path\n",
    "                \n",
    "                # Create parent directories if needed\n",
    "                dest_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Copy the file, replacing 'distributed' with 'elevation' in filename\n",
    "                shutil.copy2(src_file, dest_file)\n",
    "                if 'distributed' in dest_file.name:\n",
    "                    new_name = dest_file.parent / dest_file.name.replace('distributed', 'elevation')\n",
    "                    dest_file.rename(new_name)\n",
    "    else:\n",
    "        print(f\"Warning: {source_dir} not found in Tutorial 3 domain.\")\n",
    "\n",
    "print(\"\\nâœ“ Domain data copied from Tutorial 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GRU Boundaries\n",
    "\n",
    "Let's check the existing GRU boundaries that we'll use as a basis for our elevation-based HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check river basins (GRUs) and network\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "basin_files = list(basin_path.glob('*.shp'))\n",
    "network_files = list(network_path.glob('*.shp'))\n",
    "\n",
    "if basin_files and network_files:\n",
    "    # Load data\n",
    "    basins = gpd.read_file(basin_files[0])\n",
    "    rivers = gpd.read_file(network_files[0])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot basins (GRUs)\n",
    "    basins.plot(ax=ax, column='GRU_ID', cmap='viridis', \n",
    "               alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Plot river network\n",
    "    rivers.plot(ax=ax, color='blue', linewidth=1.5)\n",
    "    \n",
    "    # Format plot\n",
    "    ax.set_title(f'GRU Boundaries for Elevation-Based Discretization\\n({len(basins)} Sub-basins)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Add colorbar for GRU IDs\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                             norm=plt.Normalize(vmin=basins['GRU_ID'].min(), \n",
    "                                               vmax=basins['GRU_ID'].max()))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('GRU ID', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of GRUs: {len(basins)}\")\n",
    "else:\n",
    "    print(\"GRU files not found. Make sure Tutorial 3 was completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Elevation-Based HRUs\n",
    "\n",
    "Now we'll apply elevation-based discretization to create HRUs within each GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Discretization Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"Elevation Band Size: {confluence.config['ELEVATION_BAND_SIZE']} m\")\n",
    "print(f\"Minimum HRU Size: {confluence.config['MIN_HRU_SIZE']} kmÂ²\")\n",
    "\n",
    "# Apply discretization\n",
    "print(\"\\nApplying elevation-based discretization...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Load and analyze the resulting HRU shapefile\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "if catchment_path.exists():\n",
    "    hru_files = list(catchment_path.glob('*.shp'))\n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        \n",
    "        # Store the GRU GeoDataFrame for later use\n",
    "        gru_gdf = gpd.read_file(basin_files[0])\n",
    "        \n",
    "        print(f\"\\nâœ“ Created elevation-based HRUs\")\n",
    "        print(f\"Number of HRUs: {len(hru_gdf)}\")\n",
    "        print(f\"Number of GRUs: {hru_gdf['GRU_ID'].nunique()}\")\n",
    "        \n",
    "        # Calculate HRUs per GRU\n",
    "        hru_counts = hru_gdf.groupby('GRU_ID').size()\n",
    "        avg_hrus_per_gru = hru_counts.mean()\n",
    "        print(f\"Average HRUs per GRU: {avg_hrus_per_gru:.1f}\")\n",
    "        \n",
    "        # Show the first few GRUs with their HRU counts\n",
    "        print(\"\\nHRUs per GRU (first 10):\")\n",
    "        for gru_id, count in hru_counts.head(10).items():\n",
    "            print(f\"  GRU {gru_id}: {count} HRUs\")\n",
    "else:\n",
    "    print(\"Failed to create elevation-based HRUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Elevation-Based HRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the HRUs\n",
    "if 'hru_gdf' in locals() and len(hru_gdf) > 0:\n",
    "    # Check if mean elevation is available in the data\n",
    "    has_elevation = 'mean_elev' in hru_gdf.columns\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Plot HRUs colored by elevation if available, otherwise by HRU ID\n",
    "    if has_elevation:\n",
    "        hru_gdf.plot(ax=ax, column='mean_elev', cmap='terrain', \n",
    "                   edgecolor='gray', linewidth=0.5, alpha=0.7,\n",
    "                   legend=True, legend_kwds={'label': 'Mean Elevation (m)'})\n",
    "    else:\n",
    "        hru_gdf.plot(ax=ax, column='HRU_ID', cmap='viridis', \n",
    "                   edgecolor='gray', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Overlay GRU boundaries\n",
    "    gru_gdf.boundary.plot(ax=ax, color='red', linewidth=1)\n",
    "    \n",
    "    # Add river network for context\n",
    "    if 'rivers' in locals():\n",
    "        rivers.plot(ax=ax, color='blue', linewidth=1)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(f'Elevation-Based HRUs\\n{len(hru_gdf)} HRUs in {hru_gdf[\"GRU_ID\"].nunique()} GRUs',\n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No HRUs available to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the Modeling Workflow\n",
    "\n",
    "Now we'll complete the model setup and run the simulation using our elevation-based HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing observed streamflow data\")\n",
    "\n",
    "confluence.managers['data'].process_observed_data()\n",
    "\n",
    "print(\"\\nâœ“ Observed data processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Acquire Forcing Data\n",
    "# Check if we can reuse forcing data from previous tutorials\n",
    "forcing_dir = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_distributed' / 'forcing'\n",
    "if forcing_dir.exists():\n",
    "    print(\"Reusing forcing data from Tutorial 3...\")\n",
    "    \n",
    "    # Copy forcing data\n",
    "    target_dir = project_dir / 'forcing'\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy all subdirectories\n",
    "    for src_dir in forcing_dir.iterdir():\n",
    "        if src_dir.is_dir():\n",
    "            dest_dir = target_dir / src_dir.name\n",
    "            if not dest_dir.exists():\n",
    "                shutil.copytree(src_dir, dest_dir)\n",
    "            print(f\"Copied {src_dir.name} forcing data\")\n",
    "    \n",
    "    # Run model-agnostic preprocessing to handle the new HRU structure\n",
    "    print(\"\\nRunning model-agnostic preprocessing for new HRU structure...\")\n",
    "    confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "else:\n",
    "    print(\"Acquiring forcing data from scratch...\")\n",
    "    confluence.managers['data'].acquire_forcings()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nâœ“ Forcing data processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nâœ“ Model-specific preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Running {confluence.config['HYDROLOGICAL_MODEL']} with elevation-based HRUs...\")\n",
    "print(\"Note: This may take some time depending on the number of HRUs.\")\n",
    "\n",
    "confluence.managers['model'].run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Observed vs. Simulated Streamflow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "\n",
    "# Load and plot simulation results\n",
    "sim_path = project_dir / 'simulations' / confluence.config['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "sim_files = list(sim_path.glob('*.nc'))\n",
    "\n",
    "# Load simulation data\n",
    "print(f\"Loading simulation data from: {sim_files[0]}\")\n",
    "sim_data = xr.open_dataset(sim_files[0])\n",
    "\n",
    "# Load observation data\n",
    "obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "\n",
    "if not obs_path.exists():\n",
    "    print(f\"Warning: Observation data not found at expected path: {obs_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    alt_obs_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "        f\"domain_{config_dict['DOMAIN_NAME']}/observations/streamflow/preprocessed/*_streamflow_processed.csv\"))\n",
    "    \n",
    "    if alt_obs_paths:\n",
    "        obs_path = alt_obs_paths[0]\n",
    "        print(f\"Found alternative observation data at: {obs_path}\")\n",
    "    else:\n",
    "        print(\"No observation data found. Only simulations will be displayed.\")\n",
    "\n",
    "if obs_path.exists():\n",
    "    print(f\"Loading observation data from: {obs_path}\")\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    obs_df.set_index('datetime', inplace=True)\n",
    "    print(f\"Observation period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "else:\n",
    "    obs_df = None\n",
    "    \n",
    "# Find the segment ID for the outlet\n",
    "reach_id = int(confluence.config.get('SIM_REACH_ID', 0))\n",
    "print(f\"Using reach ID for outlet: {reach_id}\")\n",
    "\n",
    "# Find the index where reachID matches the target reach_id\n",
    "segment_indices = np.where(sim_data.reachID.values == reach_id)[0]\n",
    "\n",
    "if len(segment_indices) == 0:\n",
    "    print(f\"Error: Reach ID {reach_id} not found in simulation data\")\n",
    "    print(f\"Available reach IDs: {sim_data.reachID.values}\")\n",
    "else:\n",
    "    # Extract flow at the outlet segment using the index\n",
    "    segment_index = segment_indices[0]\n",
    "    sim_flow = sim_data.IRFroutedRunoff.isel(seg=segment_index).to_series()\n",
    "    sim_df = pd.DataFrame(sim_flow)\n",
    "    sim_df.columns = ['discharge_cms']\n",
    "\n",
    "    # Determine common time period if observations exist\n",
    "    if obs_df is not None:\n",
    "        # Align to daily timestep for comparison\n",
    "        obs_daily = obs_df.resample('D').mean()\n",
    "        sim_daily = sim_df.resample('D').mean()\n",
    "        \n",
    "        # Find overlapping time period\n",
    "        start_date = max(obs_daily.index.min(), sim_daily.index.min())\n",
    "        end_date = min(obs_daily.index.max(), sim_daily.index.max())\n",
    "        \n",
    "        # Advance start date by 1 month to skip initial spinup\n",
    "        start_date = start_date + pd.DateOffset(months=1)\n",
    "        \n",
    "        print(f\"Common data period: {start_date} to {end_date}\")\n",
    "        \n",
    "        # Filter to common period\n",
    "        obs_period = obs_daily.loc[start_date:end_date]\n",
    "        sim_period = sim_daily.loc[start_date:end_date]\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        rmse = np.sqrt(((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).mean())\n",
    "        \n",
    "        # Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "        mean_obs = obs_period['discharge_cms'].mean()\n",
    "        numerator = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).sum()\n",
    "        denominator = ((obs_period['discharge_cms'] - mean_obs)**2).sum()\n",
    "        nse = 1 - (numerator / denominator)\n",
    "        \n",
    "        # Calculate Percent Bias (PBIAS)\n",
    "        pbias = 100 * (sim_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "        \n",
    "        # Calculate Kling-Gupta Efficiency (KGE)\n",
    "        r = obs_period['discharge_cms'].corr(sim_period['discharge_cms'])  # Correlation\n",
    "        alpha = sim_period['discharge_cms'].std() / obs_period['discharge_cms'].std()  # Relative variability\n",
    "        beta = sim_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()  # Bias ratio\n",
    "        kge = 1 - ((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)**0.5\n",
    "        \n",
    "        print(f\"Performance metrics:\")\n",
    "        print(f\"  - RMSE: {rmse:.2f} mÂ³/s\")\n",
    "        print(f\"  - NSE: {nse:.2f}\")\n",
    "        print(f\"  - PBIAS: {pbias:.2f}%\")\n",
    "        print(f\"  - KGE: {kge:.2f}\")\n",
    "        \n",
    "        # Create figure with two subplots for time series and flow duration curve\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 16))\n",
    "        fig.suptitle(f\"Distributed Model Results - {confluence.config['DOMAIN_NAME'].replace('_', ' ').title()}\", \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot time series\n",
    "        ax1.plot(obs_period.index, obs_period['discharge_cms'], \n",
    "                 'b-', label='Observed', linewidth=1.5, alpha=0.7)\n",
    "        ax1.plot(sim_period.index, sim_period['discharge_cms'], \n",
    "                 'r-', label='Simulated (Distributed)', linewidth=1.5, alpha=0.7)\n",
    "            \n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "        ax1.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "        ax1.set_title('Streamflow Comparison', fontsize=14)\n",
    "        ax1.legend(loc='upper right', fontsize=10)\n",
    "        ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax1.set_facecolor('#f0f0f0')\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        \n",
    "        # Add metrics as text\n",
    "        ax1.text(0.02, 0.95, \n",
    "                 f\"RMSE: {rmse:.2f} mÂ³/s\\nNSE: {nse:.2f}\\nPBIAS: {pbias:.2f}%\\nKGE: {kge:.2f}\",\n",
    "                 transform=ax1.transAxes, \n",
    "                 fontsize=12,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        # Plot flow duration curve\n",
    "        # Sort values in descending order\n",
    "        obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "        sim_sorted = sim_period['discharge_cms'].sort_values(ascending=False)\n",
    "        \n",
    "        # Calculate exceedance probabilities\n",
    "        obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "        sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted)\n",
    "        \n",
    "        # Plot Flow Duration Curves\n",
    "        ax2.loglog(obs_ranks * 100, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "        ax2.loglog(sim_ranks * 100, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "        ax2.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "        ax2.set_title('Flow Duration Curve', fontsize=14)\n",
    "        ax2.legend(loc='best', fontsize=10)\n",
    "        ax2.grid(True, which='both', linestyle=':', alpha=0.6)\n",
    "        ax2.set_facecolor('#f0f0f0')\n",
    "        \n",
    "        # Add flow regime regions\n",
    "        ax2.axvspan(0, 20, alpha=0.2, color='blue', label='High Flows')\n",
    "        ax2.axvspan(20, 70, alpha=0.2, color='green', label='Medium Flows')\n",
    "        ax2.axvspan(70, 100, alpha=0.2, color='red', label='Low Flows')\n",
    "        \n",
    "        # Save the plot to file\n",
    "        plot_folder = project_dir / \"plots\" / \"results\"\n",
    "        plot_folder.mkdir(parents=True, exist_ok=True)\n",
    "        plot_filename = plot_folder / f\"{confluence.config['EXPERIMENT_ID']}_streamflow_comparison.png\"\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {plot_filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        # If no observations, just plot simulation\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        ax.plot(sim_df.index, sim_df['discharge_cms'], \n",
    "                color='red', linewidth=1.5, label='Simulated (Distributed)')\n",
    "        \n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('Discharge (mÂ³/s)', fontsize=12)\n",
    "        ax.set_title(f'Distributed Model Results - {confluence.config[\"DOMAIN_NAME\"].replace(\"_\", \" \").title()}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "sim_data.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONFLUENCE (HPC Modules)",
   "language": "python",
   "name": "conf-env-hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
