{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUXNET Large Sample Experiment Tutorial\n",
    "\n",
    "This notebook demonstrates how to run CONFLUENCE over multiple FLUXNET tower sites for point-scale large-sample analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "FLUXNET represents a global network of eddy covariance tower sites that measure exchanges of carbon dioxide, water vapor, and energy between the biosphere and atmosphere. Running CONFLUENCE at these point locations allows us to:\n",
    "\n",
    "- Validate model physics against direct observations\n",
    "- Compare model performance across diverse climates and ecosystems\n",
    "- Develop parameter estimation relationships\n",
    "- Test model transferability across sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the FLUXNET large sample experiment\n",
    "experiment_config = {\n",
    "    'experiment_name': 'fluxnet_tutorial',\n",
    "    'max_sites': 5,  # Number of FLUXNET sites to process\n",
    "    'dry_run': False,  # Set to True to test without submitting jobs\n",
    "    'template_config': '../CONFLUENCE/0_config_files/config_point_template.yaml',\n",
    "    'config_dir': '../CONFLUENCE/0_config_files/fluxnet',\n",
    "    'fluxnet_script': '../CONFLUENCE/9_scripts/run_towers_fluxnet.py',\n",
    "    'fluxnet_csv': 'fluxnet_transformed.csv'\n",
    "}\n",
    "\n",
    "# Create experiment directory\n",
    "experiment_dir = Path(f\"./experiments/{experiment_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(experiment_dir / 'experiment_config.yaml', 'w') as f:\n",
    "    yaml.dump(experiment_config, f)\n",
    "\n",
    "print(f\"Experiment configured: {experiment_config['experiment_name']}\")\n",
    "print(f\"Processing {experiment_config['max_sites']} FLUXNET sites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore FLUXNET Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLUXNET sites data\n",
    "fluxnet_df = pd.read_csv(experiment_config['fluxnet_csv'])\n",
    "\n",
    "print(f\"Loaded {len(fluxnet_df)} FLUXNET sites\")\n",
    "print(\"\\nColumns in dataset:\")\n",
    "for col in fluxnet_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Display first few sites\n",
    "print(\"\\nFirst 5 sites:\")\n",
    "display(fluxnet_df[['ID', 'Watershed_Name', 'KG', 'Dominant_LC', 'Area_km2']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize FLUXNET Site Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from POUR_POINT_COORDS\n",
    "coords = fluxnet_df['POUR_POINT_COORDS'].str.split('/', expand=True)\n",
    "fluxnet_df['lat'] = coords[0].astype(float)\n",
    "fluxnet_df['lon'] = coords[1].astype(float)\n",
    "\n",
    "# Create global distribution plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(fluxnet_df['lon'], fluxnet_df['lat'], \n",
    "           c='red', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.title('Global Distribution of FLUXNET Sites', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.ylabel('Latitude', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-180, 180)\n",
    "plt.ylim(-60, 80)\n",
    "\n",
    "# Add continent labels for context\n",
    "plt.text(-100, 40, 'North\\nAmerica', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "plt.text(-60, -10, 'South\\nAmerica', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "plt.text(15, 50, 'Europe', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "plt.text(20, 0, 'Africa', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "plt.text(100, 30, 'Asia', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "plt.text(135, -25, 'Australia', fontsize=10, ha='center', style='italic', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Site Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate distribution (Köppen-Geiger)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Climate type distribution\n",
    "climate_counts = fluxnet_df['KG'].value_counts()\n",
    "climate_counts.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Distribution of Köppen-Geiger Climate Types', fontsize=14)\n",
    "ax1.set_xlabel('Climate Type', fontsize=12)\n",
    "ax1.set_ylabel('Number of Sites', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Land cover distribution\n",
    "lc_counts = fluxnet_df['Dominant_LC'].value_counts()\n",
    "lc_counts.plot(kind='bar', ax=ax2, color='lightgreen', edgecolor='black')\n",
    "ax2.set_title('Distribution of Dominant Land Cover Types', fontsize=14)\n",
    "ax2.set_xlabel('Land Cover Type', fontsize=12)\n",
    "ax2.set_ylabel('Number of Sites', fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Site Characteristics Summary:\")\n",
    "print(f\"Total sites: {len(fluxnet_df)}\")\n",
    "print(f\"Climate types: {len(climate_counts)} unique\")\n",
    "print(f\"Land cover types: {len(lc_counts)} unique\")\n",
    "print(f\"Average site area: {fluxnet_df['Area_km2'].mean():.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select Sites for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sites based on criteria or randomly\n",
    "# For this tutorial, we'll select diverse climate types\n",
    "\n",
    "# Get unique climate types\n",
    "climate_types = fluxnet_df['KG'].unique()\n",
    "\n",
    "# Select one site from each climate type (up to max_sites)\n",
    "selected_sites = []\n",
    "for climate in climate_types[:experiment_config['max_sites']]:\n",
    "    site = fluxnet_df[fluxnet_df['KG'] == climate].iloc[0]\n",
    "    selected_sites.append(site)\n",
    "\n",
    "selected_df = pd.DataFrame(selected_sites)\n",
    "\n",
    "print(f\"Selected {len(selected_df)} sites for processing:\")\n",
    "display(selected_df[['ID', 'Watershed_Name', 'KG', 'Dominant_LC', 'Area_km2']])\n",
    "\n",
    "# Save selected sites\n",
    "selected_df.to_csv(experiment_dir / 'selected_sites.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config directory if it doesn't exist\n",
    "config_dir = Path(experiment_config['config_dir'])\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate configs for selected sites\n",
    "generated_configs = []\n",
    "\n",
    "for _, site in selected_df.iterrows():\n",
    "    site_id = site['ID']\n",
    "    site_name = site['Watershed_Name']\n",
    "    pour_point = site['POUR_POINT_COORDS']\n",
    "    bounding_box = site['BOUNDING_BOX_COORDS']\n",
    "    \n",
    "    # Create config file name\n",
    "    config_name = f\"config_{site_name}.yaml\"\n",
    "    config_path = config_dir / config_name\n",
    "    \n",
    "    # Generate config using the script function\n",
    "    cmd = [\n",
    "        'python', '-c',\n",
    "        f\"\"\"\n",
    "import sys\n",
    "sys.path.append('{str(confluence_path)}/9_scripts')\n",
    "from run_towers_fluxnet import generate_config_file\n",
    "generate_config_file(\n",
    "    '{experiment_config['template_config']}',\n",
    "    '{config_path}',\n",
    "    '{site_name}',\n",
    "    '{pour_point}',\n",
    "    '{bounding_box}'\n",
    ")\n",
    "\"\"\"\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        generated_configs.append(config_path)\n",
    "        print(f\"Generated config for {site_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to generate config for {site_name}: {result.stderr}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(generated_configs)} configuration files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch CONFLUENCE for Selected Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch CONFLUENCE runs\n",
    "cmd = [\n",
    "    'python', experiment_config['fluxnet_script']\n",
    "]\n",
    "\n",
    "# For dry run, add appropriate option\n",
    "if experiment_config['dry_run']:\n",
    "    print(\"DRY RUN MODE - No jobs will be submitted\")\n",
    "    # Note: the fluxnet script asks interactively, so we'll need to handle that\n",
    "\n",
    "print(f\"Launching CONFLUENCE for FLUXNET sites...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "# Execute the script\n",
    "# Note: The script requires user input, so for notebook usage we'll simulate it\n",
    "result = subprocess.run(cmd, input='n\\n' if experiment_config['dry_run'] else 'y\\n', \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "print(result.stdout[:1000] + \"...\" if len(result.stdout) > 1000 else result.stdout)\n",
    "\n",
    "# Save submission log\n",
    "with open(experiment_dir / 'submission.log', 'w') as f:\n",
    "    f.write(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitor Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SLURM job status\n",
    "def check_job_status(user=None):\n",
    "    user = user or os.environ.get('USER')\n",
    "    cmd = ['squeue', '-u', user]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "print(\"Current jobs:\")\n",
    "print(check_job_status())\n",
    "\n",
    "# Extract job IDs from submission log\n",
    "import re\n",
    "job_ids = []\n",
    "with open(experiment_dir / 'submission.log', 'r') as f:\n",
    "    content = f.read()\n",
    "    job_pattern = r'job ID: (\\d+)'\n",
    "    job_ids = re.findall(job_pattern, content)\n",
    "\n",
    "if job_ids:\n",
    "    print(f\"\\nSubmitted job IDs: {', '.join(job_ids)}\")\n",
    "else:\n",
    "    print(\"\\nNo job IDs found in submission log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Find Completed Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find completed FLUXNET simulations\n",
    "confluence_data_dir = Path(\"/work/comphyd_lab/data/CONFLUENCE_data\")\n",
    "fluxnet_dir = confluence_data_dir / \"fluxnet\"\n",
    "\n",
    "completed = []\n",
    "if fluxnet_dir.exists():\n",
    "    for domain_dir in fluxnet_dir.glob(\"domain_*\"):\n",
    "        site_name = domain_dir.name.replace(\"domain_\", \"\")\n",
    "        sim_dir = domain_dir / \"simulations\"\n",
    "        \n",
    "        # Check if simulation files exist\n",
    "        if sim_dir.exists() and list(sim_dir.rglob(\"*.nc\")):\n",
    "            completed.append({\n",
    "                'site_name': site_name,\n",
    "                'domain_dir': domain_dir,\n",
    "                'sim_dir': sim_dir\n",
    "            })\n",
    "\n",
    "print(f\"Completed simulations: {len(completed)}\")\n",
    "for site in completed:\n",
    "    print(f\"  - {site['site_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load SUMMA output\n",
    "def load_summa_output(sim_dir, variable='scalarLatHeatTotal'):\n",
    "    import xarray as xr\n",
    "    \n",
    "    output_files = list(sim_dir.rglob(\"*timestep*.nc\"))\n",
    "    if output_files:\n",
    "        ds = xr.open_dataset(output_files[0])\n",
    "        if variable in ds.variables:\n",
    "            return pd.DataFrame({\n",
    "                'time': pd.to_datetime(ds.time.values),\n",
    "                'value': ds[variable].values.flatten()\n",
    "            })\n",
    "    return None\n",
    "\n",
    "# Plot energy fluxes for completed sites\n",
    "if completed:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for site in completed[:3]:  # Plot first 3\n",
    "        data = load_summa_output(site['sim_dir'], 'scalarLatHeatTotal')\n",
    "        if data is not None:\n",
    "            # Plot daily means\n",
    "            daily = data.set_index('time').resample('D').mean()\n",
    "            ax.plot(daily.index, daily['value'], \n",
    "                   label=f\"{site['site_name']}\", \n",
    "                   linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Latent Heat Flux (W/m²)')\n",
    "    ax.set_title('Latent Heat Flux Comparison - FLUXNET Sites')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare Model with FLUXNET Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load FLUXNET observations (placeholder)\n",
    "def load_fluxnet_obs(site_name, variable='LE'):\n",
    "    # This would load actual FLUXNET observations\n",
    "    # For demo, we'll create synthetic data\n",
    "    dates = pd.date_range('2020-01-01', '2020-12-31', freq='D')\n",
    "    values = 50 + 30 * np.sin(2 * np.pi * dates.dayofyear / 365) + np.random.normal(0, 10, len(dates))\n",
    "    return pd.DataFrame({\n",
    "        'time': dates,\n",
    "        'obs': values\n",
    "    })\n",
    "\n",
    "# Compare model with observations for one site\n",
    "if completed:\n",
    "    site = completed[0]\n",
    "    \n",
    "    # Load model output\n",
    "    model_data = load_summa_output(site['sim_dir'], 'scalarLatHeatTotal')\n",
    "    \n",
    "    # Load observations (simulated)\n",
    "    obs_data = load_fluxnet_obs(site['site_name'])\n",
    "    \n",
    "    if model_data is not None:\n",
    "        # Resample to daily means\n",
    "        model_daily = model_data.set_index('time').resample('D').mean()\n",
    "        obs_daily = obs_data.set_index('time').resample('D').mean()\n",
    "        \n",
    "        # Merge datasets\n",
    "        comparison = pd.merge(model_daily, obs_daily, left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), height_ratios=[2, 1])\n",
    "        \n",
    "        # Time series\n",
    "        ax1.plot(comparison.index, comparison['obs'], label='Observed', color='black', linewidth=2)\n",
    "        ax1.plot(comparison.index, comparison['value'], label='Modeled', color='red', alpha=0.7)\n",
    "        ax1.set_ylabel('Latent Heat Flux (W/m²)')\n",
    "        ax1.set_title(f'Model vs Observations - {site[\"site_name\"]}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax2.scatter(comparison['obs'], comparison['value'], alpha=0.5)\n",
    "        ax2.plot([0, comparison[['obs', 'value']].max().max()], \n",
    "                [0, comparison[['obs', 'value']].max().max()], \n",
    "                'k--', alpha=0.5)\n",
    "        ax2.set_xlabel('Observed (W/m²)')\n",
    "        ax2.set_ylabel('Modeled (W/m²)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import r2_score, mean_squared_error\n",
    "        r2 = r2_score(comparison['obs'], comparison['value'])\n",
    "        rmse = np.sqrt(mean_squared_error(comparison['obs'], comparison['value']))\n",
    "        \n",
    "        print(f\"Performance Metrics for {site['site_name']}:\")\n",
    "        print(f\"  R²: {r2:.3f}\")\n",
    "        print(f\"  RMSE: {rmse:.1f} W/m²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cross-Site Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance across different climate types\n",
    "performance_data = []\n",
    "\n",
    "for site in completed:\n",
    "    # Get site info from original dataframe\n",
    "    site_info = fluxnet_df[fluxnet_df['Watershed_Name'] == site['site_name']].iloc[0]\n",
    "    \n",
    "    # Placeholder performance metrics\n",
    "    # In reality, you would calculate these from actual model-obs comparison\n",
    "    performance_data.append({\n",
    "        'site': site['site_name'],\n",
    "        'climate': site_info['KG'],\n",
    "        'landcover': site_info['Dominant_LC'],\n",
    "        'r2': np.random.uniform(0.6, 0.9),  # Placeholder\n",
    "        'rmse': np.random.uniform(20, 60)    # Placeholder\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "\n",
    "# Plot performance by climate type\n",
    "if len(performance_df) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # R² by climate\n",
    "    climate_means = performance_df.groupby('climate')['r2'].mean()\n",
    "    climate_means.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('Model Performance (R²) by Climate Type', fontsize=14)\n",
    "    ax1.set_ylabel('R²', fontsize=12)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # RMSE by land cover\n",
    "    lc_means = performance_df.groupby('landcover')['rmse'].mean()\n",
    "    lc_means.plot(kind='bar', ax=ax2, color='lightcoral', edgecolor='black')\n",
    "    ax2.set_title('Model Error (RMSE) by Land Cover', fontsize=14)\n",
    "    ax2.set_ylabel('RMSE (W/m²)', fontsize=12)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary Report"
   ]
  }
]}