{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 02b â€” Basin-Scale Workflow (Bow River at Banff, Semi-Distributed)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial advances from lumped to semi-distributed watershed modeling. Instead of representing the basin as a single unit, we now subdivide it into multiple connected sub-basins (GRUs) that capture spatial variability while maintaining computational efficiency.\n",
    "\n",
    "Building on Tutorial 02a's lumped approach, semi-distributed modeling adds spatial detail through automated watershed delineation that creates multiple sub-basins, stream network topology that connects GRUs through routing, and spatially-distributed characteristics that better represent elevation gradients and heterogeneous processes.\n",
    "\n",
    "The key configuration change is `DOMAIN_DEFINITION_METHOD: 'delineate'` with a `STREAM_THRESHOLD` parameter controlling the number of sub-basins. Smaller thresholds create more GRUs (finer spatial detail) but increase computational cost.\n",
    "\n",
    "We continue with the **Bow River at Banff** watershed, now discretized into multiple GRUs connected by mizuRoute for explicit stream network routing. This approach improves representation of snowmelt timing, spatial climate variability, and runoff generation patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 â€” Configuration and data reuse\n",
    "\n",
    "We generate a semi-distributed configuration and intelligently reuse data from Tutorial 02a where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 â€” Semi-distributed configuration with data reuse\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "from symfluence import SYMFLUENCE\n",
    "\n",
    "# Define directories\n",
    "SYMFLUENCE_CODE_DIR = Path(\"../..\").resolve()\n",
    "SYMFLUENCE_DATA_DIR = Path(\"/Users/darrieythorsson/compHydro/test/SYMFLUENCE_data/\").resolve()\n",
    "\n",
    "# Load template\n",
    "config_template = SYMFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "with open(config_template, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify for semi-distributed basin ===\n",
    "config['SYMFLUENCE_CODE_DIR'] = str(SYMFLUENCE_CODE_DIR)\n",
    "config['SYMFLUENCE_DATA_DIR'] = str(SYMFLUENCE_DATA_DIR)\n",
    "config['DOMAIN_NAME'] = 'Bow_at_Banff_semi_distributed'\n",
    "config['EXPERIMENT_ID'] = 'run_1'\n",
    "config['POUR_POINT_COORDS'] = '51.1722/-115.5717'\n",
    "\n",
    "# Key changes for semi-distributed\n",
    "config['DELINEATION_METHOD'] = 'stream_threshold'  # Watershed subdivision\n",
    "config['DOMAIN_DEFINITION_METHOD'] = 'delineate'  # Watershed\n",
    "config['STREAM_THRESHOLD'] = 5000  # Controls number of sub-basins\n",
    "config['DOMAIN_DISCRETIZATION'] = 'GRUs'\n",
    "\n",
    "config['HYDROLOGICAL_MODEL'] = 'SUMMA'\n",
    "config['ROUTING_MODEL'] = 'mizuRoute'\n",
    "config['MIZU_FROM_MODEL'] = 'SUMMA'\n",
    "\n",
    "\n",
    "config['SETTINGS_MIZU_ROUTING_VAR'] = 'averageRoutedRunoff'\n",
    "config['SETTINGS_MIZU_ROUTING_UNITS'] = 'm/s'\n",
    "config['SETTINGS_MIZU_ROUTING_DT'] = '3600'\n",
    "\n",
    "# Temporal extent\n",
    "config['EXPERIMENT_TIME_START'] = '2004-01-01 01:00'\n",
    "config['EXPERIMENT_TIME_END'] = '2007-12-31 23:00'\n",
    "config['CALIBRATION_PERIOD'] = '2005-10-01, 2006-09-30'\n",
    "config['EVALUATION_PERIOD'] = '2006-10-01, 2007-12-30'\n",
    "config['SPINUP_PERIOD'] = '2004-01-01, 2005-09-30'\n",
    "\n",
    "config['STATION_ID'] = '05BB001'\n",
    "config['DOWNLOAD_WSC_DATA'] = True\n",
    "\n",
    "config['PARAMS_TO_CALIBRATE'] = 'minStomatalResistance,cond2photo_slope,vcmax25_canopyTop,jmax25_scale,summerLAI,rootingDepth,soilStressParam,z0Canopy,windReductionParam'\n",
    "config['OPTIMISATION_TARGET'] = 'streamflow'\n",
    "config['ITERATIVE_OPTIMIZATION_ALGORITHM'] = 'DDS'\n",
    "config['OPTIMIZATION_METRIC'] = 'KGE'\n",
    "config['CALIBRATION_TIMESTEP'] = 'hourly'  \n",
    "\n",
    "# Save configuration\n",
    "config_path = SYMFLUENCE_CODE_DIR / '0_config_files' / 'config_semi_distributed.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ… Configuration saved: {config_path}\")\n",
    "\n",
    "# === Data reuse from Tutorial 02a ===\n",
    "lumped_domain = 'Bow_at_Banff_lumped'\n",
    "lumped_data_dir = SYMFLUENCE_DATA_DIR / f'domain_{lumped_domain}'\n",
    "\n",
    "def copy_with_name_adaptation(src, dst, old_name, new_name):\n",
    "    \"\"\"Copy directory and adapt filenames\"\"\"\n",
    "    if not src.exists():\n",
    "        return False\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if src.is_file():\n",
    "        shutil.copy2(src, dst)\n",
    "        return True\n",
    "    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    # Rename files containing old domain name\n",
    "    for file in dst.rglob('*'):\n",
    "        if file.is_file() and old_name in file.name:\n",
    "            new_file = file.parent / file.name.replace(old_name, new_name)\n",
    "            file.rename(new_file)\n",
    "    return True\n",
    "\n",
    "# Initialize SYMFLUENCE first\n",
    "symfluence = SYMFLUENCE(config_path)\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "\n",
    "if lumped_data_dir.exists():\n",
    "    print(f\"\\nðŸ“‹ Reusing data from Tutorial 02a: {lumped_data_dir}\")\n",
    "    \n",
    "    reusable_data = {\n",
    "        'Elevation': lumped_data_dir / 'attributes' / 'elevation',\n",
    "        'Land Cover': lumped_data_dir / 'attributes' / 'landclass',\n",
    "        'Soils': lumped_data_dir / 'attributes' / 'soilclass',\n",
    "        'Forcing': lumped_data_dir / 'forcing' / 'raw_data',\n",
    "        'Streamflow': lumped_data_dir / 'observations' / 'streamflow'\n",
    "    }\n",
    "    \n",
    "    for data_type, src_path in reusable_data.items():\n",
    "        if src_path.exists():\n",
    "            rel_path = src_path.relative_to(lumped_data_dir)\n",
    "            dst_path = project_dir / rel_path\n",
    "            success = copy_with_name_adaptation(src_path, dst_path, lumped_domain, config['DOMAIN_NAME'])\n",
    "            if success:\n",
    "                print(f\"   âœ… {data_type}: Copied\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“‹ {data_type}: Not found\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No data from Tutorial 02a found. Will acquire fresh data.\")\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "print(f\"\\nâœ… Project structure created at: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” Domain definition (multi-GRU)\n",
    "\n",
    "Delineate the watershed into multiple sub-basins using stream network analysis and create connected GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a â€” Attribute check\n",
    "\n",
    "Verify DEM availability from data reuse, or acquire fresh if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a â€” DEM availability check\n",
    "dem_path = project_dir / 'attributes' / 'elevation' / 'dem'\n",
    "if not dem_path.exists() or len(list(dem_path.glob('*.tif'))) == 0:\n",
    "    print(\"   DEM not found, acquiring geospatial attributes...\")\n",
    "    # If using MAF supported HPC, uncomment the line below\n",
    "    # symfluence.managers['data'].acquire_attributes()\n",
    "    print(\"âœ… Geospatial attributes acquired\")\n",
    "else:\n",
    "    print(\"âœ… DEM available from previous workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b â€” Stream network delineation\n",
    "\n",
    "Automated watershed subdivision based on stream threshold parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b â€” Stream network delineation\n",
    "watershed_path = symfluence.managers['domain'].define_domain()\n",
    "print(\"âœ… Stream network delineation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c â€” GRU discretization\n",
    "\n",
    "Convert sub-basins to GRUs with routing connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c â€” GRU discretization\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(\"âœ… GRU discretization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2d â€” Network visualization\n",
    "\n",
    "Visualize the semi-distributed structure: sub-basins and stream network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2d â€” Network structure visualization\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load spatial products\n",
    "basin_dir = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_dir = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "basin_files = list(basin_dir.glob('*.shp'))\n",
    "network_files = list(network_dir.glob('*.shp'))\n",
    "\n",
    "if basin_files:\n",
    "    basins_gdf = gpd.read_file(basin_files[0])\n",
    "    print(f\"Number of GRUs: {len(basins_gdf)}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    basins_gdf.boundary.plot(ax=ax, color='blue', linewidth=1)\n",
    "    basins_gdf.plot(ax=ax, column='GRU_ID', cmap='tab20', alpha=0.5, legend=False)\n",
    "    \n",
    "    if network_files:\n",
    "        network_gdf = gpd.read_file(network_files[0])\n",
    "        network_gdf.plot(ax=ax, color='darkblue', linewidth=2, label='Stream Network')\n",
    "    \n",
    "    pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "    pour_point_gdf.plot(ax=ax, color='red', markersize=150, marker='*', label='Pour Point')\n",
    "    \n",
    "    ax.set_title(f'Semi-Distributed Structure\\n{len(basins_gdf)} Connected GRUs', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Basin shapefiles not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 â€” Data preprocessing\n",
    "\n",
    "Process forcing and observation data for multiple GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a â€” Streamflow observations\n",
    "# If using MAF supported HPC, uncomment the line below\n",
    "# symfluence.managers['data'].process_observed_data()\n",
    "print(\"âœ… Streamflow data processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b â€” Forcing data\n",
    "# If using MAF supported HPC, uncomment the line below  \n",
    "# symfluence.managers['data'].acquire_forcings()\n",
    "print(\"âœ… Forcing acquisition complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3c â€” Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"âœ… Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€” Model execution\n",
    "\n",
    "Configure and run SUMMA-mizuRoute with multiple connected GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4a â€” Model configuration\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"âœ… Semi-distributed model configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b â€” Model execution\n",
    "print(f\"Running {config['HYDROLOGICAL_MODEL']} with {config['ROUTING_MODEL']} ({len(basins_gdf)} GRUs)...\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"âœ… Semi-distributed simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 â€” Evaluation\n",
    "\n",
    "Compare semi-distributed results against observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 â€” Semi-distributed evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# --- 1) Observations (make daily & tz-naive) ---\n",
    "obs_path = project_dir / \"observations\" / \"streamflow\" / \"preprocessed\" / f\"{config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "obs_df = pd.read_csv(obs_path, parse_dates=[\"datetime\"]).set_index(\"datetime\").sort_index()\n",
    "# If tz-aware, make tz-naive for matching\n",
    "if obs_df.index.tz is not None:\n",
    "    obs_df.index = obs_df.index.tz_convert(None)\n",
    "# Put obs on daily mean (adjust if your obs are daily totals/instants)\n",
    "obs_daily = obs_df[\"discharge_cms\"].resample(\"D\").mean().rename(\"discharge_obs\")\n",
    "\n",
    "# --- 2) Routed simulation: select the gauge segment & make daily ---\n",
    "routing_dir = project_dir / \"simulations\" / config[\"EXPERIMENT_ID\"] / \"mizuRoute\"\n",
    "sim_files = list(routing_dir.glob(\"*.nc\"))\n",
    "if not sim_files:\n",
    "    raise FileNotFoundError(f\"No routed streamflow in: {routing_dir}\")\n",
    "\n",
    "ds = xr.open_dataset(sim_files[0])\n",
    "\n",
    "# --- Auto-pick the segment with the highest flow ---\n",
    "var = ds[\"IRFroutedRunoff\"]\n",
    "\n",
    "# Identify time and segment dims robustly\n",
    "time_dim = next((d for d in var.dims if \"time\" in d.lower()), None)\n",
    "if time_dim is None:\n",
    "    raise RuntimeError(\"Could not find a time dimension in IRFroutedRunoff.\")\n",
    "\n",
    "seg_dims = [d for d in var.dims if d != time_dim]\n",
    "if len(seg_dims) == 0:\n",
    "    # 1D over time (no segment dimension) -> just use it\n",
    "    sim_series = var\n",
    "else:\n",
    "    # Assume single segment dimension; if multiple, just take the first\n",
    "    seg_dim = seg_dims[0]\n",
    "\n",
    "    # Compute mean over time per segment (skip NaNs), pick the largest\n",
    "    seg_mean = var.mean(time_dim, skipna=True)\n",
    "    # argmax returns an index along seg_dim\n",
    "    max_idx = seg_mean.argmax(dim=seg_dim)\n",
    "\n",
    "    # Get the coordinate value (segment id) at that index\n",
    "    seg_id = seg_mean[seg_dim].isel({seg_dim: max_idx}).item()\n",
    "\n",
    "    # Select that segmentâ€™s time series\n",
    "    sim_series = var.sel({seg_dim: seg_id})\n",
    "\n",
    "    # Optional: quick sanity log\n",
    "    print(f\"Selected segment dim: {seg_dim}, id: {seg_id}\")\n",
    "    try:\n",
    "        print(\"Segment mean flow (m^3/s):\", float(seg_mean.sel({seg_dim: seg_id}).values))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Convert to pandas\n",
    "sim_df = sim_series.to_series().sort_index()\n",
    "# Make tz-naive if needed\n",
    "if getattr(sim_df.index, \"tz\", None) is not None:\n",
    "    sim_df.index = sim_df.index.tz_convert(None)\n",
    "\n",
    "# Put sim on daily mean to match obs cadence (change to .sum() if routed values are volumes)\n",
    "sim_daily = sim_df.resample(\"D\").mean().rename(\"discharge_sim\")\n",
    "\n",
    "# --- 3) Align & join on dates we actually have in obs ---\n",
    "\n",
    "\n",
    "spinup_end = (sim_daily.index.min() + pd.DateOffset(years=1)).normalize()\n",
    "\n",
    "\n",
    "\n",
    "# Trim\n",
    "obs_daily_trim = obs_daily.loc[obs_daily.index >= spinup_end]\n",
    "sim_daily_trim = sim_daily.loc[sim_daily.index >= spinup_end]\n",
    "\n",
    "# Rebuild evaluation frame\n",
    "eval_df = pd.concat([obs_daily_trim.rename(\"discharge_obs\"),\n",
    "                     sim_daily_trim.rename(\"discharge_sim\")], axis=1) \\\n",
    "             .dropna(subset=[\"discharge_obs\", \"discharge_sim\"]) \\\n",
    "             .sort_index()\n",
    "\n",
    "# Short-circuit if empty\n",
    "if eval_df.empty:\n",
    "    raise RuntimeError(\"After spinup trimming there is no overlap. Check your spinup_end rule/time ranges.\")\n",
    "\n",
    "obs_valid = eval_df[\"discharge_obs\"]\n",
    "sim_valid = eval_df[\"discharge_sim\"]\n",
    "\n",
    "# --- 4) Robust metrics ---\n",
    "def _safe_div(a, b):\n",
    "    return np.nan if b == 0 or np.isnan(b) else a / b\n",
    "\n",
    "def nse(obs, sim):\n",
    "    num = np.nansum((obs - sim) ** 2)\n",
    "    den = np.nansum((obs - np.nanmean(obs)) ** 2)\n",
    "    return float(1 - _safe_div(num, den))\n",
    "\n",
    "def kge(obs, sim):\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = np.nanstd(sim) / np.nanstd(obs)\n",
    "    beta = np.nanmean(sim) / np.nanmean(obs)\n",
    "    return float(1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2))\n",
    "\n",
    "def pbias(obs, sim):\n",
    "    num = np.nansum(sim - obs)\n",
    "    den = np.nansum(obs)\n",
    "    return float(100 * _safe_div(num, den))\n",
    "\n",
    "nse_val = np.round(nse(obs_valid, sim_valid), 3)\n",
    "kge_val = np.round(kge(obs_valid, sim_valid), 3)\n",
    "pbias_val = np.round(pbias(obs_valid, sim_valid), 1)\n",
    "\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  NSE: {nse_val}\")\n",
    "print(f\"  KGE: {kge_val}\")\n",
    "print(f\"  PBIAS: {pbias_val}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series\n",
    "axes[0, 0].plot(obs_valid.index, obs_valid.values, 'b-', label='Observed', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].plot(sim_valid.index, sim_valid.values, 'r-', label=f'Semi-Distributed ({len(basins_gdf)} GRUs)', \n",
    "                linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Discharge (mÂ³/s)')\n",
    "axes[0, 0].set_title('Semi-Distributed Streamflow')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].text(0.02, 0.95, f\"NSE: {nse_val}\\nKGE: {kge_val}\\nBias: {pbias_val}%\\nGRUs: {len(basins_gdf)}\",\n",
    "                transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=9)\n",
    "\n",
    "# Scatter\n",
    "axes[0, 1].scatter(obs_valid, sim_valid, alpha=0.5, s=10, c='green')\n",
    "max_val = max(obs_valid.max(), sim_valid.max())\n",
    "axes[0, 1].plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Observed (mÂ³/s)')\n",
    "axes[0, 1].set_ylabel('Simulated (mÂ³/s)')\n",
    "axes[0, 1].set_title('Observed vs Simulated')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly climatology\n",
    "monthly_obs = obs_valid.groupby(obs_valid.index.month).mean()\n",
    "monthly_sim = sim_valid.groupby(sim_valid.index.month).mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[1, 0].plot(monthly_obs.index, monthly_obs.values, 'b-o', label='Observed', markersize=6)\n",
    "axes[1, 0].plot(monthly_sim.index, monthly_sim.values, 'r-o', label='Simulated', markersize=6)\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "axes[1, 0].set_xticklabels(month_names)\n",
    "axes[1, 0].set_ylabel('Mean Discharge (mÂ³/s)')\n",
    "axes[1, 0].set_title('Seasonal Flow Regime')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Flow duration curve\n",
    "obs_sorted = obs_valid.sort_values(ascending=False)\n",
    "sim_sorted = sim_valid.sort_values(ascending=False)\n",
    "obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted) * 100\n",
    "sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted) * 100\n",
    "axes[1, 1].semilogy(obs_ranks, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "axes[1, 1].semilogy(sim_ranks, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Exceedance Probability (%)')\n",
    "axes[1, 1].set_ylabel('Discharge (mÂ³/s)')\n",
    "axes[1, 1].set_title('Flow Duration Curve')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Semi-Distributed Evaluation â€” {config[\"DOMAIN_NAME\"]} ({len(basins_gdf)} GRUs)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Semi-distributed evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5b â€” Run calibration \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = symfluence.managers['optimization'].calibrate_model()  \n",
    "print(\"Calibration results file:\", results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CONFLUENCE root venv)",
   "language": "python",
   "name": "confluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
