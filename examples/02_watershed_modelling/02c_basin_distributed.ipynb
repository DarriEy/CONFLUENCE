{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYMFLUENCE Tutorial 02c â€” Basin-Scale Workflow (Bow River at Banff, Elevation-Based Distributed)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates the most spatially-detailed modeling approach: elevation-based HRU discretization. Building on Tutorials 02a (lumped) and 02b (semi-distributed), we now subdivide each GRU into elevation bands that capture altitudinal controls on mountain hydrology.\n",
    "\n",
    "Elevation-based discretization is critical in mountain watersheds where temperature and precipitation vary systematically with altitude. By stratifying each sub-basin into elevation bands, we better represent snowpack dynamics, seasonal timing differences, and orographic effects.\n",
    "\n",
    "The key configuration parameter is `ELEVATION_BAND_SIZE`, which controls the vertical resolution (e.g., 100m bands). Smaller bands increase spatial detail but add computational cost. This approach maintains the validated GRU structure from Tutorial 02b while adding vertical stratification.\n",
    "\n",
    "For the **Bow River at Banff** (elevation range: 1,384â€“3,400 m), elevation bands capture the transition from low-elevation rain-dominated zones to high-elevation snow-dominated headwaters, improving simulation of spring freshet timing and runoff generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 â€” Configuration and data reuse\n",
    "\n",
    "We configure elevation-based discretization and reuse data from Tutorial 02b where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 â€” Elevation-based configuration with data reuse\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "from SYMFLUENCE import SYMFLUENCE\n",
    "\n",
    "# Define directories\n",
    "SYMFLUENCE_CODE_DIR = Path(\"../..\").resolve()\n",
    "SYMFLUENCE_DATA_DIR = Path(\"/Users/darrieythorsson/compHydro/test/SYMFLUENCE_data\").resolve()\n",
    "\n",
    "# Load template\n",
    "config_template = SYMFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "with open(config_template, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify for elevation-based distributed ===\n",
    "config['SYMFLUENCE_CODE_DIR'] = str(SYMFLUENCE_CODE_DIR)\n",
    "config['DOMAIN_NAME'] = 'Bow_at_Banff_elevation'\n",
    "config['EXPERIMENT_ID'] = 'run_1'\n",
    "config['POUR_POINT_COORDS'] = '51.1722/-115.5717'\n",
    "\n",
    "# Elevation-based discretization\n",
    "config['DOMAIN_DEFINITION_METHOD'] = 'delineate'\n",
    "config['STREAM_THRESHOLD'] = 5000  # Same as 02b\n",
    "config['DOMAIN_DISCRETIZATION'] = 'elevation'  # Key change\n",
    "config['ELEVATION_BAND_SIZE'] = 400  # 400m elevation bands\n",
    "\n",
    "\n",
    "config['HYDROLOGICAL_MODEL'] = 'SUMMA'\n",
    "config['ROUTING_MODEL'] = 'mizuRoute'\n",
    "config['MIZU_FROM_MODEL'] = 'SUMMA'\n",
    "\n",
    "\n",
    "config['SETTINGS_MIZU_ROUTING_VAR'] = 'averageRoutedRunoff'\n",
    "config['SETTINGS_MIZU_ROUTING_UNITS'] = 'm/s'\n",
    "config['SETTINGS_MIZU_ROUTING_DT'] = '3600'\n",
    "\n",
    "# Temporal extent\n",
    "config['EXPERIMENT_TIME_START'] = '2004-01-01 01:00'\n",
    "config['EXPERIMENT_TIME_END'] = '2007-12-31 23:00'\n",
    "config['CALIBRATION_PERIOD'] = '2005-10-01, 2006-09-30'\n",
    "config['EVALUATION_PERIOD'] = '2006-10-01, 2007-12-30'\n",
    "config['SPINUP_PERIOD'] = '2004-01-01, 2005-09-30'\n",
    "\n",
    "config['STATION_ID'] = '05BB001'\n",
    "config['DOWNLOAD_WSC_DATA'] = True\n",
    "\n",
    "config['PARAMS_TO_CALIBRATE'] = 'minStomatalResistance,cond2photo_slope,vcmax25_canopyTop,jmax25_scale,summerLAI,rootingDepth,soilStressParam,z0Canopy,windReductionParam'\n",
    "config['OPTIMISATION_TARGET'] = 'streamflow'\n",
    "config['ITERATIVE_OPTIMIZATION_ALGORITHM'] = 'DDS'\n",
    "config['OPTIMIZATION_METRIC'] = 'KGE'\n",
    "config['CALIBRATION_TIMESTEP'] = 'hourly'  \n",
    "\n",
    "\n",
    "# Save configuration\n",
    "config_path = SYMFLUENCE_CODE_DIR / '0_config_files' / 'config_elevation_distributed.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ… Configuration saved: {config_path}\")\n",
    "\n",
    "# === Data reuse from Tutorial 02b ===\n",
    "semi_dist_domain = 'Bow_at_Banff_semi_distributed'\n",
    "semi_dist_data_dir = SYMFLUENCE_DATA_DIR / f'domain_{semi_dist_domain}'\n",
    "\n",
    "def copy_with_name_adaptation(src, dst, old_name, new_name):\n",
    "    \"\"\"Copy directory and adapt filenames\"\"\"\n",
    "    if not src.exists():\n",
    "        return False\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if src.is_file():\n",
    "        shutil.copy2(src, dst)\n",
    "        return True\n",
    "    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    for file in dst.rglob('*'):\n",
    "        if file.is_file() and old_name in file.name:\n",
    "            new_file = file.parent / file.name.replace(old_name, new_name)\n",
    "            file.rename(new_file)\n",
    "    return True\n",
    "\n",
    "# Initialize SYMFLUENCE\n",
    "symfluence = SYMFLUENCE(config_path)\n",
    "project_dir = symfluence.managers['project'].setup_project()\n",
    "\n",
    "if semi_dist_data_dir.exists():\n",
    "    print(f\"\\nðŸ“‹ Reusing data from Tutorial 02b: {semi_dist_data_dir}\")\n",
    "    \n",
    "    reusable_data = {\n",
    "        'Elevation': semi_dist_data_dir / 'attributes' / 'elevation',\n",
    "        'Land Cover': semi_dist_data_dir / 'attributes' / 'landclass',\n",
    "        'Soils': semi_dist_data_dir / 'attributes' / 'soilclass',\n",
    "        'Forcing': semi_dist_data_dir / 'forcing' / 'raw_data',\n",
    "        'Stream Network': semi_dist_data_dir / 'shapefiles' / 'river_network',\n",
    "        'GRUs': semi_dist_data_dir / 'shapefiles' / 'river_basins',\n",
    "        'Streamflow': semi_dist_data_dir / 'observations' / 'streamflow'\n",
    "    }\n",
    "    \n",
    "    for data_type, src_path in reusable_data.items():\n",
    "        if src_path.exists():\n",
    "            rel_path = src_path.relative_to(semi_dist_data_dir)\n",
    "            dst_path = project_dir / rel_path\n",
    "            success = copy_with_name_adaptation(src_path, dst_path, semi_dist_domain, config['DOMAIN_NAME'])\n",
    "            if success:\n",
    "                print(f\"   âœ… {data_type}: Copied\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“‹ {data_type}: Not found\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No data from Tutorial 02b found. Will acquire fresh data.\")\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = symfluence.managers['project'].create_pour_point()\n",
    "print(f\"\\nâœ… Project structure created at: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” Elevation-based discretization\n",
    "\n",
    "Subdivide each GRU from Tutorial 02b into elevation bands for vertical stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a â€” Attribute check\n",
    "\n",
    "Verify DEM and GRU availability from data reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a â€” DEM and GRU availability check\n",
    "dem_path = project_dir / 'attributes' / 'elevation' / 'dem'\n",
    "gru_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "\n",
    "if not dem_path.exists() or not gru_path.exists():\n",
    "    print(\"   Required data not found, acquiring...\")\n",
    "    # If using MAF supported HPC, uncomment the lines below\n",
    "    # symfluence.managers['data'].acquire_attributes()\n",
    "    # symfluence.managers['domain'].define_domain()\n",
    "    print(\"âœ… Geospatial data acquired\")\n",
    "else:\n",
    "    print(\"âœ… DEM and GRU data available from previous workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b â€” Elevation band creation\n",
    "\n",
    "Create HRUs by intersecting GRUs with elevation bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b â€” Elevation-based HRU discretization\n",
    "hru_path = symfluence.managers['domain'].discretize_domain()\n",
    "print(\"âœ… Elevation-based HRU discretization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c â€” Elevation structure visualization\n",
    "\n",
    "Visualize the elevation-stratified HRU structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c â€” Elevation band visualization\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load HRUs with elevation information\n",
    "hru_file = project_dir / 'shapefiles' / 'catchment' / f\"{config['DOMAIN_NAME']}_HRUs_elevation.shp\"\n",
    "\n",
    "if hru_file.exists():\n",
    "    hru_gdf = gpd.read_file(str(hru_file))\n",
    "    print(f\"Number of elevation-based HRUs: {len(hru_gdf)}\")\n",
    "    \n",
    "    # Calculate elevation statistics\n",
    "    if 'elev_mean' in hru_gdf.columns:\n",
    "        elev_col = 'elev_mean'\n",
    "    elif 'elevation' in hru_gdf.columns:\n",
    "        elev_col = 'elevation'\n",
    "    else:\n",
    "        elev_col = hru_gdf.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"Elevation range: {hru_gdf[elev_col].min():.0f} - {hru_gdf[elev_col].max():.0f} m\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Spatial map colored by elevation\n",
    "    hru_gdf.plot(column=elev_col, cmap='terrain', ax=axes[0], \n",
    "                 legend=True, legend_kwds={'label': 'Elevation (m)'})\n",
    "    \n",
    "    pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "    pour_point_gdf.plot(ax=axes[0], color='red', markersize=150, marker='*', label='Pour Point')\n",
    "    \n",
    "    axes[0].set_title(f'Elevation-Based HRU Distribution\\n{len(hru_gdf)} HRUs', fontweight='bold')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Elevation distribution histogram\n",
    "    axes[1].hist(hru_gdf[elev_col], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Elevation (m)')\n",
    "    axes[1].set_ylabel('Number of HRUs')\n",
    "    axes[1].set_title('HRU Elevation Distribution', fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  HRU shapefile not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 â€” Data preprocessing\n",
    "\n",
    "Process forcing and observation data for elevation-stratified HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a â€” Streamflow observations\n",
    "# If using MAF supported HPC, uncomment the line below\n",
    "# symfluence.managers['data'].process_observed_data()\n",
    "print(\"âœ… Streamflow data processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b â€” Forcing data\n",
    "# If using MAF supported HPC, uncomment the line below\n",
    "# symfluence.managers['data'].acquire_forcings()\n",
    "print(\"âœ… Forcing acquisition complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3c â€” Model-agnostic preprocessing\n",
    "symfluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"âœ… Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€” Model execution\n",
    "\n",
    "Configure and run SUMMA-mizuRoute with elevation-stratified HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a â€” Model configuration\n",
    "symfluence.managers['model'].preprocess_models()\n",
    "print(\"âœ… Elevation-based model configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b â€” Model execution\n",
    "print(f\"Running {config['HYDROLOGICAL_MODEL']} with {config['ROUTING_MODEL']} ({len(hru_gdf)} elevation-based HRUs)...\")\n",
    "symfluence.managers['model'].run_models()\n",
    "print(\"âœ… Elevation-based distributed simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 â€” Evaluation\n",
    "\n",
    "Compare elevation-based results against observations and previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 â€” Elevation-based HRUs evaluation \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Observations (daily, tz-naive)\n",
    "# ---------------------------\n",
    "obs_path = project_dir / \"observations\" / \"streamflow\" / \"preprocessed\" / f\"{config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "obs_df = pd.read_csv(obs_path, parse_dates=[\"datetime\"]).set_index(\"datetime\").sort_index()\n",
    "\n",
    "# Make tz-naive if needed\n",
    "if obs_df.index.tz is not None:\n",
    "    obs_df.index = obs_df.index.tz_convert(None)\n",
    "\n",
    "# Put obs on daily mean (change to .sum() if your obs file is daily volumes)\n",
    "obs_daily = obs_df[\"discharge_cms\"].resample(\"D\").mean().rename(\"discharge_obs\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Simulation (select segment, daily, tz-naive)\n",
    "# ---------------------------\n",
    "routing_dir = project_dir / \"simulations\" / config[\"EXPERIMENT_ID\"] / \"mizuRoute\"\n",
    "sim_files = list(routing_dir.glob(\"*.nc\"))  # allow any routed filename\n",
    "if not sim_files:\n",
    "    raise FileNotFoundError(f\"No routed streamflow in: {routing_dir}\")\n",
    "\n",
    "ds = xr.open_dataset(sim_files[0], decode_times=False)\n",
    "\n",
    "# Fix non-CF units on the time coordinate if needed\n",
    "time_name = next((d for d in ds.dims if 'time' in d.lower()), 'time')\n",
    "tvar = ds[time_name]\n",
    "units = (tvar.attrs or {}).get('units', '')\n",
    "cal = (tvar.attrs or {}).get('calendar', 'standard')\n",
    "\n",
    "def _parse_ref_date(u):\n",
    "    # Accepts \"s since 1990-1-1 0:0:0\" or \"seconds since 1990-01-01 00:00:00\"\n",
    "    if 'since' not in u:\n",
    "        return None, None\n",
    "    unit_part, ref_part = u.split('since', 1)\n",
    "    unit_part = unit_part.strip().lower()\n",
    "    ref_part = ref_part.strip()\n",
    "    # Normalize short forms\n",
    "    unit_norm = {'s': 'seconds', 'sec': 'seconds', 'secs': 'seconds'}.get(unit_part, unit_part)\n",
    "    return unit_norm, ref_part\n",
    "\n",
    "unit_norm, ref = _parse_ref_date(units)\n",
    "\n",
    "if unit_norm == 'seconds' and ref is not None:\n",
    "    # Build a pandas datetime index\n",
    "    ref_ts = pd.to_datetime(ref)\n",
    "    # ensure numeric array (int/float)\n",
    "    tvals = xr.apply_ufunc(lambda x: x.astype('float64'), tvar, dask='allowed')\n",
    "    new_time = ref_ts + pd.to_timedelta(tvals, unit='s')\n",
    "    # assign fixed, CF-friendly coordinate\n",
    "    ds = ds.assign_coords({time_name: ('time', pd.DatetimeIndex(new_time.values))})\n",
    "    # Optional: clean attrs to a CF-compliant string\n",
    "    ds[time_name].attrs['units'] = f\"seconds since {ref_ts.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    ds[time_name].attrs['calendar'] = cal\n",
    "else:\n",
    "    # If units are already CF-compliant or different, you can try decoding now\n",
    "    try:\n",
    "        ds = xr.decode_cf(ds)\n",
    "    except Exception:\n",
    "        pass  # fall back to naive numeric time if needed\n",
    "\n",
    "var = ds[\"IRFroutedRunoff\"]\n",
    "\n",
    "# find time dimension\n",
    "time_dim = next((d for d in var.dims if \"time\" in d.lower()), None)\n",
    "if time_dim is None:\n",
    "    raise RuntimeError(\"Could not find a time dimension in IRFroutedRunoff.\")\n",
    "\n",
    "# choose the (first) non-time dimension as the segment dim\n",
    "seg_dims = [d for d in var.dims if d != time_dim]\n",
    "if not seg_dims:\n",
    "    sim_series = var  # 1D over time\n",
    "else:\n",
    "    seg_dim = seg_dims[0]\n",
    "\n",
    "    # 1) count finite points per segment\n",
    "    good_counts = xr.apply_ufunc(np.isfinite, var).sum(dim=time_dim)\n",
    "\n",
    "    # 2) keep only segments with at least one finite value\n",
    "    valid_mask = good_counts > 0\n",
    "    if not bool(valid_mask.any()):\n",
    "        raise RuntimeError(\n",
    "            \"All segments are NaN over time in IRFroutedRunoff. \"\n",
    "            \"Check that you opened the mizuRoute file (not SUMMA), the variable name, \"\n",
    "            \"and that the run produced non-empty routed flows.\"\n",
    "        )\n",
    "\n",
    "    var_valid = var.where(valid_mask, drop=True)\n",
    "\n",
    "    # 3) select segment with highest mean (over valid data)\n",
    "    seg_mean = var_valid.mean(time_dim, skipna=True)\n",
    "    max_idx = seg_mean.argmax(dim=seg_dim)\n",
    "    seg_id = seg_mean[seg_dim].isel({seg_dim: max_idx}).item()\n",
    "\n",
    "    # 4) extract the time series for that segment (use original var to keep coordinates)\n",
    "    sim_series = var.sel({seg_dim: seg_id})\n",
    "    print(f\"[02c] Selected segment dim: {seg_dim}, id: {seg_id}\")\n",
    "    \n",
    "# Convert to pandas\n",
    "sim_df = sim_series.to_series().sort_index()\n",
    "if getattr(sim_df.index, \"tz\", None) is not None:\n",
    "    sim_df.index = sim_df.index.tz_convert(None)\n",
    "\n",
    "# Daily cadence (use .sum() if routed values are volumes per timestep)\n",
    "sim_daily = sim_df.resample(\"D\").mean().rename(\"discharge_sim\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Spinup: skip first year from simulation start\n",
    "# ---------------------------\n",
    "spinup_end = (sim_daily.index.min() + pd.DateOffset(years=1)).normalize()\n",
    "print(\"Spinup ends at:\", spinup_end)\n",
    "\n",
    "obs_daily_trim = obs_daily.loc[obs_daily.index >= spinup_end]\n",
    "sim_daily_trim = sim_daily.loc[sim_daily.index >= spinup_end]\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Align & build evaluation frame\n",
    "# ---------------------------\n",
    "eval_df = pd.concat([obs_daily_trim, sim_daily_trim], axis=1) \\\n",
    "             .dropna(subset=[\"discharge_obs\", \"discharge_sim\"]) \\\n",
    "             .sort_index()\n",
    "\n",
    "if eval_df.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No overlap after spinup/alignment. Check segment selection, units (mean vs sum), \"\n",
    "        \"timezone, and cadences.\"\n",
    "    )\n",
    "\n",
    "obs_valid = eval_df[\"discharge_obs\"]\n",
    "sim_valid = eval_df[\"discharge_sim\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 5) NaN-safe metrics\n",
    "# ---------------------------\n",
    "def _safe_div(a, b):\n",
    "    return np.nan if (b == 0 or np.isnan(b)) else a / b\n",
    "\n",
    "def nse(obs, sim):\n",
    "    num = np.nansum((obs - sim) ** 2)\n",
    "    den = np.nansum((obs - np.nanmean(obs)) ** 2)\n",
    "    return float(1 - _safe_div(num, den))\n",
    "\n",
    "def kge(obs, sim):\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = np.nanstd(sim) / np.nanstd(obs)\n",
    "    beta = np.nanmean(sim) / np.nanmean(obs)\n",
    "    return float(1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2))\n",
    "\n",
    "def pbias(obs, sim):\n",
    "    num = np.nansum(sim - obs)\n",
    "    den = np.nansum(obs)\n",
    "    return float(100 * _safe_div(num, den))\n",
    "\n",
    "nse_val   = np.round(nse(obs_valid, sim_valid), 3)\n",
    "kge_val   = np.round(kge(obs_valid, sim_valid), 3)\n",
    "pbias_val = np.round(pbias(obs_valid, sim_valid), 1)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  NSE:  {nse_val}\")\n",
    "print(f\"  KGE:  {kge_val}\")\n",
    "print(f\"  PBIAS:{pbias_val}%\")\n",
    "print(f\"  HRUs: {len(hru_gdf)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Visualization (same layout)\n",
    "# ---------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series\n",
    "axes[0, 0].plot(eval_df.index, eval_df[\"discharge_obs\"].values, 'b-', label='Observed', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].plot(eval_df.index, eval_df[\"discharge_sim\"].values, 'r-', \n",
    "                label=f'Elevation-Based ({len(hru_gdf)} HRUs)', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Discharge (mÂ³/s)')\n",
    "axes[0, 0].set_title('Elevation-Based Streamflow')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].text(0.02, 0.95, f\"NSE: {nse_val}\\nKGE: {kge_val}\\nBias: {pbias_val}%\\nHRUs: {len(hru_gdf)}\",\n",
    "                transform=axes[0, 0].transAxes, va='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=9)\n",
    "\n",
    "# Scatter\n",
    "axes[0, 1].scatter(obs_valid, sim_valid, alpha=0.5, s=10)\n",
    "mmax = float(max(obs_valid.max(), sim_valid.max()))\n",
    "axes[0, 1].plot([0, mmax], [0, mmax], 'k--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Observed (mÂ³/s)')\n",
    "axes[0, 1].set_ylabel('Simulated (mÂ³/s)')\n",
    "axes[0, 1].set_title('Observed vs Simulated')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly climatology\n",
    "monthly_obs = obs_valid.groupby(obs_valid.index.month).mean()\n",
    "monthly_sim = sim_valid.groupby(sim_valid.index.month).mean()\n",
    "month_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "axes[1, 0].plot(monthly_obs.index, monthly_obs.values, 'b-o', label='Observed', markersize=6)\n",
    "axes[1, 0].plot(monthly_sim.index, monthly_sim.values, 'r-o', label='Simulated', markersize=6)\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "axes[1, 0].set_xticklabels(month_names)\n",
    "axes[1, 0].set_ylabel('Mean Discharge (mÂ³/s)')\n",
    "axes[1, 0].set_title('Seasonal Flow Regime')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tutorial progression bar: robust to missing earlier metrics\n",
    "labels, units, nses = [], [], []\n",
    "# If youâ€™ve saved/defined earlier metrics, append them here; otherwise we only plot 02c\n",
    "if \"nse_val_02a\" in globals():\n",
    "    labels.append(\"02a\\n(Lumped)\"); units.append(1); nses.append(float(nse_val_02a))\n",
    "if \"nse_val_02b\" in globals():\n",
    "    labels.append(\"02b\\n(Semi-Dist)\"); units.append(globals().get(\"gru_count_02b\", \"N/A\")); nses.append(float(nse_val_02b))\n",
    "\n",
    "labels.append(\"02c\\n(Elevation)\")\n",
    "units.append(len(hru_gdf))\n",
    "nses.append(float(nse_val))\n",
    "\n",
    "x_pos = np.arange(len(labels))\n",
    "bars = axes[1, 1].bar(x_pos, nses, alpha=0.7, edgecolor='k')\n",
    "axes[1, 1].set_xlabel('Tutorial Progression')\n",
    "axes[1, 1].set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "axes[1, 1].set_title('Performance vs Complexity')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(labels)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, units_i, nse_i in zip(bars, units, nses):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., float(nse_i) + 0.02,\n",
    "                    f'{nse_i:.3f}\\n({units_i} units)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle(f'Elevation-Based Evaluation â€” {config[\"DOMAIN_NAME\"]} ({len(hru_gdf)} HRUs)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Elevation-based evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5b â€” Run calibration \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = symfluence.managers['optimization'].calibrate_model()  \n",
    "print(\"Calibration results file:\", results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CONFLUENCE root venv)",
   "language": "python",
   "name": "confluence-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
