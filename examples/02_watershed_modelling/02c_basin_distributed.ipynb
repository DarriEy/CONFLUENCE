{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial 02c ‚Äî Basin-Scale Workflow (Bow River at Banff, Elevation-Based Distributed)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates the most spatially-detailed modeling approach: elevation-based HRU discretization. Building on Tutorials 02a (lumped) and 02b (semi-distributed), we now subdivide each GRU into elevation bands that capture altitudinal controls on mountain hydrology.\n",
    "\n",
    "Elevation-based discretization is critical in mountain watersheds where temperature and precipitation vary systematically with altitude. By stratifying each sub-basin into elevation bands, we better represent snowpack dynamics, seasonal timing differences, and orographic effects.\n",
    "\n",
    "The key configuration parameter is `ELEVATION_BAND_SIZE`, which controls the vertical resolution (e.g., 100m bands). Smaller bands increase spatial detail but add computational cost. This approach maintains the validated GRU structure from Tutorial 02b while adding vertical stratification.\n",
    "\n",
    "For the **Bow River at Banff** (elevation range: 1,384‚Äì3,400 m), elevation bands capture the transition from low-elevation rain-dominated zones to high-elevation snow-dominated headwaters, improving simulation of spring freshet timing and runoff generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 ‚Äî Configuration and data reuse\n",
    "\n",
    "We configure elevation-based discretization and reuse data from Tutorial 02b where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 ‚Äî Elevation-based configuration with data reuse\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Define directories\n",
    "CONFLUENCE_CODE_DIR = Path(\"../..\").resolve()\n",
    "CONFLUENCE_DATA_DIR = Path(\"/path/to/CONFLUENCE_data\").resolve()\n",
    "\n",
    "# Load template\n",
    "config_template = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "with open(config_template, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# === Modify for elevation-based distributed ===\n",
    "config['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "config['DOMAIN_NAME'] = 'Bow_at_Banff_elevation'\n",
    "config['EXPERIMENT_ID'] = 'run_1'\n",
    "config['POUR_POINT_COORDS'] = '51.1722/-115.5717'\n",
    "\n",
    "# Elevation-based discretization\n",
    "config['DOMAIN_DEFINITION_METHOD'] = 'delineate'\n",
    "config['STREAM_THRESHOLD'] = 5000  # Same as 02b\n",
    "config['DOMAIN_DISCRETIZATION'] = 'elevation'  # Key change\n",
    "config['ELEVATION_BAND_SIZE'] = 100  # 100m elevation bands\n",
    "\n",
    "config['HYDROLOGICAL_MODEL'] = 'SUMMA'\n",
    "config['ROUTING_MODEL'] = 'mizuRoute'\n",
    "\n",
    "config['EXPERIMENT_TIME_START'] = '2011-01-01 01:00'\n",
    "config['EXPERIMENT_TIME_END'] = '2018-12-31 23:00'\n",
    "config['CALIBRATION_PERIOD'] = '2011-01-01, 2015-12-31'\n",
    "config['EVALUATION_PERIOD'] = '2016-01-01, 2018-12-31'\n",
    "config['SPINUP_PERIOD'] = '2011-01-01, 2011-12-31'\n",
    "config['STATION_ID'] = '05BB001'\n",
    "config['DOWNLOAD_WSC_DATA'] = True\n",
    "\n",
    "# Save configuration\n",
    "config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_elevation_distributed.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved: {config_path}\")\n",
    "\n",
    "# === Data reuse from Tutorial 02b ===\n",
    "semi_dist_domain = 'Bow_at_Banff_semi_distributed'\n",
    "semi_dist_data_dir = CONFLUENCE_DATA_DIR / f'domain_{semi_dist_domain}'\n",
    "\n",
    "def copy_with_name_adaptation(src, dst, old_name, new_name):\n",
    "    \"\"\"Copy directory and adapt filenames\"\"\"\n",
    "    if not src.exists():\n",
    "        return False\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if src.is_file():\n",
    "        shutil.copy2(src, dst)\n",
    "        return True\n",
    "    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    for file in dst.rglob('*'):\n",
    "        if file.is_file() and old_name in file.name:\n",
    "            new_file = file.parent / file.name.replace(old_name, new_name)\n",
    "            file.rename(new_file)\n",
    "    return True\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(config_path)\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "if semi_dist_data_dir.exists():\n",
    "    print(f\"\\nüìã Reusing data from Tutorial 02b: {semi_dist_data_dir}\")\n",
    "    \n",
    "    reusable_data = {\n",
    "        'Elevation': semi_dist_data_dir / 'attributes' / 'elevation',\n",
    "        'Land Cover': semi_dist_data_dir / 'attributes' / 'land_cover',\n",
    "        'Soils': semi_dist_data_dir / 'attributes' / 'soils',\n",
    "        'Forcing': semi_dist_data_dir / 'forcing' / 'raw_data',\n",
    "        'Stream Network': semi_dist_data_dir / 'shapefiles' / 'river_network',\n",
    "        'GRUs': semi_dist_data_dir / 'shapefiles' / 'river_basins',\n",
    "        'Streamflow': semi_dist_data_dir / 'observations' / 'streamflow'\n",
    "    }\n",
    "    \n",
    "    for data_type, src_path in reusable_data.items():\n",
    "        if src_path.exists():\n",
    "            rel_path = src_path.relative_to(semi_dist_data_dir)\n",
    "            dst_path = project_dir / rel_path\n",
    "            success = copy_with_name_adaptation(src_path, dst_path, semi_dist_domain, config['DOMAIN_NAME'])\n",
    "            if success:\n",
    "                print(f\"   ‚úÖ {data_type}: Copied\")\n",
    "        else:\n",
    "            print(f\"   üìã {data_type}: Not found\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No data from Tutorial 02b found. Will acquire fresh data.\")\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "print(f\"\\n‚úÖ Project structure created at: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Elevation-based discretization\n",
    "\n",
    "Subdivide each GRU from Tutorial 02b into elevation bands for vertical stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a ‚Äî Attribute check\n",
    "\n",
    "Verify DEM and GRU availability from data reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a ‚Äî DEM and GRU availability check\n",
    "dem_path = project_dir / 'attributes' / 'elevation' / 'dem'\n",
    "gru_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "\n",
    "if not dem_path.exists() or not gru_path.exists():\n",
    "    print(\"   Required data not found, acquiring...\")\n",
    "    # If using MAF supported HPC, uncomment the lines below\n",
    "    # confluence.managers['data'].acquire_attributes()\n",
    "    # confluence.managers['domain'].define_domain()\n",
    "    print(\"‚úÖ Geospatial data acquired\")\n",
    "else:\n",
    "    print(\"‚úÖ DEM and GRU data available from previous workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b ‚Äî Elevation band creation\n",
    "\n",
    "Create HRUs by intersecting GRUs with elevation bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b ‚Äî Elevation-based HRU discretization\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "print(\"‚úÖ Elevation-based HRU discretization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c ‚Äî Elevation structure visualization\n",
    "\n",
    "Visualize the elevation-stratified HRU structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c ‚Äî Elevation band visualization\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load HRUs with elevation information\n",
    "hru_file = project_dir / 'shapefiles' / 'catchment' / f\"{config['DOMAIN_NAME']}_HRUs_elevation.shp\"\n",
    "\n",
    "if hru_file.exists():\n",
    "    hru_gdf = gpd.read_file(str(hru_file))\n",
    "    print(f\"Number of elevation-based HRUs: {len(hru_gdf)}\")\n",
    "    \n",
    "    # Calculate elevation statistics\n",
    "    if 'elev_mean' in hru_gdf.columns:\n",
    "        elev_col = 'elev_mean'\n",
    "    elif 'elevation' in hru_gdf.columns:\n",
    "        elev_col = 'elevation'\n",
    "    else:\n",
    "        elev_col = hru_gdf.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"Elevation range: {hru_gdf[elev_col].min():.0f} - {hru_gdf[elev_col].max():.0f} m\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Spatial map colored by elevation\n",
    "    hru_gdf.plot(column=elev_col, cmap='terrain', ax=axes[0], \n",
    "                 legend=True, legend_kwds={'label': 'Elevation (m)'})\n",
    "    \n",
    "    pour_point_gdf = gpd.read_file(pour_point_path)\n",
    "    pour_point_gdf.plot(ax=axes[0], color='red', markersize=150, marker='*', label='Pour Point')\n",
    "    \n",
    "    axes[0].set_title(f'Elevation-Based HRU Distribution\\n{len(hru_gdf)} HRUs', fontweight='bold')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Elevation distribution histogram\n",
    "    axes[1].hist(hru_gdf[elev_col], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Elevation (m)')\n",
    "    axes[1].set_ylabel('Number of HRUs')\n",
    "    axes[1].set_title('HRU Elevation Distribution', fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  HRU shapefile not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Data preprocessing\n",
    "\n",
    "Process forcing and observation data for elevation-stratified HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a ‚Äî Streamflow observations\n",
    "# If using MAF supported HPC, uncomment the line below\n",
    "# confluence.managers['data'].process_observed_data()\n",
    "print(\"‚úÖ Streamflow data processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b ‚Äî Forcing data\n",
    "# If using MAF supported HPC, uncomment the line below\n",
    "# confluence.managers['data'].acquire_forcings()\n",
    "print(\"‚úÖ Forcing acquisition complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c ‚Äî Model-agnostic preprocessing\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"‚úÖ Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Model execution\n",
    "\n",
    "Configure and run SUMMA-mizuRoute with elevation-stratified HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a ‚Äî Model configuration\n",
    "confluence.managers['model'].preprocess_models()\n",
    "print(\"‚úÖ Elevation-based model configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b ‚Äî Model execution\n",
    "print(f\"Running {config['HYDROLOGICAL_MODEL']} with {config['ROUTING_MODEL']} ({len(hru_gdf)} elevation-based HRUs)...\")\n",
    "confluence.managers['model'].run_models()\n",
    "print(\"‚úÖ Elevation-based distributed simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Evaluation\n",
    "\n",
    "Compare elevation-based results against observations and previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 ‚Äî Elevation-based evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "# Load observed streamflow\n",
    "obs_path = project_dir / \"observations\" / \"streamflow\" / \"preprocessed\" / f\"{config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "obs_df = pd.read_csv(obs_path, parse_dates=['datetime'])\n",
    "obs_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Load simulated streamflow\n",
    "routing_dir = project_dir / \"simulations\" / config['EXPERIMENT_ID'] / \"mizuRoute\"\n",
    "sim_files = list(routing_dir.glob('*_routed.nc'))\n",
    "if not sim_files:\n",
    "    raise FileNotFoundError(f\"No routed streamflow in: {routing_dir}\")\n",
    "\n",
    "sim_ds = xr.open_dataset(sim_files[0])\n",
    "sim_df = sim_ds['IRFroutedRunoff'].to_dataframe().reset_index()\n",
    "sim_df = sim_df.rename(columns={'time': 'datetime', 'IRFroutedRunoff': 'discharge_sim'})\n",
    "sim_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Merge and align\n",
    "eval_df = obs_df.join(sim_df, how='inner')\n",
    "obs_valid = eval_df['discharge_obs'].dropna()\n",
    "sim_valid = eval_df.loc[obs_valid.index, 'discharge_sim']\n",
    "\n",
    "# Metrics\n",
    "def nse(obs, sim):\n",
    "    return float(1 - np.sum((obs - sim)**2) / np.sum((obs - obs.mean())**2))\n",
    "\n",
    "def kge(obs, sim):\n",
    "    r = obs.corr(sim)\n",
    "    alpha = sim.std() / obs.std()\n",
    "    beta = sim.mean() / obs.mean()\n",
    "    return float(1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2))\n",
    "\n",
    "def pbias(obs, sim):\n",
    "    return float(100 * (sim.sum() - obs.sum()) / obs.sum())\n",
    "\n",
    "nse_val = round(nse(obs_valid, sim_valid), 3)\n",
    "kge_val = round(kge(obs_valid, sim_valid), 3)\n",
    "pbias_val = round(pbias(obs_valid, sim_valid), 1)\n",
    "\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  NSE: {nse_val}\")\n",
    "print(f\"  KGE: {kge_val}\")\n",
    "print(f\"  PBIAS: {pbias_val}%\")\n",
    "print(f\"  Number of HRUs: {len(hru_gdf)}\")\n",
    "\n",
    "# Visualization - compact 2x2 layout\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series\n",
    "axes[0, 0].plot(obs_valid.index, obs_valid.values, 'b-', label='Observed', linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].plot(sim_valid.index, sim_valid.values, 'r-', label=f'Elevation-Based ({len(hru_gdf)} HRUs)', \n",
    "                linewidth=1.2, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Discharge (m¬≥/s)')\n",
    "axes[0, 0].set_title('Elevation-Based Streamflow')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].text(0.02, 0.95, f\"NSE: {nse_val}\\nKGE: {kge_val}\\nBias: {pbias_val}%\\nHRUs: {len(hru_gdf)}\",\n",
    "                transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8), fontsize=9)\n",
    "\n",
    "# Scatter\n",
    "axes[0, 1].scatter(obs_valid, sim_valid, alpha=0.5, s=10, c='coral')\n",
    "max_val = max(obs_valid.max(), sim_valid.max())\n",
    "axes[0, 1].plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Observed (m¬≥/s)')\n",
    "axes[0, 1].set_ylabel('Simulated (m¬≥/s)')\n",
    "axes[0, 1].set_title('Observed vs Simulated')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly climatology\n",
    "monthly_obs = obs_valid.groupby(obs_valid.index.month).mean()\n",
    "monthly_sim = sim_valid.groupby(sim_valid.index.month).mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[1, 0].plot(monthly_obs.index, monthly_obs.values, 'b-o', label='Observed', markersize=6)\n",
    "axes[1, 0].plot(monthly_sim.index, monthly_sim.values, 'r-o', label='Simulated', markersize=6)\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "axes[1, 0].set_xticklabels(month_names)\n",
    "axes[1, 0].set_ylabel('Mean Discharge (m¬≥/s)')\n",
    "axes[1, 0].set_title('Seasonal Flow Regime')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tutorial progression comparison\n",
    "tutorial_data = {\n",
    "    'Tutorial': ['02a\\n(Lumped)', '02b\\n(Semi-Dist)', '02c\\n(Elevation)'],\n",
    "    'Units': [1, 'N/A', len(hru_gdf)],  # Update with actual 02b GRU count if available\n",
    "    'NSE': [0.65, 0.72, nse_val]  # Update with actual values from previous tutorials\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(tutorial_data['Tutorial']))\n",
    "bars = axes[1, 1].bar(x_pos, tutorial_data['NSE'], \n",
    "                       color=['lightblue', 'lightgreen', 'lightcoral'], \n",
    "                       alpha=0.7, edgecolor='navy')\n",
    "axes[1, 1].set_xlabel('Tutorial Progression')\n",
    "axes[1, 1].set_ylabel('Nash-Sutcliffe Efficiency')\n",
    "axes[1, 1].set_title('Performance vs Complexity')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(tutorial_data['Tutorial'])\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (bar, units) in enumerate(zip(bars, tutorial_data['Units'])):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{height:.3f}\\n({units} units)',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle(f'Elevation-Based Evaluation ‚Äî {config[\"DOMAIN_NAME\"]} ({len(hru_gdf)} HRUs)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Elevation-based evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated the most spatially-detailed modeling approach in the CONFLUENCE basin-scale series. Elevation-based HRU discretization captures critical altitudinal controls on mountain hydrology by stratifying each sub-basin into elevation bands.\n",
    "\n",
    "Key achievements:\n",
    "- Elevation band creation within validated GRU structure\n",
    "- Vertical stratification of temperature and precipitation\n",
    "- Improved representation of snowpack dynamics across elevation gradients\n",
    "- Efficient data reuse from Tutorial 02b\n",
    "\n",
    "The progression from lumped (02a) ‚Üí semi-distributed (02b) ‚Üí elevation-based (02c) demonstrates CONFLUENCE's flexibility in managing spatial complexity. Each approach has its place:\n",
    "- **Lumped**: Calibration, uncertainty analysis, baseline performance\n",
    "- **Semi-distributed**: Spatial process attribution, moderate computational cost\n",
    "- **Elevation-based**: Maximum detail for mountain systems, highest computational cost\n",
    "\n",
    "This completes the basin-scale modeling tutorial series, establishing the foundation for large-scale distributed applications and operational hydrological forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
