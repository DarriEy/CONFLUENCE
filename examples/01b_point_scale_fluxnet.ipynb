{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Point-Scale Workflow (Fluxnet Example)\n",
    "\n",
    "In this notebook demonstrates we will build on our experience with the snow simulations for the previous notebook. We'll simulate vertical processes at a single flux tower and compare our results with the flux tower observations ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Point-Scale Configuration\n",
    "\n",
    "We'll use a SNOTEL site as our example: Loveland Pass in Colorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Update name and experiment id\n",
    "config_dict['DOMAIN_NAME'] = 'CA-NS7_fluxnet'\n",
    "config_dict['EXPERIMENT_ID'] = 'CA-NS7_tutorial'\n",
    "config_dict['POUR_POINT_COORDS'] = '56.6358/-99.9483'\n",
    "\n",
    "# Save point-scale configuration to temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE with the new configuration\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Print summary of the key seetings\n",
    "print(\"=== Point-Scale Configuration ===\")\n",
    "print(f\"Domain Name: {config_dict['DOMAIN_NAME']}\")\n",
    "print(f\"Spatial Mode: {config_dict['SPATIAL_MODE']}\")\n",
    "print(f\"Location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"Period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"Forcing Data: {config_dict['FORCING_DATASET']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "print(\"Creating point-scale project structure...\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point (in this case, our SNOTEL location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition - Data acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes for point location...\")\n",
    "\n",
    "print(f\"Minimal bounding box: {confluence.config.get('BOUNDING_BOX_COORDS')}\")\n",
    "\n",
    "confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition - Domain creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Geospatial Domain Definition and Analysis\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Analysis ===\")\n",
    "\n",
    "# Define domain\n",
    "print(\"\\nDefining minimal domain for point-scale simulation...\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "# Discretize domain (single HRU for point-scale)\n",
    "print(\"\\nCreating single HRU for point-scale simulation...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")\n",
    "\n",
    "# Verify single HRU\n",
    "hru_dir = project_dir / 'shapefiles' / 'catchment'\n",
    "if hru_dir.exists():\n",
    "    hru_files = list(hru_dir.glob('*.shp'))\n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        print(f\"  - Number of HRUs: {len(hru_gdf)} (should be 1 for point-scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Agnostic Data Pre-Processing - Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Agnostic Data Pre-Processing\n",
    "print(\"=== Step 3: Model Agnostic Data Pre-Processing ===\")\n",
    "\n",
    "# Process observed data \n",
    "print(\"Processing observed data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data for point location...\")\n",
    "print(f\"Dataset: {confluence.config['FORCING_DATASET']}\")\n",
    "#confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Agnostic Data Pre-Processing - Remapping and zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nModel-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Specific - Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Specific Processing and Initialization\n",
    "print(\"=== Step 4: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nModel-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Specific - Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Result visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Visualize Observed vs. Simulated SWE\n",
    "print(\"=== Step 11: Comparing Observed vs. Simulated SWE ===\")\n",
    "\n",
    "# 1. Load the observed SWE data\n",
    "obs_swe_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"observations\" / \"snow\" / \"swe\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "# Check if observed data exists\n",
    "if not obs_swe_path.exists():\n",
    "    print(f\"Warning: Observed SWE data not found at {obs_swe_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    # Try to find data in parent directories\n",
    "    alt_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"**/observations/snow/swe/*_swe_processed.csv\"))\n",
    "    if alt_paths:\n",
    "        obs_swe_path = alt_paths[0]\n",
    "        print(f\"Found alternative SWE data at: {obs_swe_path}\")\n",
    "    else:\n",
    "        print(\"No observed SWE data found. Only simulated data will be displayed.\")\n",
    "\n",
    "# Load observed SWE data if available\n",
    "if obs_swe_path.exists():\n",
    "    print(f\"Loading observed SWE data from: {obs_swe_path}\")\n",
    "    obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'])\n",
    "    obs_swe.set_index('Date', inplace=True)\n",
    "    print(f\"Observed data period: {obs_swe.index.min()} to {obs_swe.index.max()}\")\n",
    "    print(f\"Observed SWE range: {obs_swe['SWE'].min():.2f} to {obs_swe['SWE'].max():.2f} mm\")\n",
    "else:\n",
    "    obs_swe = None\n",
    "\n",
    "# 2. Load the simulated SWE data\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Extract scalarSWE and convert to DataFrame\n",
    "sim_swe = ds['scalarSWE'].to_dataframe().reset_index()\n",
    "# Assuming first HRU for point-scale simulation\n",
    "sim_swe = sim_swe[sim_swe['hru'] == 1][['time', 'scalarSWE']]\n",
    "sim_swe.columns = ['Date', 'SWE']\n",
    "sim_swe.set_index('Date', inplace=True)\n",
    "print(f\"Simulated data period: {sim_swe.index.min()} to {sim_swe.index.max()}\")\n",
    "print(f\"Simulated SWE range: {sim_swe['SWE'].min():.2f} to {sim_swe['SWE'].max():.2f} mm\")\n",
    "\n",
    "# 3. Find common date range if observed data exists\n",
    "if obs_swe is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_swe = obs_swe.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    sim_swe = sim_swe.resample('D').mean()  # Daily mean if sub-daily sim data\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "    end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_swe.loc[start_date:end_date]\n",
    "    sim_period = sim_swe.loc[start_date:end_date]\n",
    "    \n",
    "    # Handle different units if necessary (assuming both are in mm)\n",
    "    # If different units, add conversion here\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(((obs_period['SWE'] - sim_period['SWE']) ** 2).mean())\n",
    "    bias = (sim_period['SWE'] - obs_period['SWE']).mean()\n",
    "    corr = obs_period['SWE'].corr(sim_period['SWE'])\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} mm\")\n",
    "    print(f\"  - Bias: {bias:.2f} mm\")\n",
    "    print(f\"  - Correlation: {corr:.2f}\")\n",
    "    \n",
    "    # 4. Visualize the comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot both time series\n",
    "    plt.plot(obs_period.index, obs_period['SWE'], 'o-', label='Observed SWE', color='black', alpha=0.7, markersize=4)\n",
    "    plt.plot(sim_period.index, sim_period['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "        \n",
    "    # Styling\n",
    "    plt.title(f\"SWE Comparison at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(obs_period['SWE'], sim_period['SWE'], color='blue', alpha=0.7)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    max_val = max(obs_period['SWE'].max(), sim_period['SWE'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    # Styling\n",
    "    plt.title(f\"Observed vs. Simulated SWE\", fontsize=14)\n",
    "    plt.xlabel('Observed SWE (mm)', fontsize=12)\n",
    "    plt.ylabel('Simulated SWE (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # If no observed data, just plot simulated\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sim_swe.index, sim_swe['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "    plt.title(f\"Simulated SWE at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nSWE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Point-Scale Modeling Insights\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale up to lumped basin**: Use calibrated parameters\n",
    "2. **Compare multiple sites**: Test model transferability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
