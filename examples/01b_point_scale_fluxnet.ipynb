{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Point-Scale Workflow (Fluxnet Example)\n",
    "\n",
    "In this notebook demonstrates we will build on our experience with the snow simulations for the previous notebook. We'll simulate vertical processes at a single flux tower and compare our results with the flux tower observations ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Point-Scale Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Update name and experiment id\n",
    "config_dict['DOMAIN_NAME'] = 'CA-NS7_fluxnet'\n",
    "config_dict['EXPERIMENT_ID'] = 'CA-NS7_tutorial'\n",
    "config_dict['POUR_POINT_COORDS'] = '56.6358/-99.9483'\n",
    "\n",
    "# Save point-scale configuration to temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE with the new configuration\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Print summary of the key seetings\n",
    "print(\"=== Point-Scale Configuration ===\")\n",
    "print(f\"Domain Name: {config_dict['DOMAIN_NAME']}\")\n",
    "print(f\"Spatial Mode: {config_dict['SPATIAL_MODE']}\")\n",
    "print(f\"Location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"Period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"Forcing Data: {config_dict['FORCING_DATASET']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "print(\"Creating point-scale project structure...\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point (in this case, our SNOTEL location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition - Data acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes for point location...\")\n",
    "\n",
    "print(f\"Minimal bounding box: {confluence.config.get('BOUNDING_BOX_COORDS')}\")\n",
    "\n",
    "confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition - Domain creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Geospatial Domain Definition and Analysis\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Analysis ===\")\n",
    "\n",
    "# Define domain\n",
    "print(\"\\nDefining minimal domain for point-scale simulation...\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "# Discretize domain (single HRU for point-scale)\n",
    "print(\"\\nCreating single HRU for point-scale simulation...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")\n",
    "\n",
    "# Verify single HRU\n",
    "hru_dir = project_dir / 'shapefiles' / 'catchment'\n",
    "if hru_dir.exists():\n",
    "    hru_files = list(hru_dir.glob('*.shp'))\n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        print(f\"  - Number of HRUs: {len(hru_gdf)} (should be 1 for point-scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Agnostic Data Pre-Processing - Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Agnostic Data Pre-Processing\n",
    "print(\"=== Step 3: Model Agnostic Data Pre-Processing ===\")\n",
    "\n",
    "# Process observed data \n",
    "print(\"Processing observed data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data for point location...\")\n",
    "print(f\"Dataset: {confluence.config['FORCING_DATASET']}\")\n",
    "#confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Agnostic Data Pre-Processing - Remapping and zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nModel-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Specific - Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Specific Processing and Initialization\n",
    "print(\"=== Step 4: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nModel-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Specific - Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulated SWE data\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Extract scalarSWE and convert to DataFrame\n",
    "sim_swe = ds['scalarSWE'].to_dataframe().reset_index()\n",
    "# Assuming first HRU for point-scale simulation\n",
    "sim_swe = sim_swe[sim_swe['hru'] == 1][['time', 'scalarSWE']]\n",
    "sim_swe.columns = ['Date', 'SWE']\n",
    "sim_swe.set_index('Date', inplace=True)\n",
    "print(f\"Simulated data period: {sim_swe.index.min()} to {sim_swe.index.max()}\")\n",
    "print(f\"Simulated SWE range: {sim_swe['SWE'].min():.2f} to {sim_swe['SWE'].max():.2f} mm\")\n",
    "\n",
    "# If no observed data, just plot simulated\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sim_swe.index, sim_swe['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "plt.title(f\"Simulated SWE at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nSWE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Point-Scale Modeling Insights\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale up to lumped basin**: Use calibrated parameters\n",
    "2. **Compare multiple sites**: Test model transferability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
