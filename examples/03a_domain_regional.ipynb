{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 6: Regional Domain Modeling (Iceland)\n",
    "\n",
    "## Introduction\n",
    "This tutorial marks a fundamental shift in scale and approach from single-watershed modeling to regional domain modeling. Instead of focusing on one watershed draining to a single outlet, we now model an entire region containing multiple independent drainage systems. Using Iceland as our example, we demonstrate how CONFLUENCE handles complex regional hydrology including coastal watersheds, multiple drainage basins, and diverse hydrological regimes within a single integrated modeling framework.\n",
    "\n",
    "## Regional Domain Modeling Philosophy\n",
    "Regional domain modeling treats an entire geographic region such as a country, province, or large administrative area as the modeling domain, fundamentally departing from traditional watershed-centric approaches. This methodology encompasses multiple independent watersheds within the domain, including coastal watersheds that drain directly to ocean boundaries rather than to interior confluence points. The approach enables comprehensive regional water resources assessment by providing understanding of water availability patterns across entire geographic regions while simultaneously addressing diverse hydrological conditions including multiple climate zones, varied topographic settings, and contrasting hydrological regimes within a unified analytical framework.\n",
    "\n",
    "The conceptual differences between watershed and regional modeling are substantial and fundamental. Drainage patterns shift from single outlet points to multiple independent outlets, while domain boundaries transition from topographic divides to administrative or geographic boundaries. Spatial scales expand from tens or thousands of square kilometers to tens of thousands or hundreds of thousands of square kilometers. Outlet systems evolve from rivers and streams to include both riverine systems and coastal discharge points. The analytical purpose advances from basin-specific studies to comprehensive regional water resources assessment and management.\n",
    "\n",
    "## Case Study: Iceland Regional Analysis\n",
    "Iceland provides an exceptional case study for regional modeling through its unique combination of geographic characteristics, hydrological diversity, and practical advantages for comprehensive analysis. The island nation offers clear regional boundaries with diverse drainage systems spanning approximately 103,000 square kilometers, creating a manageable demonstration scale that remains large enough to illustrate regional concepts effectively. The volcanic landscape creates complex topography that generates numerous independent watersheds, while the clear geographic boundaries of the island setting eliminate the boundary condition complexities that complicate continental regional studies.\n",
    "\n",
    "The hydrological diversity of Iceland encompasses glacial systems with ice-dominated watersheds that exhibit unique hydrological behaviors, volcanic terrain with permeable basalts that significantly affect groundwater flow patterns, coastal climate zones with maritime influences that create strong seasonal variations, and elevation gradients ranging from sea level to over 2000 meters that drive diverse hydrological responses. This diversity enables demonstration of regional modeling capabilities across fundamentally different hydrological regimes within a single coherent framework.\n",
    "\n",
    "Practical advantages include excellent data availability with good coverage of meteorological and topographic datasets, computational feasibility that balances demonstration scale with local execution capabilities, and clear boundary definition through the island setting that eliminates complex boundary condition specifications required for continental applications.\n",
    "\n",
    "## Technical Implementation Framework\n",
    "Regional modeling requires several key technical approaches that distinguish it from traditional watershed-specific methods. Bounding box delineation defines the region using geographic coordinates rather than pour point specifications, enabling comprehensive coverage of administrative or natural regions. Coastal watershed inclusion captures watersheds that drain directly to ocean boundaries, ensuring complete regional water accounting. Multiple drainage system handling accommodates independent watersheds without requiring upstream-downstream connectivity assumptions. Regional forcing application distributes meteorological data across diverse topographic and climate conditions while maintaining spatial coherence and physical realism.\n",
    "\n",
    "Critical configuration parameters undergo substantial modification for regional applications. Domain setup parameters change to disable pour point delineation in favor of bounding box definitions, specify regional extents rather than watershed-specific coordinates, and enable coastal watershed inclusion to capture ocean-draining basins. Computational considerations require higher stream threshold values to manage the increased complexity of regional stream networks, careful balance between spatial resolution detail and computational feasibility, and appropriate planning for extended processing times due to regional scale demands.\n",
    "\n",
    "## Learning Objectives and Tutorial Structure\n",
    "Through this tutorial, you will master the fundamental principles of scaling hydrological modeling from individual watersheds to comprehensive regions, understand the technical requirements and implementation strategies for regional domain modeling, develop capabilities for handling multiple independent drainage systems within unified frameworks, comprehend the scientific applications and policy relevance of regional-scale hydrology, and appreciate the computational and data management challenges inherent in large-scale modeling applications.\n",
    "\n",
    "This tutorial represents the largest spatial scale in our notebook series yet, and demonstrates CONFLUENCE's capability to handle complex regional hydrology while maintaining the same workflow efficiency and scientific rigor established in smaller-scale applications. The methodological foundation established here directly enables the continental-scale applications and large-sample studies that follow in subsequent tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Regional Domain Setup with Multi-Watershed Configuration\n",
    "This tutorial marks a shift from watershed-scale to regional-scale hydrological modeling. Moving beyond the single-watershed focus of our previous tutorials, we now model an entire region containing multiple independent drainage systems. Using Iceland as our case study, we demonstrate how CONFLUENCE scales from basin-specific modeling to comprehensive regional water resources assessment across diverse hydrological regimes.\n",
    "\n",
    "The same CONFLUENCE framework seamlessly scales from single-watershed to regional modeling while maintaining workflow consistency and scientific rigor across this dramatic increase in spatial scope and hydrological complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we'll need in this notebook\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import contextily as cx\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import warnings\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import CONFLUENCE\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION FOR REGIONAL ICELAND MODELING\n",
    "# =============================================================================\n",
    "\n",
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data')  # ‚Üê Update this path\n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# Verify paths exist\n",
    "if not CONFLUENCE_CODE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"CONFLUENCE code directory not found: {CONFLUENCE_CODE_DIR}\")\n",
    "\n",
    "if not CONFLUENCE_DATA_DIR.exists():\n",
    "    print(f\"Data directory doesn't exist. Creating: {CONFLUENCE_DATA_DIR}\")\n",
    "    CONFLUENCE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Iceland configuration from template\n",
    "iceland_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_Iceland.yaml'\n",
    "with open(iceland_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update for tutorial-specific settings\n",
    "config_updates = {\n",
    "    'CONFLUENCE_CODE_DIR': str(CONFLUENCE_CODE_DIR),\n",
    "    'CONFLUENCE_DATA_DIR': str(CONFLUENCE_DATA_DIR),\n",
    "    'DOMAIN_NAME': 'Iceland',\n",
    "    'EXPERIMENT_ID': 'regional',\n",
    "    'EXPERIMENT_TIME_START': '2011-01-01 01:00',\n",
    "    'EXPERIMENT_TIME_END': '2013-12-31 23:00',  # Shorter period for tutorial efficiency\n",
    "}\n",
    "\n",
    "config_dict.update(config_updates)\n",
    "\n",
    "# Save tutorial configuration\n",
    "tutorial_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_iceland_tutorial.yaml'\n",
    "with open(tutorial_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Regional Iceland configuration saved: {tutorial_config_path}\")\n",
    "\n",
    "# Initialize CONFLUENCE with tutorial config\n",
    "confluence = CONFLUENCE(tutorial_config_path)\n",
    "\n",
    "# Initialize project directory structure\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# For regional modeling, we still need a pour point file for technical reasons,\n",
    "# but it won't be used for actual delineation\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"   üìÅ Project directory: {project_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Regional Multi-Watershed Delineation and Coastal Integration\n",
    "The transition to regional modeling requires a fundamentally different approach to domain delineation. Instead of tracing upstream from a single pour point, we now identify all independent drainage systems within a geographic region, including the critical inclusion of coastal watersheds that drain directly to ocean boundaries. This represents a paradigm shift from watershed-centric to region-centric hydrological analysis.\n",
    "### Scientific Context: Regional Drainage System Analysis\n",
    "Regional multi-watershed delineation operates on fundamentally different principles compared to traditional watershed-specific approaches. Bounding box definition establishes geographic region boundaries based on administrative or natural regional limits rather than topographic watershed constraints, enabling comprehensive coverage of entire administrative units or natural regions. Independent drainage systems are accommodated within a single modeling framework, allowing multiple unconnected watersheds to be analyzed simultaneously without requiring artificial connectivity assumptions. Coastal watershed integration becomes critically important as many basins drain directly to ocean boundaries rather than to inland confluence points, necessitating explicit inclusion of these ocean-draining systems that traditional watershed modeling often overlooks. Regional stream networks comprise multiple independent stream systems that lack upstream-downstream connectivity between different watershed units, requiring specialized handling of network topology and routing algorithms. Outlet multiplicity characterizes regional systems through many independent discharge points rather than the single downstream outlet typical of watershed-specific studies.\n",
    "\n",
    "The critical differences from watershed modeling extend across multiple fundamental aspects of hydrological analysis. Domain logic transitions from topographic divides that define natural watershed boundaries to administrative or geographic boundaries that define regions of interest for water resources management. Outlet identification evolves from single known points with established gauge networks to multiple unknown coastal outlets that require systematic identification and characterization. Connectivity assumptions shift from connected drainage networks with clear upstream-downstream relationships to independent drainage systems that operate without inter-basin connectivity. Boundary conditions change from watershed edge specifications to complex ocean interface definitions that require specialized treatment of coastal discharge processes.\n",
    "\n",
    "The regional approach captures the complete hydrological picture of a geographic area, providing essential information for comprehensive water resources assessment, climate impact studies, and regional policy applications that require understanding of hydrological processes across administrative boundaries rather than natural watershed limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Acquire regional geospatial data\n",
    "#confluence.managers['data'].acquire_attributes()\n",
    "print(\"‚úÖ Regional geospatial data acquisition complete\")\n",
    "\n",
    "# Execute regional domain delineation\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "print(\"‚úÖ Regional multi-watershed delineation complete\")\n",
    "\n",
    "# Execute domain discretization for regional multi-watershed system\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "print(\"‚úÖ Regional multi-watershed discretisation complete\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGIONAL DRAINAGE SYSTEM ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load and analyze delineated watersheds\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "basin_count = 0\n",
    "basin_files = []\n",
    "basins_gdf = None\n",
    "\n",
    "basin_files = list(basin_path.glob('*.shp'))\n",
    "basins_gdf = gpd.read_file(basin_files[0])\n",
    "basins_gdf = basins_gdf.to_crs(epsg=32627)\n",
    "basin_count = len(basins_gdf)\n",
    "\n",
    "# Calculate regional statistics\n",
    "total_area = basins_gdf.geometry.area.sum() / 1e6  # Convert to km¬≤\n",
    "avg_area = total_area / basin_count\n",
    "max_area = basins_gdf.geometry.area.max() / 1e6\n",
    "min_area = basins_gdf.geometry.area.min() / 1e6\n",
    "watershed_areas = basins_gdf.geometry.area / 1e6\n",
    "\n",
    "\n",
    "# Analyze regional stream network\n",
    "network_count = 0\n",
    "network_files = []\n",
    "rivers_gdf = None\n",
    "\n",
    "hru_path = project_dir / 'shapefiles' / 'catchment' \n",
    "hru_files = list(hru_path.glob('*.shp'))\n",
    "hru_gdf = gpd.read_file(hru_files[0])\n",
    "            \n",
    "basins_gdf['elevation'] = hru_gdf['elev_mean']\n",
    "network_files = list(network_path.glob('*.shp'))\n",
    "rivers_gdf = gpd.read_file(network_files[0])\n",
    "network_count = len(rivers_gdf)\n",
    "total_length = rivers_gdf['Length'].sum() / 1000  # Convert to km\n",
    "                \n",
    "    \n",
    "# Create standalone map figure\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "# Use appropriate CRS for Iceland and get bounds\n",
    "# Try Iceland's national grid system first, fall back to geographic\n",
    "try:\n",
    "    basins_display = basins_gdf.to_crs(epsg=3057)  # ISN93 / Lambert 1993 (Iceland)\n",
    "    crs_name = \"ISN93 Lambert 1993\"\n",
    "    xlabel = \"Easting (ISN93)\"\n",
    "    ylabel = \"Northing (ISN93)\"\n",
    "except:\n",
    "    try:\n",
    "        basins_display = basins_gdf.to_crs(epsg=4326)  # WGS84 Geographic\n",
    "        crs_name = \"WGS84 Geographic\"\n",
    "        xlabel = \"Longitude (¬∞)\"\n",
    "        ylabel = \"Latitude (¬∞)\"\n",
    "    except:\n",
    "        basins_display = basins_gdf  # Use original CRS\n",
    "        crs_name = \"UTM Zone 27N\"\n",
    "        xlabel = \"Easting (UTM 27N)\"\n",
    "        ylabel = \"Northing (UTM 27N)\"\n",
    "\n",
    "# Plot watersheds with professional styling\n",
    "if 'elevation' in basins_display.columns:\n",
    "    # Plot with elevation coloring\n",
    "    im = basins_display.plot(ax=ax1, column='elevation', cmap='terrain', \n",
    "                           edgecolor='white', linewidth=0.5, alpha=0.8, legend=False)\n",
    "    \n",
    "    # Add colorbar for elevation\n",
    "    sm = plt.cm.ScalarMappable(cmap='terrain', \n",
    "                             norm=plt.Normalize(vmin=basins_display['elevation'].min(), \n",
    "                                               vmax=basins_display['elevation'].max()))\n",
    "    sm.set_array([])\n",
    "    cbar = fig1.colorbar(sm, ax=ax1, orientation='vertical', shrink=0.8, pad=0.02)\n",
    "    cbar.set_label('Elevation (m)', fontweight='bold', fontsize=12)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "elif 'GRU_ID' in basins_display.columns:\n",
    "    basins_display.plot(ax=ax1, column='GRU_ID', cmap='tab20', \n",
    "                       edgecolor='white', linewidth=0.5, alpha=0.8, legend=False)\n",
    "else:\n",
    "    basins_display.plot(ax=ax1, cmap='Set3', \n",
    "                       edgecolor='white', linewidth=0.5, alpha=0.8, legend=False)\n",
    "\n",
    "# Add stream network if available (reproject to match basins)\n",
    "if rivers_gdf is not None:\n",
    "    try:\n",
    "        rivers_display = rivers_gdf.to_crs(basins_display.crs)\n",
    "        rivers_display.plot(ax=ax1, color='#1f77b4', linewidth=0.8, alpha=0.9)\n",
    "    except:\n",
    "        pass  # Skip if reprojection fails\n",
    "\n",
    "# Set proper extent to show full region\n",
    "bounds = basins_display.total_bounds\n",
    "x_range = bounds[2] - bounds[0]\n",
    "y_range = bounds[3] - bounds[1]\n",
    "\n",
    "# Add 5% padding around the data\n",
    "padding = 0.05\n",
    "ax1.set_xlim(bounds[0] - x_range * padding, bounds[2] + x_range * padding)\n",
    "ax1.set_ylim(bounds[1] - y_range * padding, bounds[3] + y_range * padding)\n",
    "\n",
    "# Professional styling\n",
    "ax1.set_title('Regional Watershed Delineation: Iceland Domain\\n' + \n",
    "             f'{basin_count:,} Independent Coastal Watersheds', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel(xlabel, fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Set aspect ratio to equal for proper geographic representation\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Remove grid for cleaner look\n",
    "ax1.grid(False)\n",
    "ax1.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Add scale and north arrow (simplified)\n",
    "ax1.text(0.02, 0.02, 'N ‚Üë', transform=ax1.transAxes, \n",
    "        fontsize=14, fontweight='bold',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "\n",
    "# Add summary statistics box\n",
    "stats_text = (f'Domain Summary\\n'\n",
    "             f'Total Area: {total_area:,.0f} km¬≤\\n'\n",
    "             f'Watersheds: {basin_count:,}\\n'\n",
    "             f'Stream Length: {total_length:,.0f} km')\n",
    "\n",
    "ax1.text(0.98, 0.98, stats_text, transform=ax1.transAxes, \n",
    "        fontsize=11, ha='right', va='top',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', \n",
    "                 edgecolor='gray', alpha=0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FIGURE 2: COMPREHENSIVE STATISTICAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Create analysis dashboard\n",
    "fig2, ((ax2, ax4)) = plt.subplots(1, 2, figsize=(16, 12))\n",
    "\n",
    "# Subplot 1: Watershed Size Distribution (Enhanced)\n",
    "n_bins = min(30, max(10, basin_count//10))\n",
    "n, bins, patches = ax2.hist(watershed_areas, bins=n_bins, \n",
    "                           color='skyblue', alpha=0.7, edgecolor='navy', linewidth=0.8)\n",
    "\n",
    "# Add distribution statistics\n",
    "ax2.axvline(watershed_areas.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {watershed_areas.mean():.1f} km¬≤')\n",
    "ax2.axvline(watershed_areas.median(), color='orange', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {watershed_areas.median():.1f} km¬≤')\n",
    "\n",
    "ax2.set_xlabel('Watershed Area (km¬≤)', fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "ax2.set_title('A) Watershed Size Distribution', fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Statistical Summary Table\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create statistics table\n",
    "stats_data = [\n",
    "    ['Parameter', 'Value', 'Unit'],\n",
    "    ['Total Watersheds', f'{basin_count:,}', 'count'],\n",
    "    ['Total Area', f'{total_area:,.0f}', 'km¬≤'],\n",
    "    ['Mean Area', f'{watershed_areas.mean():.1f}', 'km¬≤'],\n",
    "    ['Median Area', f'{watershed_areas.median():.1f}', 'km¬≤'],\n",
    "    ['Std Deviation', f'{watershed_areas.std():.1f}', 'km¬≤'],\n",
    "    ['Min Area', f'{watershed_areas.min():.1f}', 'km¬≤'],\n",
    "    ['Max Area', f'{watershed_areas.max():.1f}', 'km¬≤'],\n",
    "    ['Skewness', f'{watershed_areas.skew():.2f}', '-'],\n",
    "    ['Stream Length', f'{total_length:,.0f}', 'km'],\n",
    "    ['Stream Density', f'{total_length/total_area:.2f}', 'km/km¬≤']\n",
    "]\n",
    "\n",
    "table = ax4.table(cellText=stats_data, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(stats_data)):\n",
    "    for j in range(len(stats_data[0])):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # Header row\n",
    "            cell.set_facecolor('#4CAF50')\n",
    "            cell.set_text_props(weight='bold', color='white')\n",
    "        else:\n",
    "            cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n",
    "\n",
    "ax4.set_title('B) Regional Statistics Summary', fontweight='bold', fontsize=14, pad=20)\n",
    "\n",
    "plt.suptitle('Regional Watershed Analysis: Statistical Assessment', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Regional Multi-Watershed Data Pipeline\n",
    "The same model-agnostic preprocessing framework now scales to handle multiple independent drainage systems across an entire region, representing the most computationally demanding and hydrologically diverse configuration in our tutorial series. Unlike previous tutorials that processed connected watersheds or sub-watersheds, we now handle dozens of independent drainage systems with diverse hydrological regimes, coastal boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Check if forcing data acquisition is needed\n",
    "forcing_dir = project_dir / 'forcing' / 'raw_data'\n",
    "if not forcing_dir.exists() or len(list(forcing_dir.glob('*.nc'))) == 0:    \n",
    "    # confluence.managers['data'].acquire_forcings()\n",
    "    print(\"‚úÖ Regional forcing data acquisition complete\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Regional forcing data available\")\n",
    "    print(f\"   Reusing meteorological data for regional domain\")\n",
    "\n",
    "#Executing Regional Model-Agnostic Preprocessing...\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "print(\"‚úÖ Regional model-agnostic preprocessing complete\")\n",
    "\n",
    "#Executing Regional Model-Specific Preprocessing...\n",
    "confluence.managers['model'].preprocess_models()\n",
    "print(\"‚úÖ Regional model-specific preprocessing complete\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Regional Multi-Watershed Model Execution\n",
    "The same SUMMA process-based physics now executes across multiple independent drainage systems spanning an entire region. This integration of regional-scale meteorological forcing with multiple unconnected watershed systems demonstrates how the same computational framework scales to handle comprehensive regional water resources assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "# Execute the regional model system\n",
    "confluence.managers['model'].run_models()\n",
    "print(\"‚úÖ Regional multi-watershed simulation complete\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Regional Multi-Watershed Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load watershed boundaries and network data\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "# Load watershed boundaries\n",
    "basins_gdf = None\n",
    "basin_files = list(basin_path.glob('*.shp'))\n",
    "basins_gdf = gpd.read_file(basin_files[0])\n",
    "\n",
    "# Load river network for proper outlet identification\n",
    "network_gdf = None\n",
    "outlet_segments = []\n",
    "network_files = list(network_path.glob('*.shp'))\n",
    "network_gdf = gpd.read_file(network_files[0])\n",
    "\n",
    "# Identify true outlets (DSLINKNO = -1)\n",
    "if 'DSLINKNO' in network_gdf.columns:\n",
    "    outlet_links = network_gdf[network_gdf['DSLINKNO'] == -1]\n",
    "    outlet_segments = outlet_links['LINKNO'].values\n",
    "\n",
    "# Load HRU/GRU boundaries\n",
    "hru_gdf = None\n",
    "hru_files = list(catchment_path.glob('*.shp'))\n",
    "hru_gdf = gpd.read_file(hru_files[0])\n",
    "\n",
    "\n",
    "# Load SUMMA regional outputs\n",
    "simulation_dir = project_dir / 'simulations' / config_dict['EXPERIMENT_ID']\n",
    "summa_dir = simulation_dir / 'SUMMA'\n",
    "routing_dir = simulation_dir / 'mizuRoute'\n",
    "\n",
    "summa_files = list(summa_dir.glob('*day.nc')) if summa_dir.exists() else []\n",
    "regional_summa_data = None\n",
    "\n",
    "regional_summa_data = xr.open_dataset(summa_files[0])\n",
    "\n",
    "# Load mizuRoute regional outputs\n",
    "routing_files = list(routing_dir.glob('*.nc')) if routing_dir.exists() else []\n",
    "regional_routing_data = None\n",
    "regional_routing_data = xr.open_dataset(routing_files[0])\n",
    "\n",
    "\n",
    "spatial_data = {}\n",
    "# Snow Water Equivalent\n",
    "if 'scalarSWE' in regional_summa_data.data_vars:\n",
    "    swe_data = regional_summa_data['scalarSWE'].mean(dim='time')\n",
    "    spatial_data['SWE'] = {\n",
    "        'data': swe_data,\n",
    "        'name': 'Snow Water Equivalent',\n",
    "        'units': 'mm',\n",
    "        'cmap': 'Blues'\n",
    "    }\n",
    "    print(f\"   ‚úì SWE: {swe_data.mean().values:.1f} ¬± {swe_data.std().values:.1f} mm\")\n",
    "\n",
    "# Soil Moisture \n",
    "if 'mLayerVolFracLiq' in regional_summa_data.data_vars:\n",
    "    soil_data = regional_summa_data['mLayerVolFracLiq']\n",
    "    if 'midToto' in soil_data.dims:\n",
    "        soil_data = soil_data.where(soil_data != -9999)\n",
    "        soil_data = soil_data.mean(dim='midToto')\n",
    "    soil_data = soil_data.mean(dim='time')\n",
    "        \n",
    "    spatial_data['Soil'] = {\n",
    "        'data': soil_data,\n",
    "        'name': 'Soil Moisture',\n",
    "        'units': 'Vol. fraction',\n",
    "        'cmap': 'BrBG'\n",
    "    }\n",
    "    print(f\"   ‚úì Soil Moisture: {soil_data.mean().values:.2f} ¬± {soil_data.std().values:.2f} m¬≥/m¬≥\")\n",
    "\n",
    "# Evapotranspiration\n",
    "et_data = None\n",
    "if 'scalarTotalET' in regional_summa_data.data_vars:\n",
    "    et_data = regional_summa_data['scalarTotalET'].mean(dim='time')\n",
    "    et_data = np.abs(et_data)  # Make positive\n",
    "    et_name = 'scalarTotalET'\n",
    "elif 'scalarLatHeatTotal' in regional_summa_data.data_vars:\n",
    "    et_data = regional_summa_data['scalarLatHeatTotal'].mean(dim='time')\n",
    "    et_data = np.abs(et_data)  # Make positive\n",
    "    # Latent heat (W/m¬≤) to ET (mm/day): LE / 28.4\n",
    "    et_data = et_data / 28.4\n",
    "    et_name = 'scalarLatHeatTotal'\n",
    "\n",
    "if et_data is not None:    \n",
    "    spatial_data['ET'] = {\n",
    "        'data': et_data,\n",
    "        'name': 'Evapotranspiration',\n",
    "        'units': 'mm/day',\n",
    "        'cmap': 'YlOrRd'\n",
    "    }\n",
    "    print(f\"   ‚úì ET ({et_name}): {et_data.mean().values:.2f} ¬± {et_data.std().values:.2f} mm/day\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS OUTLET DATA\n",
    "# =============================================================================\n",
    "\n",
    "total_runoff_ts = None\n",
    "outlet_flows_mean = None\n",
    "outlet_gdf = None\n",
    "\n",
    "streamflow_data = regional_routing_data['IRFroutedRunoff']\n",
    "n_segments = streamflow_data.sizes.get('seg', 0)\n",
    "\n",
    "# Map outlet LINKNO to segment indices\n",
    "valid_outlets = []\n",
    "for link_no in outlet_segments:\n",
    "    if link_no < n_segments:\n",
    "        valid_outlets.append(link_no)\n",
    "\n",
    "if len(valid_outlets) > 0:\n",
    "    # Calculate total Iceland runoff\n",
    "    total_iceland_runoff = streamflow_data.isel(seg=valid_outlets).sum(dim='seg')\n",
    "    total_runoff_ts = total_iceland_runoff.to_pandas()\n",
    "    \n",
    "    # Get mean flows for each outlet\n",
    "    outlet_flows_mean = streamflow_data.isel(seg=valid_outlets).mean(dim='time')\n",
    "    \n",
    "    # Create outlet geodataframe for mapping\n",
    "    if network_gdf is not None:\n",
    "        outlet_network = network_gdf[network_gdf['DSLINKNO'] == -1].copy()\n",
    "        if len(outlet_network) == len(outlet_flows_mean):\n",
    "            outlet_network['mean_flow'] = outlet_flows_mean.values\n",
    "            outlet_gdf = outlet_network\n",
    "    \n",
    "    print(f\"   ‚úì Total outlets: {len(valid_outlets)}\")\n",
    "    print(f\"   ‚úì Flow range: {outlet_flows_mean.min().values:.1f} - {outlet_flows_mean.max().values:.1f} m¬≥/s\")\n",
    "    print(f\"   ‚úì Total discharge: {total_runoff_ts.mean():.1f} ¬± {total_runoff_ts.std():.1f} m¬≥/s\")\n",
    "\n",
    "# =============================================================================\n",
    "#  VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create main figure with subplots\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "\n",
    "# Prepare HRU data for plotting\n",
    "hru_plot = hru_gdf.copy()\n",
    "if hru_plot.crs != 'EPSG:4326':\n",
    "    try:\n",
    "        hru_plot = hru_plot.to_crs('EPSG:4326')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for i, (key, var_dict) in enumerate(spatial_data.items()):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    \n",
    "    var_data = var_dict['data']\n",
    "    if len(var_data) == len(hru_plot):\n",
    "        hru_plot[f'{key}_val'] = var_data.values\n",
    "        \n",
    "        # Create choropleth map\n",
    "        im = hru_plot.plot(ax=ax, column=f'{key}_val', \n",
    "                          cmap=var_dict['cmap'], \n",
    "                          edgecolor='none', \n",
    "                          legend=False)\n",
    "        \n",
    "        # Add colorbar\n",
    "        vmin, vmax = var_data.min().values, var_data.max().values\n",
    "        sm = plt.cm.ScalarMappable(cmap=var_dict['cmap'], \n",
    "                                 norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, shrink=0.7, pad=0.02)\n",
    "        cbar.set_label(f\"{var_dict['units']}\", fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Add watershed boundaries\n",
    "        if basins_gdf is not None:\n",
    "            basins_plot = basins_gdf.copy()\n",
    "            try:\n",
    "                if basins_plot.crs != hru_plot.crs:\n",
    "                    basins_plot = basins_plot.to_crs(hru_plot.crs)\n",
    "            except:\n",
    "                pass\n",
    "            basins_plot.boundary.plot(ax=ax, color='white', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        # Title and stats\n",
    "        mean_val = var_data.mean().values\n",
    "        std_val = var_data.std().values\n",
    "        ax.set_title(f\"{var_dict['name']}\\n{mean_val:.2f} ¬± {std_val:.2f} {var_dict['units']}\", \n",
    "                    fontweight='bold', fontsize=12)\n",
    "        \n",
    "    ax.set_xlabel('Longitude', fontweight='bold', fontsize=10)\n",
    "    ax.set_ylabel('Latitude', fontweight='bold', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ROW 1: OUTLET LOCATIONS MAP (4th plot)\n",
    "# =============================================================================\n",
    "\n",
    "ax_outlets = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "if outlet_gdf is not None and hru_gdf is not None:\n",
    "    # Plot HRUs as background\n",
    "    hru_plot.plot(ax=ax_outlets, color='lightgray', edgecolor='white', linewidth=0.1, alpha=0.6)\n",
    "    \n",
    "    # Plot outlets colored by flow magnitude\n",
    "    outlet_plot = outlet_gdf.copy()\n",
    "    try:\n",
    "        if outlet_plot.crs != hru_plot.crs:\n",
    "            outlet_plot = outlet_plot.to_crs(hru_plot.crs)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Categorize outlets by flow size\n",
    "    flows = outlet_plot['mean_flow']\n",
    "    small_outlets = outlet_plot[flows <= flows.quantile(0.7)]\n",
    "    medium_outlets = outlet_plot[(flows > flows.quantile(0.7)) & (flows <= flows.quantile(0.9))]\n",
    "    large_outlets = outlet_plot[flows > flows.quantile(0.9)]\n",
    "    \n",
    "    # Plot different sized outlets\n",
    "    if len(small_outlets) > 0:\n",
    "        small_outlets.plot(ax=ax_outlets, color='lightblue', markersize=8, alpha=0.7, label=f'Small (<{flows.quantile(0.7):.1f} m¬≥/s)')\n",
    "    if len(medium_outlets) > 0:\n",
    "        medium_outlets.plot(ax=ax_outlets, color='orange', markersize=15, alpha=0.8, label=f'Medium ({flows.quantile(0.7):.1f}-{flows.quantile(0.9):.1f} m¬≥/s)')\n",
    "    if len(large_outlets) > 0:\n",
    "        large_outlets.plot(ax=ax_outlets, color='red', markersize=25, alpha=0.9, label=f'Large (>{flows.quantile(0.9):.1f} m¬≥/s)')\n",
    "    \n",
    "    # Add watershed boundaries\n",
    "    if basins_gdf is not None:\n",
    "        basins_plot.boundary.plot(ax=ax_outlets, color='black', linewidth=0.8, alpha=0.8)\n",
    "    \n",
    "    ax_outlets.legend(fontsize=9, loc='upper right')\n",
    "    ax_outlets.set_title(f'Coastal Outlets (n={len(outlet_plot)})\\nFlow Categories', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Add summary stats\n",
    "    stats_text = (f'Total: {len(outlet_plot)} outlets\\n'\n",
    "                 f'Range: {flows.min():.1f} - {flows.max():.1f} m¬≥/s\\n'\n",
    "                 f'Median: {flows.median():.1f} m¬≥/s')\n",
    "    ax_outlets.text(0.02, 0.02, stats_text, transform=ax_outlets.transAxes,\n",
    "                   bbox=dict(facecolor='white', alpha=0.8), fontsize=9, va='bottom')\n",
    "\n",
    "ax_outlets.set_xlabel('Longitude', fontweight='bold', fontsize=10)\n",
    "ax_outlets.set_ylabel('Latitude', fontweight='bold', fontsize=10)\n",
    "ax_outlets.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create main figure with subplots\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(2, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "if total_runoff_ts is not None:\n",
    "    \n",
    "    # Time series (spans 2 columns)\n",
    "    ax_ts = fig.add_subplot(gs[0, :2])\n",
    "    ax_ts.plot(total_runoff_ts.index, total_runoff_ts.values, 'b-', linewidth=1.5, alpha=0.8)\n",
    "    ax_ts.set_title('Total Iceland Coastal Runoff', fontweight='bold', fontsize=14)\n",
    "    ax_ts.set_ylabel('Discharge (m¬≥/s)', fontweight='bold')\n",
    "    ax_ts.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics on plot\n",
    "    stats_text = (f'Mean: {total_runoff_ts.mean():.1f} m¬≥/s\\n'\n",
    "                 f'Peak: {total_runoff_ts.max():.1f} m¬≥/s\\n'\n",
    "                 f'Min: {total_runoff_ts.min():.1f} m¬≥/s\\n'\n",
    "                 f'Annual: {total_runoff_ts.sum() * 86400 / 1e9 / len(total_runoff_ts.resample(\"YE\").sum()):.0f} km¬≥/yr')\n",
    "    ax_ts.text(0.02, 0.98, stats_text, transform=ax_ts.transAxes,\n",
    "              bbox=dict(facecolor='white', alpha=0.9), fontsize=11, va='top', fontweight='bold')\n",
    "    \n",
    "    # Seasonal pattern\n",
    "    ax_seasonal = fig.add_subplot(gs[0, 2])\n",
    "    monthly_mean = total_runoff_ts.groupby(total_runoff_ts.index.month).mean()\n",
    "    monthly_std = total_runoff_ts.groupby(total_runoff_ts.index.month).std()\n",
    "    months = range(1, 13)\n",
    "    month_names = ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']\n",
    "    \n",
    "    monthly_vals = [monthly_mean.get(m, 0) for m in months]\n",
    "    monthly_errs = [monthly_std.get(m, 0) for m in months]\n",
    "    \n",
    "    bars = ax_seasonal.bar(months, monthly_vals, yerr=monthly_errs, \n",
    "                          capsize=3, color='skyblue', edgecolor='navy', alpha=0.8)\n",
    "    ax_seasonal.set_xticks(months)\n",
    "    ax_seasonal.set_xticklabels(month_names)\n",
    "    ax_seasonal.set_ylabel('Mean Discharge (m¬≥/s)', fontweight='bold')\n",
    "    ax_seasonal.set_title('Seasonal Pattern', fontweight='bold', fontsize=12)\n",
    "    ax_seasonal.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight peak season\n",
    "    peak_month = np.argmax(monthly_vals) + 1\n",
    "    peak_season = ['Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer', \n",
    "                  'Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter'][peak_month-1]\n",
    "    ax_seasonal.text(0.5, 0.95, f'Peak: {peak_season}', transform=ax_seasonal.transAxes,\n",
    "                    ha='center', fontweight='bold', fontsize=11,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Flow distribution\n",
    "    ax_dist = fig.add_subplot(gs[0, 3])\n",
    "    ax_dist.hist(total_runoff_ts.values, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax_dist.axvline(total_runoff_ts.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax_dist.axvline(total_runoff_ts.median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
    "    ax_dist.set_xlabel('Discharge (m¬≥/s)', fontweight='bold')\n",
    "    ax_dist.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax_dist.set_title('Flow Distribution', fontweight='bold', fontsize=12)\n",
    "    ax_dist.legend(fontsize=10)\n",
    "    ax_dist.grid(True, alpha=0.3)\n",
    "\n",
    "# =============================================================================\n",
    "# ROW 3: OUTLET ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if outlet_flows_mean is not None and len(outlet_flows_mean) > 0:\n",
    "    \n",
    "    # Outlet size distribution\n",
    "    ax_outlet_dist = fig.add_subplot(gs[1, 0])\n",
    "    flows = outlet_flows_mean.values\n",
    "    ax_outlet_dist.hist(flows, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax_outlet_dist.set_xlabel('Mean Flow (m¬≥/s)', fontweight='bold')\n",
    "    ax_outlet_dist.set_ylabel('Number of Outlets', fontweight='bold')\n",
    "    ax_outlet_dist.set_title('Outlet Size Distribution', fontweight='bold', fontsize=12)\n",
    "    ax_outlet_dist.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentile lines\n",
    "    p90 = np.percentile(flows, 90)\n",
    "    p50 = np.percentile(flows, 50)\n",
    "    ax_outlet_dist.axvline(p90, color='red', linestyle='--', label='90th %ile')\n",
    "    ax_outlet_dist.axvline(p50, color='orange', linestyle='--', label='Median')\n",
    "    ax_outlet_dist.legend(fontsize=10)\n",
    "    \n",
    "    # Flow contribution pie chart\n",
    "    ax_pie = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    # Categorize flows\n",
    "    large_flow = flows[flows > np.percentile(flows, 90)].sum()\n",
    "    medium_flow = flows[(flows > np.percentile(flows, 70)) & (flows <= np.percentile(flows, 90))].sum()\n",
    "    small_flow = flows[flows <= np.percentile(flows, 70)].sum()\n",
    "    \n",
    "    sizes = [large_flow, medium_flow, small_flow]\n",
    "    labels = [f'Large (>90%)\\n{len(flows[flows > np.percentile(flows, 90)])} outlets',\n",
    "              f'Medium (70-90%)\\n{len(flows[(flows > np.percentile(flows, 70)) & (flows <= np.percentile(flows, 90))])} outlets',\n",
    "              f'Small (<70%)\\n{len(flows[flows <= np.percentile(flows, 70)])} outlets']\n",
    "    colors = ['red', 'orange', 'lightblue']\n",
    "    \n",
    "    wedges, texts, autotexts = ax_pie.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "                                         startangle=90, textprops={'fontsize': 10})\n",
    "    ax_pie.set_title('Flow Contribution by Outlet Size', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Make percentage text bold\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_color('white')\n",
    "    \n",
    "    # Cumulative flow analysis\n",
    "    ax_cumul = fig.add_subplot(gs[1, 2:])\n",
    "    sorted_flows = np.sort(flows)[::-1]  # Descending\n",
    "    cumulative_pct = np.cumsum(sorted_flows) / sorted_flows.sum() * 100\n",
    "    outlet_pct = np.arange(1, len(sorted_flows) + 1) / len(sorted_flows) * 100\n",
    "    \n",
    "    ax_cumul.plot(outlet_pct, cumulative_pct, 'b-', linewidth=3, alpha=0.8)\n",
    "    ax_cumul.axhline(80, color='red', linestyle='--', alpha=0.7, label='80% of flow')\n",
    "    ax_cumul.axhline(50, color='orange', linestyle='--', alpha=0.7, label='50% of flow')\n",
    "    \n",
    "    # Find what % of outlets produce 80% and 50% of flow\n",
    "    idx_80 = np.where(cumulative_pct >= 80)[0]\n",
    "    idx_50 = np.where(cumulative_pct >= 50)[0]\n",
    "    \n",
    "    if len(idx_80) > 0:\n",
    "        outlets_80 = outlet_pct[idx_80[0]]\n",
    "        ax_cumul.axvline(outlets_80, color='red', linestyle=':', alpha=0.7)\n",
    "        ax_cumul.text(outlets_80 + 5, 82, f'{outlets_80:.0f}% of outlets\\nproduce 80% of flow', \n",
    "                     fontweight='bold', fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    if len(idx_50) > 0:\n",
    "        outlets_50 = outlet_pct[idx_50[0]]\n",
    "        ax_cumul.axvline(outlets_50, color='orange', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    ax_cumul.set_xlabel('Outlets (% ranked by flow)', fontweight='bold')\n",
    "    ax_cumul.set_ylabel('Cumulative Flow (%)', fontweight='bold')\n",
    "    ax_cumul.set_title('Outlet Flow Concentration', fontweight='bold', fontsize=12)\n",
    "    ax_cumul.legend(fontsize=10)\n",
    "    ax_cumul.grid(True, alpha=0.3)\n",
    "\n",
    "# Main title\n",
    "fig.suptitle(f'Iceland Regional Hydrological Analysis\\n'\n",
    "           f'{len(outlet_segments) if len(outlet_segments) > 0 else \"Unknown\"} Coastal Outlets ‚Ä¢ '\n",
    "           f'{regional_summa_data.dims.get(\"time\", \"Unknown\") if regional_summa_data else \"Unknown\"} Days ‚Ä¢ '\n",
    "           f'{len(hru_gdf) if hru_gdf is not None else \"Unknown\"} HRUs',\n",
    "           fontsize=18, fontweight='bold', y=0.96)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "if regional_summa_data is not None:\n",
    "    regional_summa_data.close()\n",
    "if regional_routing_data is not None:\n",
    "    regional_routing_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamflow evaluation against the LAMAH-ICE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.edgecolor': '#333333',\n",
    "    'axes.linewidth': 0.8,\n",
    "    'xtick.color': '#333333',\n",
    "    'ytick.color': '#333333',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'legend.fontsize': 9,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "def load_and_analyze_network_data():\n",
    "    \"\"\"Load and analyze network data to understand segment indexing\"\"\"\n",
    "        \n",
    "    # Load river network shapefile\n",
    "    network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "    network_files = list(network_path.glob('*.shp'))\n",
    "    \n",
    "    if not network_files:\n",
    "        print(\"‚ùå No river network shapefile found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    network_gdf = gpd.read_file(network_files[0])\n",
    "    if network_gdf.crs and network_gdf.crs.to_string() != 'EPSG:4326':\n",
    "        network_gdf = network_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "    # Check for segment ID columns\n",
    "    id_columns = [col for col in network_gdf.columns \n",
    "                 if any(word in col.upper() for word in ['ID', 'LINK', 'SEG', 'REACH'])]\n",
    "    print(f\"   ID columns: {id_columns}\")\n",
    "    \n",
    "    # Load simulation data\n",
    "    routing_dir = project_dir / 'simulations' / config_dict['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "    routing_files = list(routing_dir.glob('*.nc'))\n",
    "    \n",
    "    if not routing_files:\n",
    "        print(\"‚ùå No simulation files found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    sim_ds = xr.open_dataset(routing_files[0])\n",
    "    streamflow_var = None\n",
    "    \n",
    "    # Find streamflow variable\n",
    "    possible_vars = ['IRFroutedRunoff', 'streamflow', 'discharge', 'runoff', 'q']\n",
    "    for var in possible_vars:\n",
    "        if var in sim_ds.variables:\n",
    "            streamflow_var = var\n",
    "            break\n",
    "    \n",
    "    if streamflow_var is None:\n",
    "        print(\"‚ùå No streamflow variable found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    streamflow_sim = sim_ds[streamflow_var]\n",
    "    n_segments = streamflow_sim.sizes.get('seg', 0)\n",
    "        \n",
    "    return network_gdf, streamflow_sim, id_columns\n",
    "\n",
    "def find_closest_river_segment(gauge_point, network_gdf, max_distance=0.01):\n",
    "    \"\"\"Find the river segment closest to a gauge location\"\"\"\n",
    "    \n",
    "    # Calculate distances from gauge to all river segments\n",
    "    distances = network_gdf.geometry.distance(gauge_point)\n",
    "    closest_idx = distances.idxmin()\n",
    "    closest_distance = distances.min()\n",
    "    \n",
    "    # Convert distance from degrees to km (approximate)\n",
    "    distance_km = closest_distance * 111  # rough conversion\n",
    "    \n",
    "    if distance_km > max_distance * 111:  # max_distance in degrees\n",
    "        return None, None, distance_km\n",
    "    \n",
    "    closest_segment = network_gdf.loc[closest_idx]\n",
    "    return closest_segment, closest_idx, distance_km\n",
    "\n",
    "def extract_segment_flow(streamflow_sim, segment_idx, id_columns, network_gdf):\n",
    "    \"\"\"Extract flow data for a specific segment with proper indexing\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Direct index (if network and sim have same order)\n",
    "        max_seg = streamflow_sim.sizes.get('seg', 0)\n",
    "        \n",
    "        if segment_idx < max_seg:\n",
    "            sim_data = streamflow_sim.isel(seg=segment_idx)\n",
    "            sim_ts = sim_data.to_pandas()\n",
    "            sim_ts.index = pd.to_datetime(sim_ts.index).normalize()\n",
    "            \n",
    "            # Check data quality\n",
    "            valid_data = sim_ts.dropna()\n",
    "            if len(valid_data) == 0:\n",
    "                return None, \"No valid data\"\n",
    "            \n",
    "            # Check for unrealistic values\n",
    "            if valid_data.max() < 0.001:  # Very small values\n",
    "                return None, f\"Suspiciously small values (max: {valid_data.max():.6f})\"\n",
    "            \n",
    "            return sim_ts, \"Success\"\n",
    "        else:\n",
    "            return None, f\"Index {segment_idx} >= {max_seg}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {str(e)}\"\n",
    "\n",
    "def process_gauge_with_improved_matching(gauge, network_gdf, streamflow_sim, obs_data_dir, id_columns):\n",
    "    \"\"\"Process a single gauge with improved segment matching\"\"\"\n",
    "    \n",
    "    gauge_id = gauge['id'] \n",
    "    gauge_name = gauge['name']\n",
    "    gauge_point = gauge.geometry\n",
    "    \n",
    "    print(f\"\\nüìç Processing {gauge_name} (ID: {gauge_id})\")\n",
    "    \n",
    "    # Find closest river segment\n",
    "    closest_segment, segment_idx, distance_km = find_closest_river_segment(gauge_point, network_gdf)\n",
    "    \n",
    "    if closest_segment is None:\n",
    "        print(f\"   ‚ùå No nearby river segment (closest: {distance_km:.2f} km)\")\n",
    "        return None\n",
    "    \n",
    "    # Load observed data\n",
    "    obs_file = obs_data_dir / f\"ID_{gauge_id}.csv\"\n",
    "    # Process observed data \n",
    "    for sep in [';', ',', '\\t']:\n",
    "        try:\n",
    "            obs_df = pd.read_csv(obs_file, sep=sep)\n",
    "            if obs_df.shape[1] > 1:\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if 'YYYY' in obs_df.columns and 'MM' in obs_df.columns and 'DD' in obs_df.columns:\n",
    "        obs_df['date'] = pd.to_datetime({\n",
    "            'year': obs_df['YYYY'],\n",
    "            'month': obs_df['MM'], \n",
    "            'day': obs_df['DD']\n",
    "        })\n",
    "    else:\n",
    "        print(f\"   ‚ùå Cannot parse dates in observed data\")\n",
    "        return None\n",
    "    \n",
    "    # Find discharge column\n",
    "    discharge_col = None\n",
    "    for col in obs_df.columns:\n",
    "        if any(word in col.lower() for word in ['qobs', 'discharge', 'flow', 'q_']):\n",
    "            discharge_col = col\n",
    "            break\n",
    "    \n",
    "    if discharge_col is None:\n",
    "        print(f\"   ‚ùå No discharge column found\")\n",
    "        return None\n",
    "    \n",
    "    obs_df.set_index('date', inplace=True)\n",
    "    obs_discharge = pd.to_numeric(obs_df[discharge_col], errors='coerce').dropna()\n",
    "    obs_discharge = obs_discharge[(obs_discharge > 0) & (obs_discharge < 1e6)]\n",
    "    \n",
    "    if len(obs_discharge) < 100:\n",
    "        print(f\"   ‚ùå Insufficient observed data ({len(obs_discharge)} points)\")\n",
    "        return None\n",
    "                \n",
    "    # Extract simulated data\n",
    "    sim_discharge, status_msg = extract_segment_flow(streamflow_sim, segment_idx, id_columns, network_gdf)\n",
    "        \n",
    "    # Find common period\n",
    "    common_start = max(obs_discharge.index.min(), sim_discharge.index.min())\n",
    "    common_end = min(obs_discharge.index.max(), sim_discharge.index.max())\n",
    "    \n",
    "    if common_start >= common_end:\n",
    "        print(f\"   ‚ùå No overlapping period\")\n",
    "        return None\n",
    "    \n",
    "    # Align data\n",
    "    obs_common = obs_discharge.loc[common_start:common_end]\n",
    "    sim_common = sim_discharge.loc[common_start:common_end]\n",
    "    aligned_data = pd.DataFrame({'obs': obs_common, 'sim': sim_common}).dropna()\n",
    "    \n",
    "    if len(aligned_data) < 100:\n",
    "        print(f\"   ‚ùå Insufficient aligned data ({len(aligned_data)} points)\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    obs_vals = aligned_data['obs'].values\n",
    "    sim_vals = aligned_data['sim'].values\n",
    "    \n",
    "    if np.std(obs_vals) == 0 or len(obs_vals) < 2:\n",
    "        print(f\"   ‚ùå Insufficient variance in observed data\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r = np.corrcoef(obs_vals, sim_vals)[0, 1]\n",
    "    obs_mean = np.mean(obs_vals)\n",
    "    sim_mean = np.mean(sim_vals)\n",
    "    \n",
    "    alpha = np.std(sim_vals) / np.std(obs_vals)\n",
    "    beta = sim_mean / obs_mean if obs_mean != 0 else 0\n",
    "    kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "    nse = 1 - np.sum((obs_vals - sim_vals)**2) / np.sum((obs_vals - obs_mean)**2)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(obs_vals, sim_vals))\n",
    "    bias = sim_mean - obs_mean\n",
    "    pbias = 100 * bias / obs_mean if obs_mean != 0 else np.inf\n",
    "    \n",
    "    print(f\"   ‚úÖ KGE: {kge:.3f}, NSE: {nse:.3f}, r: {r:.3f}, PBIAS: {pbias:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'gauge_id': gauge_id,\n",
    "        'gauge_name': gauge_name,\n",
    "        'segment_idx': segment_idx,\n",
    "        'distance_km': distance_km,\n",
    "        'aligned_data': aligned_data,\n",
    "        'period_start': common_start,\n",
    "        'period_end': common_end,\n",
    "        'kge': kge, 'nse': nse, 'r': r, 'alpha': alpha, 'beta': beta,\n",
    "        'rmse': rmse, 'bias': bias, 'pbias': pbias,\n",
    "        'obs_mean': obs_mean, 'sim_mean': sim_mean,\n",
    "        'segment_info': {col: closest_segment[col] for col in id_columns if col in closest_segment.index} if id_columns else {}\n",
    "    }\n",
    "\n",
    "def create_improved_hydrograph_comparison():\n",
    "    \"\"\"Create hydrograph comparison with proper segment extraction\"\"\"\n",
    "        \n",
    "    # Load network and simulation data\n",
    "    network_gdf, streamflow_sim, id_columns = load_and_analyze_network_data()\n",
    "    \n",
    "    if any(x is None for x in [network_gdf, streamflow_sim]):\n",
    "        print(\"‚ùå Failed to load network or simulation data\")\n",
    "        return None\n",
    "    \n",
    "    # Load gauge data\n",
    "    gauges_path = \"/Users/darrieythorsson/compHydro/data/lamah_ice/D_gauges/3_shapefiles/gauges.shp\"\n",
    "    gauges_gdf = gpd.read_file(gauges_path)\n",
    "    if gauges_gdf.crs and gauges_gdf.crs.to_string() != 'EPSG:4326':\n",
    "        gauges_gdf = gauges_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    obs_data_dir = Path(\"/Users/darrieythorsson/compHydro/data/lamah_ice/D_gauges/2_timeseries/daily_filtered\")\n",
    "    \n",
    "    print(f\"\\nüîç Processing {len(gauges_gdf)} gauges...\")\n",
    "    \n",
    "    # Process all gauges\n",
    "    validation_results = []\n",
    "    \n",
    "    for idx, gauge in gauges_gdf.iterrows():\n",
    "        result = process_gauge_with_improved_matching(\n",
    "            gauge, network_gdf, streamflow_sim, obs_data_dir, id_columns\n",
    "        )\n",
    "        if result:\n",
    "            validation_results.append(result)\n",
    "    \n",
    "    if len(validation_results) == 0:\n",
    "        print(\"‚ùå No successful validations\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìä Successfully processed {len(validation_results)} validations\")\n",
    "    \n",
    "    # Create summary table\n",
    "    results_df = pd.DataFrame([{k: v for k, v in result.items() if k != 'aligned_data'} \n",
    "                              for result in validation_results])\n",
    "    \n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"   Mean KGE: {results_df['kge'].mean():.3f} ¬± {results_df['kge'].std():.3f}\")\n",
    "    print(f\"   Mean NSE: {results_df['nse'].mean():.3f} ¬± {results_df['nse'].std():.3f}\")\n",
    "    print(f\"   Mean |PBIAS|: {np.abs(results_df['pbias']).mean():.1f}%\")\n",
    "    \n",
    "    # Select 6 best performing gauges for visualization\n",
    "    best_gauges = results_df.nlargest(6, 'kge')\n",
    "    \n",
    "    print(f\"\\nüèÜ Top 6 Performing Gauges (by KGE):\")\n",
    "    for _, gauge in best_gauges.iterrows():\n",
    "        print(f\"   {gauge['gauge_name']}: KGE={gauge['kge']:.3f}, NSE={gauge['nse']:.3f}\")\n",
    "    \n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, gauge_info) in enumerate(best_gauges.iterrows()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Find corresponding detailed result\n",
    "        detailed_result = None\n",
    "        for result in validation_results:\n",
    "            if result['gauge_id'] == gauge_info['gauge_id']:\n",
    "                detailed_result = result\n",
    "                break\n",
    "        \n",
    "        if detailed_result is None:\n",
    "            continue\n",
    "        \n",
    "        aligned_data = detailed_result['aligned_data']\n",
    "        \n",
    "        # Sample data if too dense\n",
    "        if len(aligned_data) > 1000:\n",
    "            step = len(aligned_data) // 1000\n",
    "            plot_data = aligned_data.iloc[::step]\n",
    "        else:\n",
    "            plot_data = aligned_data\n",
    "        \n",
    "        # Plot hydrographs\n",
    "        ax.plot(plot_data.index, plot_data['obs'], 'b-', linewidth=1.5, \n",
    "               label='Observed', alpha=0.8)\n",
    "        ax.plot(plot_data.index, plot_data['sim'], 'r-', linewidth=1.2, \n",
    "               label='Simulated', alpha=0.9)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_ylabel('Discharge (m¬≥/s)', fontweight='bold')\n",
    "        ax.set_title(f\"{detailed_result['gauge_name']}\\n\"\n",
    "                    f\"(ID: {detailed_result['gauge_id']}, \"\n",
    "                    f\"Segment: {detailed_result['segment_idx']}, \"\n",
    "                    f\"Dist: {detailed_result['distance_km']:.2f} km)\", \n",
    "                    fontweight='bold', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        # Performance metrics\n",
    "        metrics_text = (f\"KGE = {detailed_result['kge']:.3f}\\n\"\n",
    "                       f\"NSE = {detailed_result['nse']:.3f}\\n\"\n",
    "                       f\"r = {detailed_result['r']:.3f}\\n\"\n",
    "                       f\"PBIAS = {detailed_result['pbias']:.1f}%\")\n",
    "        \n",
    "        ax.text(0.02, 0.98, metrics_text, transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9),\n",
    "                fontsize=9, va='top', fontfamily='monospace')\n",
    "        \n",
    "        # Format dates\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "        ax.tick_params(axis='y', labelsize=9)\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for i in range(len(best_gauges), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Hydrograph Comparison: Best Performing Gauges\\n'\n",
    "                 'Iceland Domain', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Execute the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    validation_results = create_improved_hydrograph_comparison()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Setup: Categorize KGE and define color palette ===\n",
    "results_df = create_improved_performance_summary()\n",
    "\n",
    "# Categorize performance based on KGE\n",
    "def categorize_performance(kge):\n",
    "    if pd.isna(kge):\n",
    "        return 'Invalid'\n",
    "    elif kge >= 0.75:\n",
    "        return 'Excellent (‚â•0.75)'\n",
    "    elif kge >= 0.5:\n",
    "        return 'Good (0.5‚Äì0.75)'\n",
    "    elif kge >= 0.25:\n",
    "        return 'Fair (0.25‚Äì0.5)'\n",
    "    else:\n",
    "        return 'Poor (<0.25)'\n",
    "\n",
    "if 'results_df' in globals():\n",
    "    results_df['performance_cat'] = results_df['kge'].apply(categorize_performance)\n",
    "    \n",
    "# Define consistent color mapping\n",
    "perf_colors = {\n",
    "    'Excellent (‚â•0.75)': '#2E8B57',\n",
    "    'Good (0.5‚Äì0.75)': '#32CD32',\n",
    "    'Fair (0.25‚Äì0.5)': '#FFA500',\n",
    "    'Poor (<0.25)': '#DC143C',\n",
    "    'Invalid': '#808080'\n",
    "}\n",
    "\n",
    "\n",
    "# Set up figure and gridspec\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "gs = GridSpec(1, 2, figure=fig, wspace=0.3)\n",
    "\n",
    "# ---- Panel 1: Spatial KGE Map ----\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "network_gdf.plot(ax=ax1, color='lightblue', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Plot categorized points\n",
    "for category, color in perf_colors.items():\n",
    "    cat_data = results_df[results_df['performance_cat'] == category]\n",
    "    if not cat_data.empty:\n",
    "        ax1.scatter(cat_data['lon'], cat_data['lat'], c=color, s=80, alpha=0.9,\n",
    "                    label=f'{category} (n={len(cat_data)})',\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title('Spatial Distribution of KGE')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# ---- Panel 2: Obs vs Simulated ----\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "scatter = ax2.scatter(results_df['obs_mean'], results_df['sim_mean'],\n",
    "                      c=results_df['kge'], cmap='RdYlGn', s=80, alpha=0.8,\n",
    "                      edgecolors='black', linewidth=0.5, vmin=-0.5, vmax=1)\n",
    "\n",
    "max_discharge = max(results_df['obs_mean'].max(), results_df['sim_mean'].max())\n",
    "ax2.plot([0, max_discharge], [0, max_discharge], 'k--', label='1:1 Line')\n",
    "\n",
    "ax2.set_xlabel('Observed Mean Discharge (m¬≥/s)')\n",
    "ax2.set_ylabel('Simulated Mean Discharge (m¬≥/s)')\n",
    "ax2.set_title('Observed vs Simulated Discharge')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "if max_discharge > 100:\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('KGE Score')\n",
    "\n",
    "plt.suptitle('Map and Discharge Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Cell 2: Performance Metrics & ECDF ===\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "gs = GridSpec(1, 3, figure=fig, wspace=0.3)\n",
    "\n",
    "# ---- Panel 1: Performance Metrics Boxplots ----\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "metrics_data = {\n",
    "    'KGE': results_df['kge'].dropna(),\n",
    "    'NSE': results_df['nse'].dropna(), \n",
    "    'r': results_df['r'].dropna(),\n",
    "    '|PBIAS|': np.abs(results_df['pbias']).replace([np.inf], np.nan).dropna()\n",
    "}\n",
    "box_data = [d.values for d in metrics_data.values()]\n",
    "box_labels = list(metrics_data.keys())\n",
    "colors_box = ['skyblue', 'lightgreen', 'salmon', 'khaki']\n",
    "\n",
    "bp = ax1.boxplot(box_data, labels=box_labels, patch_artist=True, notch=True)\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "\n",
    "ax1.set_title('Performance Metrics Distribution')\n",
    "ax1.set_ylabel('Metric Value')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Panel 2: KGE Histogram ----\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "valid_kge = results_df['kge'].dropna()\n",
    "bins = np.linspace(-1, 1, 21)\n",
    "ax2.hist(valid_kge, bins=bins, color='lightsteelblue', edgecolor='black', alpha=0.8, density=True)\n",
    "ax2.axvline(valid_kge.mean(), color='red', linestyle='-', lw=2, label=f'Mean: {valid_kge.mean():.2f}')\n",
    "ax2.axvline(valid_kge.median(), color='orange', linestyle='--', lw=2, label=f'Median: {valid_kge.median():.2f}')\n",
    "ax2.set_title('KGE Score Distribution')\n",
    "ax2.set_xlabel('KGE Score')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Panel 3: KGE Empirical CDF ----\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "sorted_kge = np.sort(valid_kge)\n",
    "ecdf = np.arange(1, len(sorted_kge)+1) / len(sorted_kge)\n",
    "ax3.plot(sorted_kge, ecdf, marker='.', linestyle='-', color='blue')\n",
    "ax3.axvline(0.5, color='gray', linestyle='--', alpha=0.6)\n",
    "ax3.set_title('Empirical CDF of KGE')\n",
    "ax3.set_xlabel('KGE Score')\n",
    "ax3.set_xlim([0,1\n",
    "ax3.set_ylabel('Cumulative Probability')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution & ECDF of Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPROVED PERFORMANCE SUMMARY - PROPER SEGMENT EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Rectangle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Professional plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.edgecolor': '#333333',\n",
    "    'axes.linewidth': 0.8,\n",
    "    'xtick.color': '#333333',\n",
    "    'ytick.color': '#333333',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'legend.fontsize': 9,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "def load_network_and_simulation_data():\n",
    "    \"\"\"Load river network and simulation data with proper structure analysis\"\"\"\n",
    "    \n",
    "    print(\"üîç Loading Network and Simulation Data...\")\n",
    "    \n",
    "    # Load river network shapefile\n",
    "    network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "    network_files = list(network_path.glob('*.shp'))\n",
    "    \n",
    "    if not network_files:\n",
    "        print(\"‚ùå No river network shapefile found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    network_gdf = gpd.read_file(network_files[0])\n",
    "    if network_gdf.crs and network_gdf.crs.to_string() != 'EPSG:4326':\n",
    "        network_gdf = network_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Load simulation data\n",
    "    routing_dir = project_dir / 'simulations' / config_dict['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "    routing_files = list(routing_dir.glob('*.nc'))\n",
    "    \n",
    "    if not routing_files:\n",
    "        print(\"‚ùå No simulation files found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    sim_ds = xr.open_dataset(routing_files[0])\n",
    "    \n",
    "    # Find streamflow variable\n",
    "    possible_vars = ['IRFroutedRunoff', 'streamflow', 'discharge', 'runoff', 'q']\n",
    "    streamflow_var = None\n",
    "    for var in possible_vars:\n",
    "        if var in sim_ds.variables:\n",
    "            streamflow_var = var\n",
    "            break\n",
    "    \n",
    "    if streamflow_var is None:\n",
    "        print(\"‚ùå No streamflow variable found\")\n",
    "        return None, None, None\n",
    "    \n",
    "    streamflow_sim = sim_ds[streamflow_var]\n",
    "    \n",
    "    print(f\"‚úÖ Network: {len(network_gdf)} segments\")\n",
    "    print(f\"‚úÖ Simulation: {streamflow_sim.sizes.get('seg', 0)} segments, {streamflow_var}\")\n",
    "    \n",
    "    return network_gdf, streamflow_sim, streamflow_var\n",
    "\n",
    "def find_closest_segment(gauge_point, network_gdf, max_distance_km=5.0):\n",
    "    \"\"\"Find closest river segment to gauge with distance validation\"\"\"\n",
    "    \n",
    "    distances = network_gdf.geometry.distance(gauge_point)\n",
    "    closest_idx = distances.idxmin()\n",
    "    closest_distance_deg = distances.min()\n",
    "    \n",
    "    # Convert to km (approximate)\n",
    "    distance_km = closest_distance_deg * 111\n",
    "    \n",
    "    if distance_km > max_distance_km:\n",
    "        return None, None, distance_km\n",
    "    \n",
    "    return closest_idx, distance_km, network_gdf.loc[closest_idx]\n",
    "\n",
    "def extract_and_validate_segment_flow(streamflow_sim, segment_idx):\n",
    "    \"\"\"Extract segment flow with validation\"\"\"\n",
    "    \n",
    "    try:\n",
    "        max_seg = streamflow_sim.sizes.get('seg', 0)\n",
    "        if segment_idx >= max_seg or segment_idx < 0:\n",
    "            return None, f\"Index {segment_idx} out of range [0, {max_seg-1}]\"\n",
    "        \n",
    "        sim_data = streamflow_sim.isel(seg=segment_idx)\n",
    "        sim_ts = sim_data.to_pandas()\n",
    "        sim_ts.index = pd.to_datetime(sim_ts.index).normalize()\n",
    "        \n",
    "        # Data quality checks\n",
    "        valid_data = sim_ts.dropna()\n",
    "        if len(valid_data) == 0:\n",
    "            return None, \"No valid data\"\n",
    "        \n",
    "        if valid_data.max() < 0.001:\n",
    "            return None, f\"Suspiciously small values (max: {valid_data.max():.6f})\"\n",
    "        \n",
    "        if valid_data.std() == 0:\n",
    "            return None, \"Zero variance in simulated data\"\n",
    "        \n",
    "        return sim_ts, \"Success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {str(e)}\"\n",
    "\n",
    "def process_gauge_for_summary(gauge, network_gdf, streamflow_sim, obs_data_dir):\n",
    "    \"\"\"Process single gauge with improved segment matching for summary statistics\"\"\"\n",
    "    \n",
    "    gauge_id = gauge['id']\n",
    "    gauge_name = gauge['name']\n",
    "    gauge_point = gauge.geometry\n",
    "    \n",
    "    # Find closest river segment\n",
    "    segment_idx, distance_km, segment_info = find_closest_segment(gauge_point, network_gdf)\n",
    "    \n",
    "    if segment_idx is None:\n",
    "        return None, 'distant_segment'\n",
    "    \n",
    "    # Load observed data\n",
    "    obs_file = obs_data_dir / f\"ID_{gauge_id}.csv\"\n",
    "    if not obs_file.exists():\n",
    "        return None, 'no_obs_file'\n",
    "    \n",
    "    try:\n",
    "        # Parse observed data\n",
    "        for sep in [';', ',', '\\t']:\n",
    "            try:\n",
    "                obs_df = pd.read_csv(obs_file, sep=sep)\n",
    "                if obs_df.shape[1] > 1:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if 'YYYY' not in obs_df.columns or 'MM' not in obs_df.columns or 'DD' not in obs_df.columns:\n",
    "            return None, 'bad_obs_format'\n",
    "        \n",
    "        obs_df['date'] = pd.to_datetime({\n",
    "            'year': obs_df['YYYY'],\n",
    "            'month': obs_df['MM'], \n",
    "            'day': obs_df['DD']\n",
    "        }, errors='coerce')\n",
    "        \n",
    "        # Find discharge column\n",
    "        discharge_col = None\n",
    "        for col in obs_df.columns:\n",
    "            if any(word in col.lower() for word in ['qobs', 'discharge', 'flow', 'q_']):\n",
    "                discharge_col = col\n",
    "                break\n",
    "        \n",
    "        if discharge_col is None:\n",
    "            return None, 'no_discharge_col'\n",
    "        \n",
    "        obs_df.set_index('date', inplace=True)\n",
    "        obs_discharge = pd.to_numeric(obs_df[discharge_col], errors='coerce').dropna()\n",
    "        obs_discharge = obs_discharge[(obs_discharge > 0) & (obs_discharge < 1e6)]\n",
    "        \n",
    "        if len(obs_discharge) < 100:\n",
    "            return None, 'insufficient_obs'\n",
    "        \n",
    "    except Exception:\n",
    "        return None, 'obs_processing_error'\n",
    "    \n",
    "    # Extract simulated data\n",
    "    sim_discharge, status = extract_and_validate_segment_flow(streamflow_sim, segment_idx)\n",
    "    \n",
    "    if sim_discharge is None:\n",
    "        return None, 'sim_extraction_failed'\n",
    "    \n",
    "    # Find overlap period\n",
    "    common_start = max(obs_discharge.index.min(), sim_discharge.index.min())\n",
    "    common_end = min(obs_discharge.index.max(), sim_discharge.index.max())\n",
    "    \n",
    "    if common_start >= common_end:\n",
    "        return None, 'no_time_overlap'\n",
    "    \n",
    "    # Align data\n",
    "    obs_common = obs_discharge.loc[common_start:common_end]\n",
    "    sim_common = sim_discharge.loc[common_start:common_end]\n",
    "    aligned_data = pd.DataFrame({'obs': obs_common, 'sim': sim_common}).dropna()\n",
    "    \n",
    "    if len(aligned_data) < 100:\n",
    "        return None, 'insufficient_aligned'\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    obs_vals = aligned_data['obs'].values\n",
    "    sim_vals = aligned_data['sim'].values\n",
    "    \n",
    "    obs_mean = np.mean(obs_vals)\n",
    "    sim_mean = np.mean(sim_vals)\n",
    "    \n",
    "    if np.std(obs_vals) == 0:\n",
    "        return None, 'zero_obs_variance'\n",
    "    \n",
    "    # Metrics calculation\n",
    "    r = np.corrcoef(obs_vals, sim_vals)[0, 1]\n",
    "    alpha = np.std(sim_vals) / np.std(obs_vals)\n",
    "    beta = sim_mean / obs_mean if obs_mean != 0 else 0\n",
    "    \n",
    "    kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "    nse = 1 - np.sum((obs_vals - sim_vals)**2) / np.sum((obs_vals - obs_mean)**2)\n",
    "    rmse = np.sqrt(mean_squared_error(obs_vals, sim_vals))\n",
    "    bias = sim_mean - obs_mean\n",
    "    pbias = 100 * bias / obs_mean if obs_mean != 0 else np.inf\n",
    "    \n",
    "    return {\n",
    "        'gauge_id': gauge_id,\n",
    "        'gauge_name': gauge_name,\n",
    "        'segment_idx': segment_idx,\n",
    "        'distance_km': distance_km,\n",
    "        'lat': gauge.geometry.y,\n",
    "        'lon': gauge.geometry.x,\n",
    "        'n_points': len(aligned_data),\n",
    "        'period_days': (common_end - common_start).days,\n",
    "        'obs_mean': obs_mean,\n",
    "        'sim_mean': sim_mean,\n",
    "        'kge': kge, 'nse': nse, 'r': r, 'alpha': alpha, 'beta': beta,\n",
    "        'rmse': rmse, 'bias': bias, 'pbias': pbias\n",
    "    }, 'success'\n",
    "\n",
    "def process_all_validations_for_summary():\n",
    "    \"\"\"Process all gauges and create comprehensive summary\"\"\"\n",
    "    \n",
    "    print(\"üìä Processing All Validations for Summary...\")\n",
    "    \n",
    "    # Load data\n",
    "    network_gdf, streamflow_sim, streamflow_var = load_network_and_simulation_data()\n",
    "    if any(x is None for x in [network_gdf, streamflow_sim]):\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load gauges\n",
    "    gauges_path = \"/Users/darrieythorsson/compHydro/data/lamah_ice/D_gauges/3_shapefiles/gauges.shp\"\n",
    "    gauges_gdf = gpd.read_file(gauges_path)\n",
    "    if gauges_gdf.crs and gauges_gdf.crs.to_string() != 'EPSG:4326':\n",
    "        gauges_gdf = gauges_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    obs_data_dir = Path(\"/Users/darrieythorsson/compHydro/data/lamah_ice/D_gauges/2_timeseries/daily_filtered\")\n",
    "    \n",
    "    print(f\"üéØ Processing {len(gauges_gdf)} gauges...\")\n",
    "    \n",
    "    # Process all gauges\n",
    "    validation_results = []\n",
    "    failure_counts = {}\n",
    "    \n",
    "    for idx, gauge in gauges_gdf.iterrows():\n",
    "        result, status = process_gauge_for_summary(gauge, network_gdf, streamflow_sim, obs_data_dir)\n",
    "        \n",
    "        if result is not None:\n",
    "            validation_results.append(result)\n",
    "        else:\n",
    "            failure_counts[status] = failure_counts.get(status, 0) + 1\n",
    "    \n",
    "    print(f\"‚úÖ Successful validations: {len(validation_results)}\")\n",
    "    print(f\"‚ùå Failure breakdown: {dict(failure_counts)}\")\n",
    "    \n",
    "    return validation_results, failure_counts, network_gdf\n",
    "\n",
    "def create_comprehensive_performance_summary(validation_results, failure_counts, network_gdf):\n",
    "    \"\"\"Create comprehensive performance summary visualization\"\"\"\n",
    "    \n",
    "    if len(validation_results) == 0:\n",
    "        print(\"‚ùå No validation results for summary\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(validation_results)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = GridSpec(4, 4, figure=fig, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ROW 1: SPATIAL PERFORMANCE ANALYSIS\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Panel 1: Spatial KGE Distribution\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    # Plot Iceland outline\n",
    "    if network_gdf is not None:\n",
    "        network_gdf.plot(ax=ax1, color='lightblue', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Define performance categories\n",
    "    def categorize_performance(kge):\n",
    "        if pd.isna(kge):\n",
    "            return 'Invalid'\n",
    "        elif kge >= 0.75:\n",
    "            return 'Excellent (‚â•0.75)'\n",
    "        elif kge >= 0.5:\n",
    "            return 'Good (0.5-0.75)'\n",
    "        elif kge >= 0.25:\n",
    "            return 'Fair (0.25-0.5)'\n",
    "        else:\n",
    "            return 'Poor (<0.25)'\n",
    "    \n",
    "    results_df['performance_cat'] = results_df['kge'].apply(categorize_performance)\n",
    "    \n",
    "    # Color scheme\n",
    "    perf_colors = {\n",
    "        'Excellent (‚â•0.75)': '#2E8B57',\n",
    "        'Good (0.5-0.75)': '#32CD32',\n",
    "        'Fair (0.25-0.5)': '#FFA500',\n",
    "        'Poor (<0.25)': '#DC143C',\n",
    "        'Invalid': '#808080'\n",
    "    }\n",
    "    \n",
    "    # Plot by performance category\n",
    "    for category, color in perf_colors.items():\n",
    "        cat_data = results_df[results_df['performance_cat'] == category]\n",
    "        if len(cat_data) > 0:\n",
    "            ax1.scatter(cat_data['lon'], cat_data['lat'], c=color, s=100, alpha=0.8,\n",
    "                       label=f'{category} (n={len(cat_data)})', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude', fontweight='bold')\n",
    "    ax1.set_ylabel('Latitude', fontweight='bold') \n",
    "    ax1.set_title('Spatial Distribution of Model Performance (KGE)', fontweight='bold', fontsize=13)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Panel 2: Performance vs Distance\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    scatter = ax2.scatter(results_df['distance_km'], results_df['kge'], \n",
    "                         c=results_df['obs_mean'], s=60, alpha=0.7, \n",
    "                         cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Distance to River (km)', fontweight='bold')\n",
    "    ax2.set_ylabel('KGE Score', fontweight='bold')\n",
    "    ax2.set_title('Performance vs Gauge-Segment Distance', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add performance thresholds\n",
    "    for thresh in [0.25, 0.5, 0.75]:\n",
    "        ax2.axhline(thresh, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Obs. Mean Discharge (m¬≥/s)', fontweight='bold')\n",
    "    \n",
    "    # Panel 3: KGE Distribution\n",
    "    ax3 = fig.add_subplot(gs[0, 3])\n",
    "    \n",
    "    valid_kge = results_df['kge'].dropna()\n",
    "    bins = np.linspace(-1, 1, 21)\n",
    "    \n",
    "    ax3.hist(valid_kge, bins=bins, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_kge = valid_kge.mean()\n",
    "    median_kge = valid_kge.median()\n",
    "    ax3.axvline(mean_kge, color='red', linestyle='-', linewidth=2, label=f'Mean: {mean_kge:.3f}')\n",
    "    ax3.axvline(median_kge, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_kge:.3f}')\n",
    "    \n",
    "    ax3.set_xlabel('KGE Score', fontweight='bold')\n",
    "    ax3.set_ylabel('Density', fontweight='bold')\n",
    "    ax3.set_title('KGE Distribution', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ROW 2: PERFORMANCE METRICS ANALYSIS\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Panel 4: Multi-metric Comparison\n",
    "    ax4 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    metrics_data = {\n",
    "        'KGE': results_df['kge'].dropna(),\n",
    "        'NSE': results_df['nse'].dropna(), \n",
    "        'Correlation (r)': results_df['r'].dropna(),\n",
    "        '|PBIAS|': np.abs(results_df['pbias']).replace([np.inf], np.nan).dropna()\n",
    "    }\n",
    "    \n",
    "    box_data = [data.values for data in metrics_data.values()]\n",
    "    box_labels = list(metrics_data.keys())\n",
    "    \n",
    "    bp = ax4.boxplot(box_data, labels=box_labels, patch_artist=True, notch=True, showfliers=True)\n",
    "    \n",
    "    colors_box = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "    for patch, color in zip(bp['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.8)\n",
    "    \n",
    "    ax4.set_ylabel('Score / Value', fontweight='bold')\n",
    "    ax4.set_title('Performance Metrics Distribution', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Panel 5: Observed vs Simulated Scatter\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    scatter = ax5.scatter(results_df['obs_mean'], results_df['sim_mean'], \n",
    "                         c=results_df['kge'], cmap='RdYlGn', s=80, alpha=0.7,\n",
    "                         edgecolors='black', linewidth=0.5, vmin=-0.5, vmax=1)\n",
    "    \n",
    "    # 1:1 line\n",
    "    max_discharge = max(results_df['obs_mean'].max(), results_df['sim_mean'].max())\n",
    "    ax5.plot([0, max_discharge], [0, max_discharge], 'k--', alpha=0.5, label='1:1 Line')\n",
    "    \n",
    "    ax5.set_xlabel('Observed Mean Discharge (m¬≥/s)', fontweight='bold')\n",
    "    ax5.set_ylabel('Simulated Mean Discharge (m¬≥/s)', fontweight='bold')\n",
    "    ax5.set_title('Observed vs Simulated Discharge', fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Use log scale if large range\n",
    "    if max_discharge > 100:\n",
    "        ax5.set_xscale('log')\n",
    "        ax5.set_yscale('log')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax5)\n",
    "    cbar.set_label('KGE Score', fontweight='bold')\n",
    "    \n",
    "    # Panel 6: Performance Categories Pie Chart\n",
    "    ax6 = fig.add_subplot(gs[1, 3])\n",
    "    \n",
    "    perf_counts = results_df['performance_cat'].value_counts()\n",
    "    colors_pie = [perf_colors[cat] for cat in perf_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = ax6.pie(perf_counts.values, labels=perf_counts.index, \n",
    "                                      autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax6.set_title('Performance Categories', fontweight='bold')\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ROW 3: DETAILED STATISTICS AND VALIDATION SUCCESS\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Panel 7: Comprehensive Statistics Table\n",
    "    ax7 = fig.add_subplot(gs[2, :2])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_rows = []\n",
    "    for metric_name, metric_data in metrics_data.items():\n",
    "        if len(metric_data) > 0:\n",
    "            stats_rows.append([\n",
    "                metric_name,\n",
    "                f\"{metric_data.mean():.3f}\",\n",
    "                f\"{metric_data.median():.3f}\",\n",
    "                f\"{metric_data.std():.3f}\",\n",
    "                f\"{metric_data.min():.3f}\",\n",
    "                f\"{metric_data.max():.3f}\",\n",
    "                f\"{len(metric_data)}\"\n",
    "            ])\n",
    "    \n",
    "    # Add performance category statistics\n",
    "    for cat, count in perf_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        stats_rows.append([f\"Count {cat}\", f\"{count}\", f\"{pct:.1f}%\", \"\", \"\", \"\", \"\"])\n",
    "    \n",
    "    table = ax7.table(\n",
    "        cellText=stats_rows,\n",
    "        colLabels=['Metric', 'Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Count'],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        bbox=[0, 0, 1, 1]\n",
    "    )\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.8)\n",
    "    \n",
    "    # Style table\n",
    "    for i in range(len(stats_rows) + 1):\n",
    "        for j in range(7):\n",
    "            if i == 0:  # Header\n",
    "                table[(i, j)].set_facecolor('#4472C4')\n",
    "                table[(i, j)].set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('#F8F9FA')\n",
    "    \n",
    "    ax7.set_title('Detailed Performance Statistics', fontweight='bold', fontsize=13, pad=20)\n",
    "    \n",
    "    # Panel 8: Validation Success Analysis\n",
    "    ax8 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    # Create validation success breakdown\n",
    "    total_gauges = len(results_df) + sum(failure_counts.values())\n",
    "    success_rate = len(results_df) / total_gauges * 100\n",
    "    \n",
    "    # Failure reasons pie chart\n",
    "    if failure_counts:\n",
    "        failure_labels = [reason.replace('_', ' ').title() for reason in failure_counts.keys()]\n",
    "        failure_values = list(failure_counts.values())\n",
    "        \n",
    "        # Add success to the pie\n",
    "        failure_labels.insert(0, f'Successful\\n({len(results_df)} gauges)')\n",
    "        failure_values.insert(0, len(results_df))\n",
    "        \n",
    "        colors_validation = ['green'] + list(plt.cm.Set3(np.linspace(0, 1, len(failure_counts))))\n",
    "\n",
    "        \n",
    "        wedges, texts, autotexts = ax8.pie(failure_values, labels=failure_labels, \n",
    "                                          autopct='%1.1f%%', colors=colors_validation, \n",
    "                                          startangle=90)\n",
    "        \n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "    \n",
    "    ax8.set_title(f'Validation Success Analysis\\n(Success Rate: {success_rate:.1f}%)', \n",
    "                 fontweight='bold')\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ROW 4: SUMMARY INSIGHTS\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Panel 9: Key Insights Text\n",
    "    ax9 = fig.add_subplot(gs[3, :])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    # Calculate key insights\n",
    "    excellent_pct = np.mean(valid_kge >= 0.75) * 100\n",
    "    good_plus_pct = np.mean(valid_kge >= 0.5) * 100\n",
    "    mean_distance = results_df['distance_km'].mean()\n",
    "    median_n_points = results_df['n_points'].median()\n",
    "    \n",
    "    insights_text = f\"\"\"\n",
    "    KEY PERFORMANCE INSIGHTS:\n",
    "    \n",
    "    üéØ OVERALL PERFORMANCE:\n",
    "    ‚Ä¢ {len(results_df)} successful validations out of {total_gauges} total gauges ({success_rate:.1f}% success rate)\n",
    "    ‚Ä¢ Mean KGE: {mean_kge:.3f} ¬± {valid_kge.std():.3f} (Median: {median_kge:.3f})\n",
    "    ‚Ä¢ Excellent performance (KGE ‚â• 0.75): {excellent_pct:.1f}% of gauges\n",
    "    ‚Ä¢ Good+ performance (KGE ‚â• 0.50): {good_plus_pct:.1f}% of gauges\n",
    "    \n",
    "    üó∫Ô∏è SPATIAL CHARACTERISTICS:\n",
    "    ‚Ä¢ Average gauge-to-segment distance: {mean_distance:.2f} km\n",
    "    ‚Ä¢ Median validation period: {median_n_points:.0f} data points\n",
    "    ‚Ä¢ Geographic coverage: {results_df['lat'].max()-results_df['lat'].min():.2f}¬∞ latitude span\n",
    "    \n",
    "    üìä DATA QUALITY:\n",
    "    ‚Ä¢ Mean observed discharge range: {results_df['obs_mean'].min():.1f} - {results_df['obs_mean'].max():.1f} m¬≥/s\n",
    "    ‚Ä¢ Correlation (r) mean: {results_df['r'].mean():.3f} ¬± {results_df['r'].std():.3f}\n",
    "    ‚Ä¢ Absolute PBIAS mean: {np.abs(results_df['pbias']).replace([np.inf], np.nan).mean():.1f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    ax9.text(0.05, 0.95, insights_text, transform=ax9.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.1))\n",
    "    \n",
    "    # Main title\n",
    "    plt.suptitle('Comprehensive Streamflow Validation Performance Summary\\n'\n",
    "                 f'Iceland Domain - Improved Segment Extraction Method', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def create_improved_performance_summary():\n",
    "    \"\"\"Main function to create improved performance summary\"\"\"\n",
    "    \n",
    "    print(\"üìä Improved Performance Summary Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process all validations\n",
    "    validation_results, failure_counts, network_gdf = process_all_validations_for_summary()\n",
    "    \n",
    "    if validation_results is None or len(validation_results) == 0:\n",
    "        print(\"‚ùå No validation results available\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    results_df = create_comprehensive_performance_summary(validation_results, failure_counts, network_gdf)\n",
    "    \n",
    "    # Print summary to console\n",
    "    if results_df is not None:\n",
    "        valid_kge = results_df['kge'].dropna()\n",
    "        total_gauges = len(results_df) + sum(failure_counts.values())\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL SUMMARY:\")\n",
    "        print(f\"   Success rate: {len(results_df)}/{total_gauges} ({len(results_df)/total_gauges*100:.1f}%)\")\n",
    "        print(f\"   Mean KGE: {valid_kge.mean():.3f} ¬± {valid_kge.std():.3f}\")\n",
    "        print(f\"   Excellent (‚â•0.75): {np.sum(valid_kge >= 0.75)} ({np.mean(valid_kge >= 0.75)*100:.1f}%)\")\n",
    "        print(f\"   Good+ (‚â•0.50): {np.sum(valid_kge >= 0.50)} ({np.mean(valid_kge >= 0.50)*100:.1f}%)\")\n",
    "        print(f\"   Mean distance: {results_df['distance_km'].mean():.2f} km\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Execute the analysis  \n",
    "if __name__ == \"__main__\":\n",
    "    results_summary = create_improved_performance_summary()\n",
    "    \n",
    "    if results_summary is not None:\n",
    "        print(f\"\\n‚úÖ Improved performance summary complete!\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Improved performance summary failed!\")\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summary: Regional Domain Modeling\n",
    "This tutorial successfully demonstrated the transition from watershed-scale to regional-scale hydrological modeling by implementing comprehensive regional domain analysis using Iceland as an exemplary case study. Through the Iceland regional domain, we illustrated how the same standardized CONFLUENCE workflow framework seamlessly scales from single-watershed applications to multi-watershed regional systems while capturing the complete hydrological picture of entire geographic regions, representing a paradigm shift from watershed-centric to region-centric hydrological analysis.\n",
    "\n",
    "## Key Methodological Achievements\n",
    "The tutorial established regional multi-watershed modeling capabilities through systematic identification and delineation of all independent drainage systems within geographic boundaries rather than topographic watershed limits. Coastal watershed integration was demonstrated through explicit inclusion of ocean-draining basins that traditional watershed modeling approaches often overlook, ensuring complete regional water accounting. Bounding box delineation methodology was successfully implemented to define regions using administrative or geographic boundaries rather than pour point specifications, enabling comprehensive coverage of entire administrative units or natural regions.\n",
    "\n",
    "## Scientific Process Understanding\n",
    "The evaluation demonstrated CONFLUENCE's capability to represent multiple independent drainage systems within a unified modeling framework, enabling simultaneous analysis of diverse hydrological regimes without artificial connectivity assumptions. Regional water resources assessment was achieved through integrated modeling of coastal watersheds, highland systems, and diverse topographic settings that capture the complete spectrum of hydrological conditions within national boundaries. Multi-outlet discharge analysis was established through systematic handling of numerous independent coastal discharge points, providing comprehensive understanding of regional water export patterns.\n",
    "\n",
    "## Framework Scalability Validation\n",
    "This tutorial confirmed CONFLUENCE's seamless regional scaling by successfully applying identical workflow principles from watershed-specific applications to comprehensive regional domains without requiring fundamental architectural modifications. The model-agnostic preprocessing approach proved equally effective for regional-scale spatial processing and multi-watershed system configuration, demonstrating robust design principles across dramatically different spatial paradigms. Computational efficiency optimization was maintained through intelligent regional processing strategies that balance comprehensive spatial coverage with practical computational requirements for regional water management applications.\n",
    "\n",
    "This advancement from watershed to regional modeling establishes essential capabilities for national water resources assessment, climate impact evaluation across administrative boundaries, and regional policy applications that require understanding of hydrological processes across entire geographic regions rather than individual watershed systems.\n",
    "\n",
    "### Next Focus: Continental Scale Domain Modelling \n",
    "\n",
    "**Ready to explore continential domain scale simulations?** ‚Üí **[Tutorial 03b: Continental Domain Scale - North America](./03b_domain_continental.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scienv)",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
