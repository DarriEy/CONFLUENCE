{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 1: Point-Scale Workflow (SNOTEL Example)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### The Scientific Importance of Point-Scale Modeling\n",
    "\n",
    "Point-scale modeling represents the fundamental building block of distributed hydrological modeling, where vertical energy and water balance processes are simulated at a single location without the complexities of lateral flow routing. This approach is scientifically valuable for several reasons:\n",
    "\n",
    "1. **Process Understanding**: Point-scale simulations isolate vertical processes (precipitation, evapotranspiration, snowmelt, infiltration, and soil moisture dynamics), allowing researchers to evaluate model physics without confounding effects from spatial heterogeneity and routing processes.\n",
    "\n",
    "2. **Model Validation**: Single-point simulations provide controlled conditions for testing model assumptions and parameter sensitivity, serving as a prerequisite for successful distributed modeling applications.\n",
    "\n",
    "3. **Observational Constraints**: Point-scale modeling leverages high-quality, long-term observational datasets to constrain model parameters and validate process representations before scaling to larger, distributed domains.\n",
    "\n",
    "### Case Study: Paradise SNOTEL Station\n",
    "\n",
    "This tutorial demonstrates  point-scale simulations in CONFLUENCE using the Paradise SNOTEL station (ID: 602) in Washington State. Located at 1,630 m elevation in the Cascade Range, this site represents a transitional snow climate with observations of both Snow Water Equivilalent (SWE) and Soil Moisture (SM) at four depths.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. **Understand CONFLUENCE architecture**: Learn how the modular framework manages complex hydrological modeling workflows\n",
    "2. **Configure point-scale simulations**: Set up CONFLUENCE for single-point SUMMA simulations with minimal spatial complexity\n",
    "3. **Evaluate model performance**: Compare simulated and observed snow water equivalent and soil moisture using quantitative metrics\n",
    "4. **Interpret results scientifically**: Analyze model-observation discrepancies in the context of process representation and parameter uncertainty\n",
    "5. **Assess workflow efficiency**: Experience CONFLUENCE's automated workflow management and reproducible modeling practices\n",
    "\n",
    "This foundation in point-scale modeling prepares you for more complex distributed modeling applications while building confidence in model physics and parameter estimation approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Point-Scale Configuration\n",
    "\n",
    "We'll use the default point configuration template in 0_config_files/0_config_point_template.yaml as our baseline. \n",
    "\n",
    "We copy the template to a new location and update the key configuration settings for our specific experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data')  # â† User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Update name and experiment id\n",
    "config_dict['DOMAIN_NAME'] = 'paradise'\n",
    "config_dict['EXPERIMENT_ID'] = 'point_scale_tutorial'\n",
    "\n",
    "# Save point-scale configuration to temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE with the new configuration\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Print summary of the key settings\n",
    "print(\"=== Point-Scale Configuration ===\")\n",
    "print(f\"Domain Name: {config_dict['DOMAIN_NAME']}\")\n",
    "print(f\"Spatial Mode: {config_dict['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"Location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"Period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"Forcing Data: {config_dict['FORCING_DATASET']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure\n",
    "\n",
    "1. We setup the basic directory structure under the root data directory specified in the CONFLUENCE_DATA_DIR setting in the configuration file\n",
    "2. We create a point shapefile from the coordinates given in POUR_POINT_COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "print(\"Creating point-scale project structure...\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point (in this case, our SNOTEL location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  ðŸ“ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition - Data acquisition \n",
    "Now we will acquire the geospatial attributes we need to setup our model\n",
    "\n",
    "- Elevation\n",
    "- Land Cover\n",
    "- Soil Classifications\n",
    "\n",
    "We use the Model Agnostic Framework [gistool (Keshavarz et al., 2025)](https://github.com/CH-Earth/gistool) to subset the data based on the coordinates set in: BOUNDING_BOX_COORDS\n",
    "\n",
    "When SPATIAL_MODE is set to Point CONFLUENCE automatically updates the BOUNDING_BOX_COORDS to a square buffer that is by default 0.001 degrees, the point buffer distance setting can be set in the POINT_BUFFER_DISTANCE setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes for point location...\")\n",
    "\n",
    "print(f\"Minimal bounding box: {confluence.config.get('BOUNDING_BOX_COORDS')}\")\n",
    "\n",
    "#confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition - Domain creation\n",
    "\n",
    "Confluence creates the required shapefiles to pre-process and configure the models\n",
    "\n",
    "When run in point configuration a square polygon is produced in path/to/domain_dir/shapefiles/catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Geospatial Domain Definition and Analysis\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Analysis ===\")\n",
    "\n",
    "# Define domain\n",
    "print(\"\\nDefining minimal domain for point-scale simulation...\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "# Discretize domain (single HRU for point-scale)\n",
    "print(\"\\nCreating single HRU for point-scale simulation...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Agnostic Data Pre-Processing - Data Acquisition\n",
    "Next we need meteorological data to run our models and observations to compare them with. \n",
    "\n",
    "We use the Model Agnostic Framework [datatool (Keshavarz et al., 2025)](https://github.com/CH-Earth/datatool) to subset the forcing dataset defined in FORCING_DATASET: for the spatial bounding box for the period configured between EXPERIMENT_TIME_START and EXPERIMENT_TIME_END.\n",
    "\n",
    "A temperature lapse rate can be applied to the forcing data by setting APPLY_LAPSE_RATE: True, the lapse rate can be set with the LAPSE_RATE which defaults to 0.0065 K m-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Agnostic Data Pre-Processing\n",
    "print(\"=== Step 3: Model Agnostic Data Pre-Processing ===\")\n",
    "\n",
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data for point location...\")\n",
    "print(f\"Dataset: {confluence.config['FORCING_DATASET']}\")\n",
    "#confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAF has the snotel dataset in storage and confluence can aquire data based on station id by setting DOWNLOAD_SNOTEL: 'true' and the appropriate setting for SNOTEL_STATION, which in our case is '602'. The path to the SNOTEL data can be set manually with the SNOTEL_PATH configuration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process observed data \n",
    "print(\"Processing observed data...\")\n",
    "confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Agnostic Data Pre-Processing - Remapping and zonal statistics\n",
    "\n",
    "Remapping of the forcing data and zonal statistics calcuations for the geospatial attributes is preformed in one model agnostic pre-processing step. \n",
    "\n",
    "The forcing data are remapped onto the defined hydrofabric using [EASYMORE (Gherari et al., 2023)](https://www.sciencedirect.com/science/article/pii/S2352711023002431)\n",
    "Zonal statistics are run using [rasterstats]('https://pypi.org/project/rasterstats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nModel-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Specific - Pre Processing \n",
    "\n",
    "Using the model agnostic output the model specific input files are prepared in one model specific preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Specific Processing and Initialization\n",
    "print(\"=== Step 4: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nModel-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Specific - Initialisation\n",
    "\n",
    "Once the model input files have been created the models are instantiated with their default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Result visualisation\n",
    "\n",
    "Now let's look how our simulations turned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Visualize Observed vs. Simulated SWE\n",
    "print(\"=== Step 11: Comparing Observed vs. Simulated SWE ===\")\n",
    "\n",
    "# 1. Load the observed SWE data\n",
    "obs_swe_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"observations\" / \"snow\" / \"snotel\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "# Check if observed data exists\n",
    "if not obs_swe_path.exists():\n",
    "    print(f\"Warning: Observed SWE data not found at {obs_swe_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    # Try to find data in parent directories\n",
    "    alt_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"**/observations/snow/swe/*_swe_processed.csv\"))\n",
    "    if alt_paths:\n",
    "        obs_swe_path = alt_paths[0]\n",
    "        print(f\"Found alternative SWE data at: {obs_swe_path}\")\n",
    "    else:\n",
    "        print(\"No observed SWE data found. Only simulated data will be displayed.\")\n",
    "\n",
    "# Load observed SWE data if available\n",
    "if obs_swe_path.exists():\n",
    "    print(f\"Loading observed SWE data from: {obs_swe_path}\")\n",
    "    obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'])\n",
    "    obs_swe.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Ensure the index is a proper DatetimeIndex\n",
    "    if not isinstance(obs_swe.index, pd.DatetimeIndex):\n",
    "        print(\"Converting index to DatetimeIndex...\")\n",
    "        # Try different date formats\n",
    "        try:\n",
    "            obs_swe.index = pd.to_datetime(obs_swe.index, format='%d/%m/%Y')\n",
    "        except:\n",
    "            try:\n",
    "                obs_swe.index = pd.to_datetime(obs_swe.index, format='%m/%d/%Y')\n",
    "            except:\n",
    "                obs_swe.index = pd.to_datetime(obs_swe.index, infer_datetime_format=True)\n",
    "    \n",
    "    print(f\"Observed data period: {obs_swe.index.min()} to {obs_swe.index.max()}\")\n",
    "    print(f\"Observed SWE range: {obs_swe['SWE'].min():.2f} to {obs_swe['SWE'].max():.2f} mm\")\n",
    "else:\n",
    "    obs_swe = None\n",
    "\n",
    "# 2. Load the simulated SWE data\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*day.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Skip the first year as spinup\n",
    "start_year = ds.time.dt.year.min().values + 1\n",
    "spinup_end = f\"{start_year}-01-01\"\n",
    "print(f\"Skipping spinup period before: {spinup_end}\")\n",
    "\n",
    "# Filter data after spinup\n",
    "time_mask = ds.time >= pd.to_datetime(spinup_end)\n",
    "ds_filtered = ds.isel(time=time_mask)\n",
    "\n",
    "# Extract scalarSWE and convert to DataFrame\n",
    "sim_swe = ds_filtered['scalarSWE'].to_dataframe().reset_index()\n",
    "# Assuming first HRU for point-scale simulation\n",
    "sim_swe = sim_swe[sim_swe['hru'] == 1][['time', 'scalarSWE']]\n",
    "sim_swe.columns = ['Date', 'SWE']\n",
    "sim_swe.set_index('Date', inplace=True)\n",
    "print(f\"Simulated data period (after spinup): {sim_swe.index.min()} to {sim_swe.index.max()}\")\n",
    "print(f\"Simulated SWE range: {sim_swe['SWE'].min():.2f} to {sim_swe['SWE'].max():.2f} mm\")\n",
    "\n",
    "# 3. Find common date range if observed data exists\n",
    "if obs_swe is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_swe = obs_swe.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    sim_swe = sim_swe.resample('D').mean()  # Daily mean if sub-daily sim data\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "    end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_swe.loc[start_date:end_date]\n",
    "    sim_period = sim_swe.loc[start_date:end_date]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(((obs_period['SWE'] - sim_period['SWE']) ** 2).mean())\n",
    "    bias = (sim_period['SWE'] - obs_period['SWE']).mean()\n",
    "    corr = obs_period['SWE'].corr(sim_period['SWE'])\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} mm\")\n",
    "    print(f\"  - Bias: {bias:.2f} mm\")\n",
    "    print(f\"  - Correlation: {corr:.2f}\")\n",
    "    \n",
    "    # 4. Visualize the comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot both time series\n",
    "    plt.plot(obs_period.index, obs_period['SWE'], 'o-', label='Observed SWE', color='black', alpha=0.7, markersize=4)\n",
    "    plt.plot(sim_period.index, sim_period['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "        \n",
    "    # Styling\n",
    "    plt.title(f\"SWE Comparison at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(obs_period['SWE'], sim_period['SWE'], color='blue', alpha=0.7)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    max_val = max(obs_period['SWE'].max(), sim_period['SWE'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    # Styling\n",
    "    plt.title(f\"Observed vs. Simulated SWE\", fontsize=14)\n",
    "    plt.xlabel('Observed SWE (mm)', fontsize=12)\n",
    "    plt.ylabel('Simulated SWE (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # If no observed data, just plot simulated\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sim_swe.index, sim_swe['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "    plt.title(f\"Simulated SWE at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nSWE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this SNOTEL station also has an ISMN soil moisture probe. Let's compare our simulations to the observed soil moisture over the depth profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Visualize Observed vs. Simulated Soil Moisture\n",
    "\n",
    "# 1. Load the observed SM data\n",
    "obs_sm_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"observations\" / \"soil_moisture\" / \"ismn\" / \"pre_processed\" / f\"{config_dict['DOMAIN_NAME']}_sm_processed.csv\"\n",
    "\n",
    "# Load observed SM data\n",
    "if obs_sm_path.exists():\n",
    "    print(f\"Loading observed SM data from: {obs_sm_path}\")\n",
    "    obs_sm = pd.read_csv(obs_sm_path, parse_dates=['timestamp'])\n",
    "    obs_sm.set_index('timestamp', inplace=True)\n",
    "    print(f\"Observed data period: {obs_sm.index.min()} to {obs_sm.index.max()}\")\n",
    "    \n",
    "    # Get observed depths from column names\n",
    "    obs_depths = [col for col in obs_sm.columns if col.startswith('sm_')]\n",
    "    print(f\"Available observed depths: {obs_depths}\")\n",
    "    \n",
    "    # Print ranges for each depth\n",
    "    for depth_col in obs_depths:\n",
    "        print(f\"Observed soil moisture range at {depth_col}: {obs_sm[depth_col].min():.3f} to {obs_sm[depth_col].max():.3f}\")\n",
    "else:\n",
    "    obs_sm = None\n",
    "    obs_depths = []\n",
    "\n",
    "# 2. Load the simulated SM data\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Get layer depths and soil moisture data\n",
    "layer_depths = ds['mLayerDepth'].isel(hru=0)  # First HRU\n",
    "soil_moisture = ds['mLayerVolFracLiq'].isel(hru=0)  # First HRU\n",
    "\n",
    "# Skip the first year as spinup\n",
    "start_year = ds.time.dt.year.min().values + 1\n",
    "spinup_end = f\"{start_year}-01-01\"\n",
    "print(f\"Skipping spinup period before: {spinup_end}\")\n",
    "\n",
    "# Filter data after spinup\n",
    "time_mask = ds.time >= pd.to_datetime(spinup_end)\n",
    "soil_moisture_filtered = soil_moisture.isel(time=time_mask)\n",
    "layer_depths_filtered = layer_depths.isel(time=time_mask)\n",
    "\n",
    "print(f\"Simulated data period (after spinup): {soil_moisture_filtered.time.min().values} to {soil_moisture_filtered.time.max().values}\")\n",
    "\n",
    "# 3. Find common date range if observed data exists\n",
    "if obs_sm is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_sm = obs_sm.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_sm.index.min(), pd.to_datetime(soil_moisture_filtered.time.min().values))\n",
    "    end_date = min(obs_sm.index.max(), pd.to_datetime(soil_moisture_filtered.time.max().values))\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter observed data to common period\n",
    "    obs_period = obs_sm.loc[start_date:end_date]\n",
    "    \n",
    "    # Filter simulated data to common period\n",
    "    sim_time_mask = (soil_moisture_filtered.time >= start_date) & (soil_moisture_filtered.time <= end_date)\n",
    "    sim_sm_common = soil_moisture_filtered.isel(time=sim_time_mask)\n",
    "    sim_depths_common = layer_depths_filtered.isel(time=sim_time_mask)\n",
    "    \n",
    "    # Convert simulated data to DataFrame for easier handling\n",
    "    sim_df = sim_sm_common.to_dataframe().reset_index()\n",
    "    depths_df = sim_depths_common.to_dataframe().reset_index()\n",
    "    \n",
    "    # Create figure with subplots for each observed depth\n",
    "    n_depths = len(obs_depths)\n",
    "    fig, axes = plt.subplots(n_depths, 1, figsize=(14, 4*n_depths))\n",
    "    \n",
    "    # If only one depth, make axes a list for consistency\n",
    "    if n_depths == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Define depth mapping (observed depth to simulated layer)\n",
    "    # Extract numeric depths from observed column names\n",
    "    obs_depth_values = []\n",
    "    for depth_col in obs_depths:\n",
    "        # Extract the first depth value from column name like 'sm_0.0508_0.0508'\n",
    "        depth_str = depth_col.split('_')[1]\n",
    "        obs_depth_values.append(float(depth_str))\n",
    "    \n",
    "    # For each observed depth, find the closest simulated layer\n",
    "    for i, (depth_col, obs_depth) in enumerate(zip(obs_depths, obs_depth_values)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Find the closest simulated layer depth\n",
    "        # Calculate mean layer depths over time to find the best match\n",
    "        mean_layer_depths = sim_depths_common.mean(dim='time')\n",
    "        \n",
    "        # Filter out missing values (negative values)\n",
    "        valid_layers = mean_layer_depths > 0\n",
    "        if valid_layers.sum() > 0:\n",
    "            valid_depths = mean_layer_depths.where(valid_layers)\n",
    "            # Find closest layer\n",
    "            depth_diff = np.abs(valid_depths - obs_depth)\n",
    "            closest_layer_idx = depth_diff.argmin().values\n",
    "            \n",
    "            # Extract simulated data for this layer\n",
    "            sim_layer_data = sim_sm_common.isel(midToto=closest_layer_idx)\n",
    "            \n",
    "            # Filter out missing values (negative values indicate missing data)\n",
    "            sim_layer_data = sim_layer_data.where(sim_layer_data > -100)\n",
    "            \n",
    "            # Plot observed data\n",
    "            ax.plot(obs_period.index, obs_period[depth_col], 'o-', \n",
    "                   label=f'Observed (depth: {obs_depth:.4f}m)', \n",
    "                   color='black', alpha=0.7, markersize=3)\n",
    "            \n",
    "            # Plot simulated data\n",
    "            ax.plot(sim_layer_data.time, sim_layer_data.values, '-', \n",
    "                   label=f'Simulated (layer {closest_layer_idx}, depth: {valid_depths[closest_layer_idx].values:.4f}m)', \n",
    "                   color='blue', linewidth=2)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            # Align time series\n",
    "            sim_resampled = sim_layer_data.resample(time='D').mean()\n",
    "            sim_interp = sim_resampled.interp(time=obs_period.index)\n",
    "            \n",
    "            # Remove NaN values for metrics calculation\n",
    "            valid_mask = ~(np.isnan(obs_period[depth_col]) | np.isnan(sim_interp.values))\n",
    "            if valid_mask.sum() > 0:\n",
    "                obs_valid = obs_period[depth_col][valid_mask]\n",
    "                sim_valid = sim_interp.values[valid_mask]\n",
    "                \n",
    "                rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                bias = (sim_valid - obs_valid).mean()\n",
    "                corr = np.corrcoef(obs_valid, sim_valid)[0, 1]\n",
    "                \n",
    "                # Add annotation with metrics\n",
    "                ax.text(0.02, 0.95, f\"RMSE: {rmse:.3f}\\nBias: {bias:.3f}\\nCorr: {corr:.3f}\", \n",
    "                       transform=ax.transAxes, fontsize=10, \n",
    "                       bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f\"Soil Moisture at {obs_depth:.4f}m depth\", fontsize=12)\n",
    "        ax.set_ylabel('Soil Moisture (mÂ³/mÂ³)', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        \n",
    "        # Only add x-label to bottom subplot\n",
    "        if i == n_depths - 1:\n",
    "            ax.set_xlabel('Date', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f\"Soil Moisture Comparison at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", \n",
    "                 fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create scatter plots for each depth\n",
    "    fig, axes = plt.subplots(1, n_depths, figsize=(5*n_depths, 4))\n",
    "    \n",
    "    # If only one depth, make axes a list for consistency\n",
    "    if n_depths == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (depth_col, obs_depth) in enumerate(zip(obs_depths, obs_depth_values)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Find the closest simulated layer (same as above)\n",
    "        mean_layer_depths = sim_depths_common.mean(dim='time')\n",
    "        valid_layers = mean_layer_depths > 0\n",
    "        if valid_layers.sum() > 0:\n",
    "            valid_depths = mean_layer_depths.where(valid_layers)\n",
    "            depth_diff = np.abs(valid_depths - obs_depth)\n",
    "            closest_layer_idx = depth_diff.argmin().values\n",
    "            \n",
    "            sim_layer_data = sim_sm_common.isel(midToto=closest_layer_idx)\n",
    "            sim_layer_data = sim_layer_data.where(sim_layer_data > -100)\n",
    "            \n",
    "            # Align time series for scatter plot\n",
    "            sim_resampled = sim_layer_data.resample(time='D').mean()\n",
    "            sim_interp = sim_resampled.interp(time=obs_period.index)\n",
    "            \n",
    "            # Remove NaN values\n",
    "            valid_mask = ~(np.isnan(obs_period[depth_col]) | np.isnan(sim_interp.values))\n",
    "            if valid_mask.sum() > 0:\n",
    "                obs_valid = obs_period[depth_col][valid_mask]\n",
    "                sim_valid = sim_interp.values[valid_mask]\n",
    "                \n",
    "                # Scatter plot\n",
    "                ax.scatter(obs_valid, sim_valid, color='blue', alpha=0.6, s=20)\n",
    "                \n",
    "                # Add 1:1 line\n",
    "                min_val = min(obs_valid.min(), sim_valid.min())\n",
    "                max_val = max(obs_valid.max(), sim_valid.max())\n",
    "                ax.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "                \n",
    "                # Calculate and display metrics\n",
    "                rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                bias = (sim_valid - obs_valid).mean()\n",
    "                corr = np.corrcoef(obs_valid, sim_valid)[0, 1]\n",
    "                \n",
    "                ax.text(0.02, 0.95, f\"RMSE: {rmse:.3f}\\nBias: {bias:.3f}\\nCorr: {corr:.3f}\", \n",
    "                       transform=ax.transAxes, fontsize=10, \n",
    "                       bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f\"Depth: {obs_depth:.4f}m\", fontsize=12)\n",
    "        ax.set_xlabel('Observed SM (mÂ³/mÂ³)', fontsize=10)\n",
    "        ax.set_ylabel('Simulated SM (mÂ³/mÂ³)', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.suptitle(f\"Observed vs. Simulated Soil Moisture Scatter Plots\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # If no observed data, just plot simulated data for all layers\n",
    "    print(\"No observed data available. Plotting simulated data only.\")\n",
    "    \n",
    "    # Plot simulated data for first few layers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Select first 4 layers that have valid data\n",
    "    valid_layers = []\n",
    "    mean_depths = layer_depths_filtered.mean(dim='time')\n",
    "    for i in range(min(4, soil_moisture_filtered.sizes['midToto'])):\n",
    "        if mean_depths.isel(midToto=i) > 0:\n",
    "            valid_layers.append(i)\n",
    "    \n",
    "    for i, layer_idx in enumerate(valid_layers):\n",
    "        sim_layer = soil_moisture_filtered.isel(midToto=layer_idx)\n",
    "        sim_layer = sim_layer.where(sim_layer > -100)  # Remove missing values\n",
    "        mean_depth = mean_depths.isel(midToto=layer_idx).values\n",
    "        \n",
    "        plt.plot(sim_layer.time, sim_layer.values, '-', \n",
    "                label=f'Layer {layer_idx} (depth: {mean_depth:.4f}m)', \n",
    "                linewidth=2)\n",
    "    \n",
    "    plt.title(f\"Simulated Soil Moisture at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Soil Moisture (mÂ³/mÂ³)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nSoil Moisture visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Point-Scale Modeling Insights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scienv)",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
