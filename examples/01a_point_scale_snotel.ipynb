{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Point-Scale Workflow (Snotel Example)\n",
    "\n",
    "This notebook demonstrates CONFLUENCE's simplest spatial configuration: point-scale modeling. We'll simulate vertical processes at a single location, specifically for a snow monitoring location: [The Loveland Basin SNOTEL Station (id: 602)]('https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=602')\n",
    "\n",
    "## Overview\n",
    "\n",
    "Point-scale modeling focuses on vertical processes without spatial heterogeneity here we will compare snow accumulation and melt at the station. \n",
    " \n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand point-scale modeling in CONFLUENCE\n",
    "2. Configure CONFLUENCE for single-point simulations\n",
    "3. Analyze point-scale SUMMA snow simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Point-Scale Configuration\n",
    "\n",
    "We'll use the point configuration template in 0_config_files/0_config_point_template.yaml as our baseline. \n",
    "\n",
    "We copy the template to a new location and update teh key configuration settings for our specific experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and settings \n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Update name and experiment id\n",
    "config_dict['DOMAIN_NAME'] = 'loveland_pass_snotel'\n",
    "config_dict['EXPERIMENT_ID'] = 'point_scale_tutorial'\n",
    "\n",
    "# Save point-scale configuration to temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE with the new configuration\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Print summary of the key settings\n",
    "print(\"=== Point-Scale Configuration ===\")\n",
    "print(f\"Domain Name: {config_dict['DOMAIN_NAME']}\")\n",
    "print(f\"Spatial Mode: {config_dict['SPATIAL_MODE']}\")\n",
    "print(f\"Location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"Period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"Forcing Data: {config_dict['FORCING_DATASET']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Project Structure\n",
    "\n",
    "1. We setup the basic directory structure under the root data directory specified in the CONFLUENCE_DATA_DIR setting in the configuration file\n",
    "2. We create a point shapefile from the coordinates given in POUR_POINT_COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "print(\"Creating point-scale project structure...\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point (in this case, our SNOTEL location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition - Data acquisition \n",
    "Now we will acquire the geospatial attributes we need to setup our model\n",
    "\n",
    "- Elevation\n",
    "- Land Cover\n",
    "- Soil Classifications\n",
    "\n",
    "We use the Model Agnostic Framework [gistool (Keshavarz et al., 2025)](https://github.com/CH-Earth/gistool) to subset the data based on the coordinates set in: BOUNDING_BOX_COORDS\n",
    "\n",
    "When SPATIAL_MODE is set to Point confluence automatically updates the BOUNDING_BOX_COORDS to a square buffer that is by default 0.001 degrees, the point buffer distance setting can be set in the POINT_BUFFER_DISTANCE setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire attributes\n",
    "print(\"Acquiring geospatial attributes for point location...\")\n",
    "\n",
    "print(f\"Minimal bounding box: {confluence.config.get('BOUNDING_BOX_COORDS')}\")\n",
    "\n",
    "#confluence.managers['data'].acquire_attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition - Domain creation\n",
    "\n",
    "Confluence creates the required shapefiles to pre-process and configure the models\n",
    "\n",
    "When run in point configuration a square polygon is produced in path/to/domain_dir/shapefiles/catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Geospatial Domain Definition and Analysis\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Analysis ===\")\n",
    "\n",
    "# Define domain\n",
    "print(\"\\nDefining minimal domain for point-scale simulation...\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "# Discretize domain (single HRU for point-scale)\n",
    "print(\"\\nCreating single HRU for point-scale simulation...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check outputs\n",
    "print(\"\\nDomain definition complete:\")\n",
    "print(f\"  - Watershed defined: {watershed_path is not None}\")\n",
    "print(f\"  - HRUs created: {hru_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Agnostic Data Pre-Processing - Data Acquisition\n",
    "Next we need meteorological data to run our models and observations to compare them with. \n",
    "\n",
    "We use the Model Agnostic Framework [datatool (Keshavarz et al., 2025)](https://github.com/CH-Earth/datatool) to subset the forcing dataset defined in FORCING_DATASET: for the spatial bounding box for the period configured between EXPERIMENT_TIME_START and EXPERIMENT_TIME_END.\n",
    "\n",
    "A temperature lapse rate can be applied to the forcing data by setting APPLY_LAPSE_RATE: True, the lapse rate can be set with the LAPSE_RATE which defaults to 0.0065 K m-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Agnostic Data Pre-Processing\n",
    "print(\"=== Step 3: Model Agnostic Data Pre-Processing ===\")\n",
    "\n",
    "# Acquire forcings\n",
    "print(f\"\\nAcquiring forcing data for point location...\")\n",
    "print(f\"Dataset: {confluence.config['FORCING_DATASET']}\")\n",
    "#confluence.managers['data'].acquire_forcings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAF has the snotel dataset in storage and confluence can aquire data based on station id by setting DOWNLOAD_SNOTEL: 'true' and the appropriate setting for SNOTEL_STATION, which in our case is '602'. The path to the SNOTEL data can be set manually with the SNOTEL_PATH configuration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process observed data \n",
    "print(\"Processing observed data...\")\n",
    "#confluence.managers['data'].process_observed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Agnostic Data Pre-Processing - Remapping and zonal statistics\n",
    "\n",
    "Remapping of the forcing data and zonal statistics calcuations for the geospatial attributes is preformed in one model agnostic pre-processing step. \n",
    "\n",
    "The forcing data are remapped onto the defined hydrofabric using [EASYMORE (Gherari et al., 2023)](https://www.sciencedirect.com/science/article/pii/S2352711023002431)\n",
    "Zonal statistics are run using [rasterstats]('https://pypi.org/project/rasterstats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\nModel-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Specific - Pre Processing \n",
    "\n",
    "Using the model agnostic output the model specific input files are prepared in one model specific preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Specific Processing and Initialization\n",
    "print(\"=== Step 4: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\nModel-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Specific - Initialisation\n",
    "\n",
    "Once the model input files have been created the models are instantiated with their default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Result visualisation\n",
    "\n",
    "Now let's look how our simulations turned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Visualize Observed vs. Simulated SWE\n",
    "print(\"=== Step 11: Comparing Observed vs. Simulated SWE ===\")\n",
    "\n",
    "# 1. Load the observed SWE data\n",
    "obs_swe_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"observations\" / \"snow\" / \"swe\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "# Check if observed data exists\n",
    "if not obs_swe_path.exists():\n",
    "    print(f\"Warning: Observed SWE data not found at {obs_swe_path}\")\n",
    "    print(\"Checking for alternative locations...\")\n",
    "    # Try to find data in parent directories\n",
    "    alt_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"**/observations/snow/swe/*_swe_processed.csv\"))\n",
    "    if alt_paths:\n",
    "        obs_swe_path = alt_paths[0]\n",
    "        print(f\"Found alternative SWE data at: {obs_swe_path}\")\n",
    "    else:\n",
    "        print(\"No observed SWE data found. Only simulated data will be displayed.\")\n",
    "\n",
    "# Load observed SWE data if available\n",
    "if obs_swe_path.exists():\n",
    "    print(f\"Loading observed SWE data from: {obs_swe_path}\")\n",
    "    obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'])\n",
    "    obs_swe.set_index('Date', inplace=True)\n",
    "    print(f\"Observed data period: {obs_swe.index.min()} to {obs_swe.index.max()}\")\n",
    "    print(f\"Observed SWE range: {obs_swe['SWE'].min():.2f} to {obs_swe['SWE'].max():.2f} mm\")\n",
    "else:\n",
    "    obs_swe = None\n",
    "\n",
    "# 2. Load the simulated SWE data\n",
    "sim_path = Path(config_dict['CONFLUENCE_DATA_DIR']) / f\"domain_{config_dict['DOMAIN_NAME']}\" / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\" / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Check for alternative NetCDF file patterns if not found\n",
    "if not sim_path.exists():\n",
    "    print(f\"Simulated data not found at {sim_path}\")\n",
    "    print(\"Checking for alternative NetCDF files...\")\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/SUMMA/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_path = alt_sim_paths[0]\n",
    "        print(f\"Found alternative simulation data at: {sim_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulation results found for experiment {config_dict['EXPERIMENT_ID']}\")\n",
    "\n",
    "# Load simulated data\n",
    "print(f\"Loading simulated data from: {sim_path}\")\n",
    "ds = xr.open_dataset(sim_path)\n",
    "\n",
    "# Extract scalarSWE and convert to DataFrame\n",
    "sim_swe = ds['scalarSWE'].to_dataframe().reset_index()\n",
    "# Assuming first HRU for point-scale simulation\n",
    "sim_swe = sim_swe[sim_swe['hru'] == 1][['time', 'scalarSWE']]\n",
    "sim_swe.columns = ['Date', 'SWE']\n",
    "sim_swe.set_index('Date', inplace=True)\n",
    "print(f\"Simulated data period: {sim_swe.index.min()} to {sim_swe.index.max()}\")\n",
    "print(f\"Simulated SWE range: {sim_swe['SWE'].min():.2f} to {sim_swe['SWE'].max():.2f} mm\")\n",
    "\n",
    "# 3. Find common date range if observed data exists\n",
    "if obs_swe is not None:\n",
    "    # Ensure same frequency for both datasets\n",
    "    obs_swe = obs_swe.resample('D').mean()  # Daily mean if multiple obs per day\n",
    "    sim_swe = sim_swe.resample('D').mean()  # Daily mean if sub-daily sim data\n",
    "    \n",
    "    # Find common date range\n",
    "    start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "    end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "    \n",
    "    print(f\"\\nCommon data period: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter to common period\n",
    "    obs_period = obs_swe.loc[start_date:end_date]\n",
    "    sim_period = sim_swe.loc[start_date:end_date]\n",
    "    \n",
    "    # Handle different units if necessary (assuming both are in mm)\n",
    "    # If different units, add conversion here\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(((obs_period['SWE'] - sim_period['SWE']) ** 2).mean())\n",
    "    bias = (sim_period['SWE'] - obs_period['SWE']).mean()\n",
    "    corr = obs_period['SWE'].corr(sim_period['SWE'])\n",
    "    \n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - RMSE: {rmse:.2f} mm\")\n",
    "    print(f\"  - Bias: {bias:.2f} mm\")\n",
    "    print(f\"  - Correlation: {corr:.2f}\")\n",
    "    \n",
    "    # 4. Visualize the comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot both time series\n",
    "    plt.plot(obs_period.index, obs_period['SWE'], 'o-', label='Observed SWE', color='black', alpha=0.7, markersize=4)\n",
    "    plt.plot(sim_period.index, sim_period['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "        \n",
    "    # Styling\n",
    "    plt.title(f\"SWE Comparison at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Scatter plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(obs_period['SWE'], sim_period['SWE'], color='blue', alpha=0.7)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    max_val = max(obs_period['SWE'].max(), sim_period['SWE'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "    \n",
    "    # Styling\n",
    "    plt.title(f\"Observed vs. Simulated SWE\", fontsize=14)\n",
    "    plt.xlabel('Observed SWE (mm)', fontsize=12)\n",
    "    plt.ylabel('Simulated SWE (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Add annotation with metrics\n",
    "    plt.text(0.02, 0.95, f\"RMSE: {rmse:.2f} mm\\nBias: {bias:.2f} mm\\nCorr: {corr:.2f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, \n",
    "             bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # If no observed data, just plot simulated\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sim_swe.index, sim_swe['SWE'], '-', label='Simulated SWE', color='blue', linewidth=2)\n",
    "    plt.title(f\"Simulated SWE at {config_dict['DOMAIN_NAME'].replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Snow Water Equivalent (mm)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n",
    "\n",
    "print(\"\\nSWE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Point-Scale Modeling Insights\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale up to lumped basin**: Use calibrated parameters\n",
    "2. **Compare multiple sites**: Test model transferability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONFLUENCE (HPC Modules)",
   "language": "python",
   "name": "conf-env-hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
