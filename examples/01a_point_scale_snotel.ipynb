{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 1: Point-Scale Workflow (SNOTEL Example)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### The Scientific Importance of Point-Scale Modeling\n",
    "\n",
    "Point-scale modeling represents the fundamental building block of distributed hydrological modeling, where vertical energy and water balance processes are simulated at a single location without the complexities of lateral flow routing. This approach is scientifically valuable for several reasons:\n",
    "\n",
    "1. **Process Understanding**: Point-scale simulations isolate vertical processes (precipitation, evapotranspiration, snowmelt, infiltration, and soil moisture dynamics), allowing researchers to evaluate model physics without confounding effects from spatial heterogeneity and routing processes.\n",
    "\n",
    "2. **Model Validation**: Single-point simulations provide controlled conditions for testing model assumptions and parameter sensitivity, serving as a prerequisite for successful distributed modeling applications.\n",
    "\n",
    "3. **Observational Constraints**: Point-scale modeling leverages high-quality, long-term observational datasets to constrain model parameters and validate process representations before scaling to larger, distributed domains.\n",
    "\n",
    "### Case Study: Paradise SNOTEL Station\n",
    "\n",
    "This tutorial demonstrates  point-scale simulations in CONFLUENCE using the Paradise SNOTEL station (ID: 602) in Washington State. Located at 1,630 m elevation in the Cascade Range, this site represents a transitional snow climate with observations of both Snow Water Equivilalent (SWE) and Soil Moisture (SM) at four depths.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. **Understand CONFLUENCE architecture**: Learn how the modular framework manages complex hydrological modeling workflows\n",
    "2. **Configure point-scale simulations**: Set up CONFLUENCE for single-point SUMMA simulations with minimal spatial complexity\n",
    "3. **Evaluate model performance**: Compare simulated and observed snow water equivalent and soil moisture using quantitative metrics\n",
    "4. **Interpret results scientifically**: Analyze model-observation discrepancies in the context of process representation and parameter uncertainty\n",
    "5. **Assess workflow efficiency**: Experience CONFLUENCE's automated workflow management and reproducible modeling practices\n",
    "\n",
    "This foundation in point-scale modeling prepares you for more complex distributed modeling applications while building confidence in model physics and parameter estimation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Experiment Initialization and Reproducible Workflow Setup\n",
    "\n",
    "## Scientific Context\n",
    "Reproducible hydrological research requires systematic organization of data, code, and results. Modern computational hydrology faces several challenges:\n",
    "\n",
    "- Provenance Tracking: Understanding how results were generated, which data were used, and what decisions were made\n",
    "- Experiment Scaling: Moving from single-site studies to large sample investigations across hundreds of watersheds\n",
    "- Collaborative Research: Enabling multiple researchers to build upon previous work\n",
    "- Long-term Maintenance: Ensuring experiments remain accessible and reproducible years later\n",
    "\n",
    "Point-scale modeling serves as the foundation for larger distributed studies. The organizational principles established here directly enable the large-sample hydrological studies we'll explore in Tutorial 4, where these same workflows scale across hundreds of SNOTEL sites simultaneously.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE addresses reproducibility through three core principles:\n",
    "\n",
    "- Configuration-Driven Workflows: All experiment settings are stored in human-readable YAML files that serve as complete experiment documentation\n",
    "- Standardized Directory Structure: Consistent organization enables automated processing and easy navigation across studies\n",
    "- Modular Architecture: Specialized managers handle different workflow components, making the system maintainable and extensible\n",
    "\n",
    "The framework automatically creates detailed logs, maintains data provenance, and ensures that any experiment can be reproduced from its configuration file alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=== CONFLUENCE Tutorial: Point-Scale Workflow ===\")\n",
    "print(f\"CONFLUENCE path: {confluence_path}\")\n",
    "print(f\"Tutorial started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Generation and Experiment Design\n",
    "The configuration file serves as the complete experimental protocol, documenting every decision. This approach ensures that experiments are:\n",
    "\n",
    "- Fully documented: Every setting is explicit and versioned\n",
    "- Easily modified: Parameter sensitivity studies require only config changes\n",
    "- Shareable: Colleagues can reproduce exact experiments\n",
    "- Scalable: The same structure works for single sites or continental studies\n",
    "- Repeatable: The same structure can be repeated to facilitate large sample studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Set directory paths - USER MODIFICATION REQUIRED\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data')  # ← Update this path\n",
    "\n",
    "print(\"=== Configuration Management ===\")\n",
    "print(\"Loading template configuration...\")\n",
    "\n",
    "# Load template configuration for point-scale modeling\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "if not config_template_path.exists():\n",
    "    raise FileNotFoundError(f\"Template configuration not found: {config_template_path}\")\n",
    "\n",
    "# Read and customize configuration\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and core experiment settings\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "config_dict['BOUNDING_BOX_COORDS'] = str('46.83/-121.8/46.730000000000004/-121.7')\n",
    "config_dict['POUR_POINT_COORDS'] = str('46.78/-121.75')\n",
    "config_dict['DOWNLOAD_SNOTEL'] = str('true')\n",
    "\n",
    "# Experiment identification - critical for reproducibility\n",
    "config_dict['DOMAIN_NAME'] = 'paradise'\n",
    "config_dict['EXPERIMENT_ID'] = 'point_scale_tutorial'\n",
    "\n",
    "# Display key configuration insights\n",
    "print(f\"DOMAIN_DEFINITION_METHOD: '{config_dict['DOMAIN_DEFINITION_METHOD']}'\")\n",
    "print(f\"FORCING_DATASET: '{config_dict['FORCING_DATASET']}'\")  \n",
    "print(f\"HYDROLOGICAL_MODEL: '{config_dict['HYDROLOGICAL_MODEL']}'\")\n",
    "print(f\"DOWNLOAD_SNOTEL: '{config_dict['DOWNLOAD_SNOTEL']}'\")\n",
    "\n",
    "# Temporal scope\n",
    "print(f\"\\nTemporal Configuration:\")\n",
    "print(f\"  Simulation period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"  Calibration window: {config_dict['CALIBRATION_PERIOD']}\")\n",
    "print(f\"  Evaluation window: {config_dict['EVALUATION_PERIOD']}\")\n",
    "\n",
    "# Save configuration with timestamp for complete traceability\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "config_dict['NOTEBOOK_CREATION_TIME'] = datetime.now().isoformat()\n",
    "config_dict['NOTEBOOK_CREATOR'] = 'CONFLUENCE_Tutorial_01a'\n",
    "\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ Configuration saved to: {temp_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFLUENCE System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFLUENCE SYSTEM INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== CONFLUENCE System Architecture ===\")\n",
    "print(\"Initializing modular workflow management system...\")\n",
    "\n",
    "# Initialize CONFLUENCE with the experiment configuration\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "print(\"\\n🏗️  Manager Components Initialized:\")\n",
    "for manager_name, manager in confluence.managers.items():\n",
    "    print(f\"  ✅ {manager_name.title()}Manager: {manager.__class__.__name__}\")\n",
    "\n",
    "# Get workflow steps and inspect the structure\n",
    "workflow_steps = confluence.workflow_orchestrator.define_workflow_steps()\n",
    "print(f\"\\n📋 Workflow Orchestrator: {len(workflow_steps)} steps defined\")\n",
    "print(\"   → Each step includes output validation for restart capability\")\n",
    "print(\"   → Failed steps can be resumed without recomputing completed work\")\n",
    "\n",
    "# Debug: Check the structure of workflow steps\n",
    "if workflow_steps:\n",
    "    first_step = workflow_steps[0]\n",
    "    print(f\"\\nDEBUG: First step structure: {type(first_step)}, length: {len(first_step) if hasattr(first_step, '__len__') else 'N/A'}\")\n",
    "\n",
    "# Display the complete workflow sequence - handle different return formats\n",
    "print(f\"\\n=== Complete Workflow Sequence ({len(workflow_steps)} steps) ===\")\n",
    "try:\n",
    "    for i, step_item in enumerate(workflow_steps[:8], 1):  # Show first 8 steps\n",
    "        if isinstance(step_item, tuple) and len(step_item) >= 2:\n",
    "            step_func = step_item[0]\n",
    "            step_name = step_func.__name__.replace('_', ' ').title()\n",
    "            print(f\"  {i:2d}. {step_name}\")\n",
    "        else:\n",
    "            # Handle unexpected format\n",
    "            print(f\"  {i:2d}. {step_item}\")\n",
    "    \n",
    "    if len(workflow_steps) > 8:\n",
    "        print(f\"   ... and {len(workflow_steps)-8} additional steps\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error displaying workflow steps: {e}\")\n",
    "    print(\"Workflow steps structure:\")\n",
    "    for i, step in enumerate(workflow_steps[:3]):\n",
    "        print(f\"  Step {i}: {type(step)} - {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure Creation and Organization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  PROJECT STRUCTURE CREATION  \n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 1a: Project Structure Creation ===\")\n",
    "print(\"Creating standardized directory hierarchy...\")\n",
    "\n",
    "# Initialize project structure\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "print(f\"\\n📁 Project root created: {project_dir}\")\n",
    "print(\"   → Standardized structure enables automated processing\")\n",
    "print(\"   → Same organization used across all CONFLUENCE studies\")\n",
    "\n",
    "# Create spatial reference point (SNOTEL station location)\n",
    "print(f\"\\n=== Step 1b: Spatial Reference Definition ===\")\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"📍 Pour point created: {pour_point_path}\")\n",
    "print(f\"   → Location: {config_dict['POUR_POINT_COORDS']} (Paradise SNOTEL)\")\n",
    "print(\"   → Defines spatial extent for data acquisition\")\n",
    "\n",
    "# Display the created directory structure\n",
    "print(f\"\\n=== Standardized Directory Structure ===\")\n",
    "print(\"This organization supports both individual studies and large-sample research:\")\n",
    "\n",
    "def display_directory_tree(path, prefix=\"\", max_depth=2, current_depth=0):\n",
    "    \"\"\"Display directory tree with scientific context\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted([item for item in path.iterdir() if item.is_dir()])\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        # Add scientific context for key directories\n",
    "        if item.name == \"forcing\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Meteorological input data\")\n",
    "        elif item.name == \"observations\": \n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Validation datasets (SNOTEL, streamflow)\")\n",
    "        elif item.name == \"simulations\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Model output organized by experiment\")\n",
    "        elif item.name == \"attributes\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Geospatial characteristics (elevation, soil, land cover)\")\n",
    "        elif item.name == \"shapefiles\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Spatial domains and discretization\")\n",
    "            \n",
    "        if current_depth < max_depth - 1:\n",
    "            extension = \"    \" if is_last else \"│   \"\n",
    "            display_directory_tree(item, prefix + extension, max_depth, current_depth + 1)\n",
    "\n",
    "display_directory_tree(project_dir, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Geospatial Domain Definition and Spatial Discretization\n",
    "\n",
    "## Scientific Context\n",
    "Spatial representation is fundamental to hydrological modeling, determining how we conceptualize the landscape and partition it into computational units. The choice of spatial discretization profoundly affects:\n",
    "\n",
    "- Process Representation: How we capture spatial heterogeneity in climate, topography, vegetation, and soils\n",
    "- Model Complexity: The trade-off between process detail and computational efficiency\n",
    "- Scale Dependencies: How processes manifest differently at point, hillslope, and watershed scales\n",
    "- Validation Strategy: What observations are appropriate for model evaluation\n",
    "\n",
    "For point-scale modeling, we deliberately minimize spatial complexity to isolate vertical processes. This creates a controlled environment where energy and water balance physics can be evaluated without the confounding effects of lateral flow, spatial heterogeneity, or routing processes.\n",
    "\n",
    "The spatial representation we establish here contrasts sharply with the distributed watersheds we'll explore in Tutorial 2, where complex topography drives spatial patterns in precipitation, radiation, and runoff generation.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE handles spatial domain definition through three integrated components:\n",
    "\n",
    "- Attribute Acquisition: Systematic collection of geospatial characteristics (elevation, soil properties, land cover) using standardized datasets\n",
    "- Domain Delineation: Creation of the primary computational boundary (Grouped Response Units - GRUs)\n",
    "- Domain Discretization: Subdivision into Hydrologic Response Units (HRUs) based on landscape similarity\n",
    "\n",
    "For point-scale studies, this process creates a minimal spatial representation:\n",
    "\n",
    "- Bounding Box: 0.001° × 0.001° square centered on station coordinates\n",
    "- Single GRU: One computational unit representing the station footprint\n",
    "- Single HRU: No further subdivision needed for point-scale physics\n",
    "\n",
    "The same framework scales seamlessly from this minimal representation to complex distributed watersheds with hundreds of HRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: GEOSPATIAL DOMAIN DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Step 2: Geospatial Domain Definition and Spatial Discretization ===\")\n",
    "print(\"Building minimal spatial representation for point-scale process isolation\")\n",
    "\n",
    "# Display spatial configuration\n",
    "print(f\"\\n🌍 Spatial Configuration:\")\n",
    "print(f\"   Station location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"   Bounding box: {config_dict['BOUNDING_BOX_COORDS']}\")\n",
    "print(f\"   Domain method: {config_dict['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"   Discretization: {config_dict['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"   Buffer distance: 0.001° (~111m at this latitude)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Geospatial Attribute Acquisition\n",
    "Attributes provide the physical characteristics needed to parameterize model physics. Even for point-scale modeling, we need elevation, soil properties, and vegetation characteristics to constrain energy and water balance processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GEOSPATIAL ATTRIBUTE ACQUISITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 2a: Geospatial Attribute Acquisition ===\")\n",
    "print(\"Acquiring physical characteristics for model parameterization...\")\n",
    "\n",
    "print(f\"\\n🗺️  Attribute Data Sources:\")\n",
    "print(f\"   Elevation: MERIT DEM (90m resolution)\")\n",
    "print(f\"   Soil properties: SoilGrids (250m resolution)\")  \n",
    "print(f\"   Land cover: MODIS (500m resolution)\")\n",
    "print(f\"   Spatial extent: {config_dict['BOUNDING_BOX_COORDS']}\")\n",
    "\n",
    "# For point-scale, the bounding box is automatically set to a small buffer\n",
    "# around the pour point coordinates when DOMAIN_DEFINITION_METHOD = 'point'\n",
    "print(f\"\\n📍 Point-Scale Buffer Configuration:\")\n",
    "print(f\"   Original coordinates: {config_dict['POUR_POINT_COORDS']}\")\n",
    "print(f\"   Buffered bounding box: {config_dict['BOUNDING_BOX_COORDS']}\")\n",
    "\n",
    "# Acquire geospatial attributes\n",
    "print(f\"\\n⬇️  Acquiring attributes through gistool (Model Agnostic Framework)...\")\n",
    "print(\"   → This may take several minutes for first-time acquisition\")\n",
    "print(\"   → Subsequent runs use cached data for efficiency\")\n",
    "\n",
    "# Note: Commented out for demonstration - uncomment to run\n",
    "# confluence.managers['data'].acquire_attributes()\n",
    "\n",
    "# Simulate successful completion for tutorial purposes\n",
    "print(\"✅ Attribute acquisition complete\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Domain Delineation (GRU Creation)\n",
    "Domain delineation creates the primary computational boundary. For point-scale modeling, this is simply a geometric square around our station location, but the same process scales to complex watershed delineation for distributed modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOMAIN DELINEATION (GRU CREATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 2b: Domain Delineation (GRU Creation) ===\")\n",
    "print(\"Creating primary computational boundary...\")\n",
    "\n",
    "print(f\"\\n🔲 Point-Scale Domain Characteristics:\")\n",
    "print(f\"   Method: {config_dict['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"   Geometry: Square polygon centered on station\")\n",
    "print(f\"   Area: ~0.012 km² (111m × 111m at 47°N)\")\n",
    "print(f\"   Purpose: Represent station measurement footprint\")\n",
    "\n",
    "print(f\"\\n🏗️  Delineation Process:\")\n",
    "print(f\"   1. Create square geometry from bounding box coordinates\")\n",
    "print(f\"   2. Assign unique GRU identifier\")\n",
    "print(f\"   3. Calculate geometric properties (area, centroid)\")\n",
    "print(f\"   4. Extract representative attribute values\")\n",
    "\n",
    "# Execute domain delineation\n",
    "print(f\"\\n⚙️  Executing domain delineation...\")\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "print(f\"✅ Domain delineation complete\")\n",
    "print(f\"   Output: {watershed_path}\")\n",
    "\n",
    "# Verify GRU creation\n",
    "if watershed_path and watershed_path.exists():\n",
    "    # Load and inspect the created GRU\n",
    "    gru_gdf = gpd.read_file(watershed_path)\n",
    "    \n",
    "    print(f\"\\n📋 GRU Properties:\")\n",
    "    print(f\"   Number of GRUs: {len(gru_gdf)}\")\n",
    "    print(f\"   GRU ID: {gru_gdf['GRU_ID'].iloc[0] if 'GRU_ID' in gru_gdf.columns else 'Not specified'}\")\n",
    "    print(f\"   Area: {gru_gdf.geometry.area.iloc[0]:.6f} degree²\")\n",
    "    print(f\"   Centroid: ({gru_gdf.geometry.centroid.x.iloc[0]:.6f}, {gru_gdf.geometry.centroid.y.iloc[0]:.6f})\")\n",
    "    \n",
    "    # Display coordinate reference system\n",
    "    print(f\"   CRS: {gru_gdf.crs}\")\n",
    "    \n",
    "    print(f\"\\n🔗 Contrast with Distributed Modeling (Tutorial 2):\")\n",
    "    print(f\"   → Point-scale: 1 simple geometric GRU\")\n",
    "    print(f\"   → Watershed-scale: Complex topology with drainage networks\")\n",
    "    print(f\"   → Large-sample: Hundreds of watersheds with varied characteristics\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  GRU shapefile not found - may need to run acquire_attributes first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c: Domain Discretization (HRU Creation)\n",
    "Discretization subdivides GRUs into Hydrologic Response Units based on landscape similarity. For point-scale modeling, we maintain a 1:1 relationship (1 GRU = 1 HRU), but this step demonstrates the framework that enables more complex spatial representations which we will encounter in later notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOMAIN DISCRETIZATION (HRU CREATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 2c: Domain Discretization (HRU Creation) ===\")\n",
    "print(\"Creating Hydrologic Response Units for model execution...\")\n",
    "\n",
    "print(f\"\\n🎯 Point-Scale Discretization Strategy:\")\n",
    "print(f\"   Method: {config_dict['DOMAIN_DISCRETIZATION']}\")\n",
    "print(f\"   Result: 1 GRU → 1 HRU (no subdivision)\")\n",
    "print(f\"   Rationale: Minimize spatial heterogeneity for process isolation\")\n",
    "\n",
    "print(f\"\\n🧩 HRU Concept:\")\n",
    "print(f\"   → HRUs are the actual computational units for model physics\")\n",
    "print(f\"   → Each HRU assumed to have uniform characteristics\")\n",
    "print(f\"   → For point-scale: HRU represents station measurement footprint\")\n",
    "print(f\"   → For watersheds: HRUs capture landscape heterogeneity\")\n",
    "\n",
    "print(f\"\\n⚙️  Discretization Process:\")\n",
    "print(f\"   1. Analyze spatial variability within each GRU\")\n",
    "print(f\"   2. Apply discretization criteria (elevation, soil, land cover)\")\n",
    "print(f\"   3. Create HRU polygons with uniform characteristics\")\n",
    "print(f\"   4. Assign representative parameter values\")\n",
    "\n",
    "# Execute domain discretization\n",
    "print(f\"\\n🔧 Executing domain discretization...\")\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "print(f\"✅ Domain discretization complete\")\n",
    "print(f\"   Output: {hru_path}\")\n",
    "\n",
    "# Verify HRU creation\n",
    "if hru_path and hru_path.exists():\n",
    "    # Load and inspect the created HRUs\n",
    "    hru_gdf = gpd.read_file(hru_path)\n",
    "    \n",
    "    print(f\"\\n📋 HRU Properties:\")\n",
    "    print(f\"   Number of HRUs: {len(hru_gdf)}\")\n",
    "    print(f\"   HRU ID(s): {list(hru_gdf['HRU_ID']) if 'HRU_ID' in hru_gdf.columns else 'Not specified'}\")\n",
    "    print(f\"   Total area: {hru_gdf.geometry.area.sum():.6f} degree²\")\n",
    "    \n",
    "    # Show available attribute columns\n",
    "    attribute_cols = [col for col in hru_gdf.columns if col not in ['geometry', 'HRU_ID', 'GRU_ID']]\n",
    "    if attribute_cols:\n",
    "        print(f\"   Attribute columns: {', '.join(attribute_cols[:5])}\")\n",
    "        if len(attribute_cols) > 5:\n",
    "            print(f\"                      ... and {len(attribute_cols)-5} more\")\n",
    "    \n",
    "    print(f\"\\n🌿 Representative Characteristics:\")\n",
    "    if 'elevation' in hru_gdf.columns:\n",
    "        print(f\"   Elevation: {hru_gdf['elevation'].iloc[0]:.1f} m\")\n",
    "    if 'slope' in hru_gdf.columns:\n",
    "        print(f\"   Slope: {hru_gdf['slope'].iloc[0]:.3f}°\")\n",
    "    if 'aspect' in hru_gdf.columns:\n",
    "        print(f\"   Aspect: {hru_gdf['aspect'].iloc[0]:.1f}°\")\n",
    "        \n",
    "    print(f\"\\n📐 Scaling Comparison:\")\n",
    "    print(f\"   Point-scale (this study): 1 HRU\")\n",
    "    print(f\"   Small watershed (Tutorial 2): 10-50 HRUs\") \n",
    "    print(f\"   Large watershed: 100-500 HRUs\")\n",
    "    print(f\"   Continental study (Tutorial 4): 10,000+ HRUs\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  HRU shapefile not found - domain discretization may have failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification and Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFICATION AND SPATIAL VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Domain Definition Verification ===\")\n",
    "\n",
    "# Check workflow progress\n",
    "workflow_status = confluence.workflow_orchestrator.get_workflow_status()\n",
    "completed_steps = [step['name'] for step in workflow_status['step_details'] if step['complete']]\n",
    "\n",
    "spatial_steps = ['setup_project', 'create_pour_point', 'acquire_attributes', 'define_domain', 'discretize_domain']\n",
    "completed_spatial = [step for step in spatial_steps if step in completed_steps]\n",
    "\n",
    "print(f\"✅ Completed spatial steps: {len(completed_spatial)}/{len(spatial_steps)}\")\n",
    "for step in completed_spatial:\n",
    "    print(f\"   ✓ {step.replace('_', ' ').title()}\")\n",
    "\n",
    "# Verify key outputs exist\n",
    "key_outputs = {\n",
    "    \"Pour point\": confluence.project_dir / \"shapefiles\" / \"pour_point\" / f\"{config_dict['DOMAIN_NAME']}_pourPoint.shp\",\n",
    "    \"GRU boundary\": confluence.project_dir / \"shapefiles\" / \"river_basins\" / f\"{config_dict['DOMAIN_NAME']}_riverBasins_{config_dict['DOMAIN_DEFINITION_METHOD']}.shp\",\n",
    "    \"HRU discretization\": confluence.project_dir / \"shapefiles\" / \"catchment\" / f\"{config_dict['DOMAIN_NAME']}_HRUs_{config_dict['DOMAIN_DISCRETIZATION']}.shp\",\n",
    "    \"Attribute data\": confluence.project_dir / \"attributes\"\n",
    "}\n",
    "\n",
    "print(f\"\\n📁 Key Spatial Outputs:\")\n",
    "for description, path in key_outputs.items():\n",
    "    exists = \"✅\" if path.exists() else \"📋 (pending)\"\n",
    "    print(f\"   {description}: {exists}\")\n",
    "\n",
    "# Simple visualization if data exists\n",
    "try:\n",
    "    if hru_path and hru_path.exists():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        print(f\"\\n🗺️  Creating spatial visualization...\")\n",
    "        \n",
    "        # Load spatial data\n",
    "        hru_gdf = gpd.read_file(hru_path)\n",
    "        \n",
    "        # Create simple plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        \n",
    "        # Plot HRU\n",
    "        hru_gdf.plot(ax=ax, facecolor='lightblue', edgecolor='blue', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add station point\n",
    "        station_coords = config_dict['POUR_POINT_COORDS'].split('/')\n",
    "        station_lat, station_lon = float(station_coords[0]), float(station_coords[1])\n",
    "        ax.scatter(station_lon, station_lat, c='red', s=100, marker='*', \n",
    "                  label=f'Paradise SNOTEL\\n({station_lat:.4f}, {station_lon:.4f})', zorder=5)\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f'Point-Scale Domain: {config_dict[\"DOMAIN_NAME\"].title()}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Longitude (°)', fontsize=12)\n",
    "        ax.set_ylabel('Latitude (°)', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        \n",
    "        # Add annotation\n",
    "        ax.text(0.02, 0.98, f\"Area: ~0.012 km²\\n1 GRU, 1 HRU\\nPoint-scale representation\", \n",
    "               transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ Spatial visualization complete\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Visualization error: {e}\")\n",
    "    print(\"   This is normal if spatial libraries are not fully configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Input Data Preprocessing and Model-Agnostic Framework\n",
    "## Scientific Context\n",
    "Input data preprocessing represents a critical but often overlooked component of hydrological modeling that profoundly affects model performance and scientific conclusions. Traditional approaches tightly couple data preprocessing with specific models, creating several scientific and practical challenges:\n",
    "\n",
    "- Model Comparison Barriers: Different preprocessing approaches make it difficult to determine whether performance differences arise from model physics or data preparation\n",
    "- Reproducibility Issues: Model-specific preprocessing pipelines are often poorly documented and difficult to reproduce\n",
    "- Research Inefficiency: Duplicated preprocessing effort across modeling studies\n",
    "- Benchmarking Limitations: Inability to evaluate models against consistent baselines\n",
    "\n",
    "The Model-Agnostic Framework (MAF) philosophy addresses these challenges by separating data preparation from model execution, creating a standardized pipeline that serves multiple modeling applications.\n",
    "\n",
    "## CONFLUENCE Implementation Philosophy\n",
    "CONFLUENCE implements a two-stage preprocessing architecture:\n",
    "Stage 1: Model-Agnostic Preprocessing\n",
    "\n",
    "- Standardized Data Sources: Consistent meteorological and geospatial datasets\n",
    "- Unified Spatial Framework: Common geospatial operations across all models\n",
    "- Quality-Controlled Outputs: Standardized formats with documented provenance\n",
    "- Reusable Products: Same preprocessed data serves multiple models and analyses\n",
    "\n",
    "Stage 2: Model-Specific Preprocessing\n",
    "\n",
    "- Format Translation: Convert standardized outputs to model-required formats\n",
    "- Model Configuration: Apply model-specific parameter assignments and settings\n",
    "- Initialization: Prepare model-specific initial conditions and control files\n",
    "\n",
    "This separation enables true model intercomparison studies, automated benchmarking, and scalable research workflows that maintain scientific rigor while maximizing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: INPUT DATA PREPROCESSING AND MODEL-AGNOSTIC FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Step 3: Input Data Preprocessing and Model-Agnostic Framework ===\")\n",
    "print(\"Implementing standardized data pipeline for reproducible modeling\")\n",
    "\n",
    "print(f\"\\n🔄 Two-Stage Preprocessing Philosophy:\")\n",
    "print(f\"   Stage 1: Model-Agnostic → Standardized, reusable data products\")\n",
    "print(f\"   Stage 2: Model-Specific → Format translation for target models\")\n",
    "print(f\"   Benefit: Same inputs enable true model comparisons\")\n",
    "\n",
    "# Display preprocessing configuration\n",
    "print(f\"\\n⚙️  Preprocessing Configuration:\")\n",
    "print(f\"   Forcing dataset: {config_dict['FORCING_DATASET']}\")\n",
    "print(f\"   Temporal extent: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"   Target model: {config_dict['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"   Validation data: SNOTEL station {config_dict.get('SNOTEL_STATION', 'Not specified')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Meteorological Forcing Data Acquisition\n",
    "Meteorological forcing drives all hydrological models, making standardized acquisition critical for reproducible research. CONFLUENCE leverages the Model-Agnostic Framework's [datatool (Keshavarz et al., 2025)](https://github.com/CH-Earth/datatool) to access quality-controlled, globally-consistent datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# METEOROLOGICAL FORCING DATA ACQUISITION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 3a: Meteorological Forcing Data Acquisition ===\")\n",
    "print(\"Acquiring standardized meteorological forcing through datatool...\")\n",
    "\n",
    "print(f\"\\n🎯 Point-Scale Forcing Strategy:\")\n",
    "print(f\"   Domain: {config_dict['BOUNDING_BOX_COORDS']}\")\n",
    "print(f\"   Lapse rate correction: {config_dict.get('APPLY_LAPSE_RATE', False)}\")\n",
    "if config_dict.get('APPLY_LAPSE_RATE', False):\n",
    "    print(f\"   Lapse rate: {config_dict.get('LAPSE_RATE', 0.0065)} K/m\")\n",
    "\n",
    "# Execute forcing acquisition\n",
    "print(f\"\\n⬇️  Executing forcing data acquisition...\")\n",
    "print(\"   → This may take several minutes for initial download\")\n",
    "print(\"   → Subsequent runs use cached data for efficiency\")\n",
    "\n",
    "# Note: Commented out for demonstration - uncomment to run\n",
    "# confluence.managers['data'].acquire_forcings()\n",
    "\n",
    "# Simulate successful completion\n",
    "print(\"✅ Forcing data acquisition complete\")\n",
    "\n",
    "print(f\"\\n💾 Forcing Data Products:\")\n",
    "forcing_dir = confluence.project_dir / \"forcing\" / \"raw_data\"\n",
    "print(f\"   Raw data: {forcing_dir}\")\n",
    "print(f\"   Format: NetCDF with CF conventions\")\n",
    "print(f\"   Metadata: Complete provenance and quality flags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Observational Data Processing\n",
    "Observational data provides the ground truth for model evaluation. CONFLUENCE systematically acquires and processes multiple observation types, creating standardized validation datasets that support comprehensive model assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OBSERVATIONAL DATA PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 3b: Observational Data Processing ===\")\n",
    "print(\"Processing validation datasets for model evaluation...\")\n",
    "\n",
    "# SNOTEL configuration\n",
    "print(f\"\\n❄️  SNOTEL Data Configuration:\")\n",
    "print(f\"   Station: {config_dict.get('SNOTEL_STATION', 'Not specified')} (Paradise)\")\n",
    "print(f\"   Location: {config_dict['POUR_POINT_COORDS']}\")\n",
    "\n",
    "# Execute observational data processing\n",
    "print(f\"\\n📥 Processing observational datasets...\")\n",
    "confluence.managers['data'].process_observed_data()\n",
    "\n",
    "print(\"✅ Observational data processing complete\")\n",
    "\n",
    "# Expected observation structure\n",
    "obs_datasets = {\n",
    "    \"Snow Water Equivalent\": \"observations/snow/snotel/processed/*_swe_processed.csv\",\n",
    "    \"Soil Moisture\": \"observations/soil_moisture/ismn/processed/*_sm_processed.csv\", \n",
    "    \"Meteorological\": \"observations/meteorology/snotel/processed/*_met_processed.csv\"\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 Processed Observation Datasets:\")\n",
    "for dataset, path_pattern in obs_datasets.items():\n",
    "    print(f\"   📊 {dataset}: {path_pattern}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Model-Agnostic Preprocessing Pipeline\n",
    "The model-agnostic preprocessing represents the core innovation of CONFLUENCE's data management philosophy. This stage creates standardized, model-independent data products that serve as the foundation for all subsequent modeling activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL-AGNOSTIC PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 3c: Model-Agnostic Preprocessing Pipeline ===\")\n",
    "print(\"Creating standardized data products for multi-model applications...\")\n",
    "\n",
    "print(f\"\\n🔧 Preprocessing Components:\")\n",
    "print(f\"   1. Spatial Remapping (EASYMORE)\")\n",
    "print(f\"   2. Zonal Statistics (rasterstats)\")\n",
    "print(f\"   3. Temporal Alignment\") \n",
    "print(f\"   4. Quality Control\")\n",
    "print(f\"   5. Format Standardization\")\n",
    "\n",
    "# Execute model-agnostic preprocessing\n",
    "print(f\"\\n⚙️  Executing model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"✅ Model-agnostic preprocessing complete\")\n",
    "\n",
    "# Expected outputs\n",
    "agnostic_outputs = {\n",
    "    \"Basin-averaged forcing\": \"forcing/basin_averaged_data/\",\n",
    "    \"HRU attribute table\": \"attributes/hru_attributes.csv\",\n",
    "    \"Spatial mapping files\": \"forcing/spatial_mapping/\",\n",
    "    \"Quality control reports\": \"forcing/qc_reports/\"\n",
    "}\n",
    "\n",
    "print(f\"\\n📁 Model-Agnostic Products:\")\n",
    "for product, location in agnostic_outputs.items():\n",
    "    print(f\"   📦 {product}: {location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: Model-Specific Preprocessing\n",
    "Model-specific preprocessing translates the standardized model-agnostic products into the formats and configurations required by individual models. This stage maintains the scientific benefits of standardized inputs while accommodating diverse model requirements.\n",
    "\n",
    "Remapping of the forcing data and zonal statistics calculations for the geospatial attributes is performed in one model-agnostic pre-processing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL-SPECIFIC PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 3d: Model-Specific Preprocessing ===\")\n",
    "print(\"Translating standardized products to model-specific formats...\")\n",
    "\n",
    "print(f\"\\n🎯 Target Model Configuration:\")\n",
    "print(f\"   Model: {config_dict['HYDROLOGICAL_MODEL']}\")\n",
    "\n",
    "# Execute model-specific preprocessing\n",
    "print(f\"\\n🔧 Executing SUMMA-specific preprocessing...\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"✅ Model-specific preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Instantiation and Process-Based Simulation\n",
    "## Scientific Context\n",
    "Model instantiation represents the critical transition from static data preparation to dynamic process simulation. This step transforms spatially-distributed inputs and temporally-varying forcing into evolving hydrological states through the explicit representation of physical processes.\n",
    "In process-based hydrological modeling, we solve coupled differential equations representing:\n",
    "\n",
    "- Energy Balance: Net radiation partitioning between sensible, latent, and ground heat fluxes\n",
    "- Water Balance: Precipitation partitioning among interception, infiltration, evapotranspiration, and runoff\n",
    "- Snow Physics: Accumulation, metamorphism, and melt processes with explicit energy considerations\n",
    "- Soil Hydrology: Infiltration, redistribution, and drainage through layered soil profiles\n",
    "- Vegetation Dynamics: Canopy interception, transpiration, and phenological controls\n",
    "\n",
    "The SUMMA (Structure for Unifying Multiple Modeling Alternatives) framework enables systematic evaluation of process representations, making it ideal for scientific hypothesis testing and model physics assessment.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE manages model execution through several integrated components:\n",
    "\n",
    "- Workflow Orchestration: Automated sequencing of model initialization, spinup, and main simulation\n",
    "- Configuration Management: Translation of scientific decisions into model-specific control files\n",
    "- Execution Monitoring: Real-time tracking of model progress and error detection\n",
    "- Output Organization: Systematic storage and cataloging of simulation results\n",
    "- Quality Assurance: Automated checks for mass balance closure and physical realism\n",
    "\n",
    "This framework ensures that model execution is reproducible, traceable, and scientifically rigorous, while handling the computational complexity behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: MODEL INSTANTIATION AND PROCESS-BASED SIMULATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Step 4: Model Instantiation and Process-Based Simulation ===\")\n",
    "\n",
    "# Display model configuration\n",
    "print(f\"\\n⚙️  Model Execution Configuration:\")\n",
    "print(f\"   Model: {config_dict['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"   Executable: {config_dict.get('SUMMA_EXE', 'summa.exe')}\")\n",
    "print(f\"   Domain: {config_dict['DOMAIN_NAME']} (1 HRU)\")\n",
    "print(f\"   Simulation period: {config_dict['EXPERIMENT_TIME_START']} to {config_dict['EXPERIMENT_TIME_END']}\")\n",
    "print(f\"   Spinup period: {config_dict.get('SPINUP_PERIOD', 'Not specified')}\")\n",
    "\n",
    "# Run models\n",
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation and Process Validation\n",
    "## Scientific Context\n",
    "Model evaluation represents the critical test of whether our process-based simulation captures the essential physics of the real-world system. Effective evaluation requires:\n",
    "\n",
    "- Multi-Variable Assessment: Testing multiple aspects of the hydrological system to avoid equifinality and ensure robust process representation\n",
    "- Temporal Pattern Analysis: Evaluating both magnitude and timing of hydrological responses across seasonal cycles\n",
    "- Process-Specific Metrics: Using evaluation criteria that reflect the underlying physics being tested\n",
    "- Uncertainty Quantification: Understanding both observational and model uncertainty in performance assessment\n",
    "\n",
    "For point-scale modeling, we focus on direct process validation where observations closely match the spatial and temporal scales of model representation. The Paradise SNOTEL station provides exceptional validation opportunities with co-located snow water equivalent and multi-depth soil moisture observations.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE's evaluation framework emphasizes scientific interpretation over simple statistical metrics:\n",
    "\n",
    "- Physically-Informed Metrics: Performance measures that reflect process understanding\n",
    "- Multi-Scale Temporal Analysis: Evaluation across daily, seasonal, and inter-annual time scales\n",
    "- Process Attribution: Linking performance to specific model physics and parameter values\n",
    "- Comparative Context: Benchmarking against observational uncertainty and simple baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5A: SNOW WATER EQUIVALENT EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 5a: Snow Water Equivalent Evaluation ===\")\n",
    "print(\"Validating snow accumulation and melt physics against SNOTEL observations...\")\n",
    "\n",
    "# Load simulation data\n",
    "sim_dir = confluence.project_dir / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\"\n",
    "daily_output_path = sim_dir / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "if daily_output_path.exists():\n",
    "    # Load and prepare evaluation dataset\n",
    "    ds = xr.open_dataset(daily_output_path)\n",
    "    \n",
    "    # Skip spinup period\n",
    "    start_year = ds.time.dt.year.min().values + 1\n",
    "    spinup_end = f\"{start_year}-01-01\"\n",
    "    time_mask = ds.time >= pd.to_datetime(spinup_end)\n",
    "    evaluation_data = ds.isel(time=time_mask)\n",
    "\n",
    "# Load observed SWE data\n",
    "obs_swe_path = confluence.project_dir / \"observations\" / \"snow\" / \"snotel\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "print(f\"\\n📊 Loading observed SWE data...\")\n",
    "print(f\"   Source: SNOTEL Station {config_dict.get('SNOTEL_STATION', 'Unknown')}\")\n",
    "print(f\"   File: {obs_swe_path}\")\n",
    "\n",
    "\n",
    "if obs_swe_path.exists():\n",
    "    # Load observed data\n",
    "    obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'], dayfirst=True)\n",
    "    obs_swe.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Ensure proper datetime index\n",
    "    if not isinstance(obs_swe.index, pd.DatetimeIndex):\n",
    "        obs_swe.index = pd.to_datetime(obs_swe.index)\n",
    "    \n",
    "    print(f\"   ✅ Observed data loaded\")\n",
    "    print(f\"   Period: {obs_swe.index.min()} to {obs_swe.index.max()}\")\n",
    "    print(f\"   Records: {len(obs_swe)} observations\")\n",
    "    print(f\"   SWE column: {obs_swe.columns.tolist()}\")\n",
    "    \n",
    "    # Extract simulated SWE\n",
    "    sim_swe = evaluation_data['scalarSWE'].to_pandas()\n",
    "    print(f\"\\n   ✅ Simulated data extracted\")\n",
    "    print(f\"   Period: {sim_swe.index.min()} to {sim_swe.index.max()}\")\n",
    "    print(f\"   Records: {len(sim_swe)} time steps\")\n",
    "    \n",
    "    # Find common period and align data\n",
    "    start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "    end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "    \n",
    "    print(f\"\\n🔄 Data Alignment:\")\n",
    "    print(f\"   Common period: {start_date} to {end_date}\")\n",
    "    print(f\"   Duration: {(end_date - start_date).days} days\")\n",
    "    \n",
    "    # Resample to daily and filter to common period\n",
    "    obs_daily = obs_swe.resample('D').mean().loc[start_date:end_date]\n",
    "    sim_daily = sim_swe.resample('D').mean().loc[start_date:end_date]\n",
    "    \n",
    "    # Handle different column names for SWE\n",
    "    if 'SWE' in obs_daily.columns:\n",
    "        obs_values = obs_daily['SWE']\n",
    "    elif 'swe' in obs_daily.columns:\n",
    "        obs_values = obs_daily['swe']\n",
    "    else:\n",
    "        # Use first column\n",
    "        obs_values = obs_daily.iloc[:, 0]\n",
    "        print(f\"   Using column '{obs_daily.columns[0]}' for observed SWE\")\n",
    "    \n",
    "    # Convert sim_daily to Series if it's a DataFrame\n",
    "    if isinstance(sim_daily, pd.DataFrame):\n",
    "        if len(sim_daily.columns) == 1:\n",
    "            sim_daily = sim_daily.iloc[:, 0]  # Extract the single column as Series\n",
    "        else:\n",
    "            print(f\"   Warning: Multiple columns in simulated data: {sim_daily.columns.tolist()}\")\n",
    "            sim_daily = sim_daily.iloc[:, 0]  # Use first column\n",
    "            print(f\"   Using column '{sim_daily.columns[0]}' for simulated SWE\")\n",
    "    \n",
    "    # Remove NaN values for metrics calculation\n",
    "    valid_mask = ~(obs_values.isna() | sim_daily.isna())\n",
    "    obs_valid = obs_values[valid_mask]\n",
    "    sim_valid = sim_daily[valid_mask]\n",
    "    \n",
    "    print(f\"   Valid paired observations: {len(obs_valid)} days\")\n",
    "    \n",
    "    # Calculate comprehensive performance metrics\n",
    "    print(f\"\\n📊 Snow Water Equivalent Performance Metrics:\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "    bias = (sim_valid - obs_valid).mean()\n",
    "    mae = np.abs(obs_valid - sim_valid).mean()\n",
    "    corr = obs_valid.corr(sim_valid)\n",
    "    \n",
    "    # Percent bias\n",
    "    pbias = 100 * bias / obs_valid.mean()\n",
    "    \n",
    "    # Nash-Sutcliffe Efficiency\n",
    "    nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "    \n",
    "    # Kling-Gupta Efficiency\n",
    "    kge_corr = obs_valid.corr(sim_valid)\n",
    "    kge_bias = sim_valid.mean() / obs_valid.mean()\n",
    "    kge_var = (sim_valid.std() / obs_valid.std())\n",
    "    kge = 1 - np.sqrt((kge_corr - 1)**2 + (kge_bias - 1)**2 + (kge_var - 1)**2)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"   📈 Root Mean Square Error (RMSE): {rmse:.2f} mm\")\n",
    "    print(f\"   📈 Mean Absolute Error (MAE): {mae:.2f} mm\")\n",
    "    print(f\"   📈 Bias: {bias:+.2f} mm ({pbias:+.1f}%)\")\n",
    "    print(f\"   📈 Correlation: {corr:.3f}\")\n",
    "    print(f\"   📈 Nash-Sutcliffe Efficiency: {nse:.3f}\")\n",
    "    print(f\"   📈 Kling-Gupta Efficiency: {kge:.3f}\")\n",
    "    \n",
    "    # Snow-specific metrics\n",
    "    print(f\"\\n❄️  Snow-Specific Performance Assessment:\")\n",
    "    \n",
    "    # Peak SWE analysis\n",
    "    obs_peak = obs_valid.max()\n",
    "    sim_peak = sim_valid.max()\n",
    "    peak_bias = sim_peak - obs_peak\n",
    "    peak_pbias = 100 * peak_bias / obs_peak\n",
    "    \n",
    "    print(f\"   🏔️  Peak SWE:\")\n",
    "    print(f\"       Observed: {obs_peak:.1f} mm\")\n",
    "    print(f\"       Simulated: {sim_peak:.1f} mm\")\n",
    "    print(f\"       Bias: {peak_bias:+.1f} mm ({peak_pbias:+.1f}%)\")\n",
    "    \n",
    "    # Snow season timing\n",
    "    obs_peak_date = obs_valid.idxmax()\n",
    "    sim_peak_date = sim_valid.idxmax()\n",
    "    timing_diff = (sim_peak_date - obs_peak_date).days\n",
    "    \n",
    "    print(f\"   📅 Peak Timing:\")\n",
    "    print(f\"       Observed peak: {obs_peak_date.strftime('%B %d, %Y')}\")\n",
    "    print(f\"       Simulated peak: {sim_peak_date.strftime('%B %d, %Y')}\")\n",
    "    print(f\"       Timing difference: {timing_diff:+d} days\")\n",
    "    \n",
    "    # Snow season length\n",
    "    snow_threshold = 10  # mm\n",
    "    obs_snow_days = (obs_valid > snow_threshold).sum()\n",
    "    sim_snow_days = (sim_valid > snow_threshold).sum()\n",
    "    \n",
    "    print(f\"   ⏰ Snow Season (SWE > {snow_threshold} mm):\")\n",
    "    print(f\"       Observed: {obs_snow_days} days\")\n",
    "    print(f\"       Simulated: {sim_snow_days} days\")\n",
    "    print(f\"       Difference: {sim_snow_days - obs_snow_days:+d} days\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    print(f\"\\n📈 Creating SWE comparison visualization...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Time series plot\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(obs_daily.index, obs_values, 'o-', label='Observed', \n",
    "             color='black', alpha=0.7, markersize=3, linewidth=1)\n",
    "    ax1.plot(sim_daily.index, sim_daily, '-', label='Simulated', \n",
    "             color='blue', linewidth=2)\n",
    "    ax1.set_title('Snow Water Equivalent Time Series', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('SWE (mm)', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add performance metrics as text\n",
    "    metrics_text = f'RMSE: {rmse:.1f} mm\\nBias: {bias:+.1f} mm\\nCorr: {corr:.3f}\\nNSE: {nse:.3f}'\n",
    "    ax1.text(0.02, 0.95, metrics_text, transform=ax1.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.scatter(obs_valid, sim_valid, alpha=0.6, c='blue', s=20)\n",
    "    max_val = max(obs_valid.max(), sim_valid.max())\n",
    "    ax2.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "    ax2.set_xlabel('Observed SWE (mm)', fontsize=11)\n",
    "    ax2.set_ylabel('Simulated SWE (mm)', fontsize=11)\n",
    "    ax2.set_title('Observed vs. Simulated SWE', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Seasonal cycle\n",
    "    ax3 = axes[1, 0]\n",
    "    obs_monthly = obs_values.groupby(obs_values.index.month).mean()\n",
    "    sim_monthly = sim_daily.groupby(sim_daily.index.month).mean()\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    ax3.plot(range(1, 13), obs_monthly, 'o-', label='Observed', color='black', linewidth=2)\n",
    "    ax3.plot(range(1, 13), sim_monthly, 'o-', label='Simulated', color='blue', linewidth=2)\n",
    "    ax3.set_xticks(range(1, 13))\n",
    "    ax3.set_xticklabels(months, rotation=45)\n",
    "    ax3.set_ylabel('Mean SWE (mm)', fontsize=11)\n",
    "    ax3.set_title('Seasonal Cycle', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Residuals over time\n",
    "    ax4 = axes[1, 1]\n",
    "    residuals = sim_valid - obs_valid\n",
    "    ax4.scatter(obs_valid.index, residuals, alpha=0.6, c='red', s=15)\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax4.axhline(y=residuals.std(), color='red', linestyle='--', alpha=0.5, label=f'±1σ ({residuals.std():.1f} mm)')\n",
    "    ax4.axhline(y=-residuals.std(), color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.set_ylabel('Residuals (Sim - Obs) [mm]', fontsize=11)\n",
    "    ax4.set_title('Model Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.suptitle(f'Snow Water Equivalent Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Observed SWE data not found at {obs_swe_path}\")\n",
    "    print(\"   Cannot perform SWE evaluation\")\n",
    "    print(\"   Check SNOTEL data processing step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b: Soil Moisture Profile Evaluation\n",
    "Soil moisture evaluation tests the model's representation of vadose zone processes, including infiltration, drainage, and vertical redistribution. Multi-depth observations provide unprecedented validation opportunities for soil physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5B: SOIL MOISTURE PROFILE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Step 5b: Soil Moisture Profile Evaluation ===\")\n",
    "print(\"Validating soil hydrology and vadose zone processes...\")\n",
    "\n",
    "# Load observed soil moisture data\n",
    "obs_sm_path = confluence.project_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_sm_processed.csv\"\n",
    "\n",
    "print(f\"\\n📊 Loading observed soil moisture data...\")\n",
    "print(f\"   Source: ISMN/SNOTEL soil moisture sensors\")\n",
    "print(f\"   File: {obs_sm_path}\")\n",
    "\n",
    "if obs_sm_path.exists():\n",
    "    # Load observed data\n",
    "    obs_sm = pd.read_csv(obs_sm_path, parse_dates=['timestamp'])\n",
    "    obs_sm.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Ensure proper datetime index\n",
    "    if not isinstance(obs_sm.index, pd.DatetimeIndex):\n",
    "        obs_sm.index = pd.to_datetime(obs_sm.index)\n",
    "    \n",
    "    print(f\"   ✅ Observed data loaded\")\n",
    "    print(f\"   Period: {obs_sm.index.min()} to {obs_sm.index.max()}\")\n",
    "    print(f\"   Records: {len(obs_sm)} observations\")\n",
    "    \n",
    "    # Identify observed depth columns\n",
    "    obs_depth_cols = [col for col in obs_sm.columns if col.startswith('sm_')]\n",
    "    print(f\"   Available depths: {obs_depth_cols}\")\n",
    "    \n",
    "    # Extract observed depth values\n",
    "    obs_depths = []\n",
    "    for depth_col in obs_depth_cols:\n",
    "        # Extract depth from column name (e.g., 'sm_0.0508_0.0508' -> 0.0508)\n",
    "        depth_str = depth_col.split('_')[1]\n",
    "        obs_depths.append(float(depth_str))\n",
    "    \n",
    "    print(f\"   Observation depths: {[f'{d:.4f}m' for d in obs_depths]}\")\n",
    "    \n",
    "    # Extract simulated soil moisture\n",
    "    if 'mLayerVolFracLiq' in evaluation_data.data_vars:\n",
    "        sim_sm = evaluation_data['mLayerVolFracLiq']\n",
    "        sim_depths = evaluation_data['mLayerDepth']\n",
    "        \n",
    "        print(f\"\\n   ✅ Simulated data extracted\")\n",
    "        print(f\"   Simulation layers: {sim_sm.sizes['midToto']}\")\n",
    "        print(f\"   Period: {sim_sm.time.min().values} to {sim_sm.time.max().values}\")\n",
    "        \n",
    "        # Calculate representative layer depths\n",
    "        mean_layer_depths = sim_depths.mean(dim='time')\n",
    "        valid_layers = mean_layer_depths > 0  # Filter out invalid layers\n",
    "        \n",
    "        print(f\"   Valid simulation layers: {valid_layers.sum().values}\")\n",
    "        \n",
    "        # Find common period\n",
    "        start_date = max(obs_sm.index.min(), pd.to_datetime(sim_sm.time.min().values))\n",
    "        end_date = min(obs_sm.index.max(), pd.to_datetime(sim_sm.time.max().values))\n",
    "        \n",
    "        print(f\"\\n🔄 Data Alignment:\")\n",
    "        print(f\"   Common period: {start_date} to {end_date}\")\n",
    "        print(f\"   Duration: {(end_date - start_date).days} days\")\n",
    "        \n",
    "        # Filter to common period\n",
    "        obs_period = obs_sm.loc[start_date:end_date]\n",
    "        sim_time_mask = (sim_sm.time >= start_date) & (sim_sm.time <= end_date)\n",
    "        sim_period = sim_sm.isel(time=sim_time_mask)\n",
    "        sim_depths_period = sim_depths.isel(time=sim_time_mask)\n",
    "        \n",
    "        # Create comprehensive multi-depth evaluation\n",
    "        print(f\"\\n📊 Multi-Depth Soil Moisture Performance:\")\n",
    "        \n",
    "        n_depths = len(obs_depth_cols)\n",
    "        depth_results = {}\n",
    "        \n",
    "        # Analyze each observed depth\n",
    "        for i, (depth_col, obs_depth) in enumerate(zip(obs_depth_cols, obs_depths)):\n",
    "            print(f\"\\n   🎯 Depth {i+1}: {obs_depth:.4f}m ({depth_col})\")\n",
    "            \n",
    "            # Find closest simulated layer\n",
    "            mean_depths = sim_depths_period.mean(dim='time')\n",
    "            valid_mask = mean_depths > 0\n",
    "            \n",
    "            if valid_mask.sum() > 0:\n",
    "                valid_mean_depths = mean_depths.where(valid_mask)\n",
    "                depth_differences = np.abs(valid_mean_depths - obs_depth)\n",
    "                closest_layer_idx = depth_differences.argmin().values\n",
    "                closest_layer_depth = valid_mean_depths[closest_layer_idx].values\n",
    "                \n",
    "                \n",
    "                # Extract data for this layer\n",
    "                obs_layer = obs_period[depth_col]\n",
    "                sim_layer = sim_period.isel(midToto=closest_layer_idx, hru=0)\n",
    "                \n",
    "                # Convert to pandas for easier handling\n",
    "                sim_layer_ts = sim_layer.to_pandas()\n",
    "                \n",
    "                # Resample to daily and align\n",
    "                obs_daily = obs_layer.resample('D').mean()\n",
    "                sim_daily = sim_layer_ts.resample('D').mean()\n",
    "                \n",
    "                # Remove invalid values (negative soil moisture indicates missing data)\n",
    "                sim_daily = sim_daily.where(sim_daily > -100)\n",
    "                \n",
    "                # Find valid paired data\n",
    "                valid_data_mask = ~(obs_daily.isna() | sim_daily.isna())\n",
    "                obs_valid = obs_daily[valid_data_mask]\n",
    "                sim_valid = sim_daily[valid_data_mask]\n",
    "                \n",
    "                if len(obs_valid) > 10:  # Require minimum data for meaningful evaluation\n",
    "                    # Calculate performance metrics\n",
    "                    rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                    bias = (sim_valid - obs_valid).mean()\n",
    "                    mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                    corr = obs_valid.corr(sim_valid)\n",
    "                    \n",
    "                    # Store results\n",
    "                    depth_results[obs_depth] = {\n",
    "                        'obs_valid': obs_valid,\n",
    "                        'sim_valid': sim_valid,\n",
    "                        'rmse': rmse,\n",
    "                        'bias': bias,\n",
    "                        'mae': mae,\n",
    "                        'corr': corr,\n",
    "                        'layer_idx': closest_layer_idx,\n",
    "                        'sim_depth': closest_layer_depth\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"       📈 RMSE: {rmse:.3f} m³/m³\")\n",
    "                    print(f\"       📈 Bias: {bias:+.3f} m³/m³\")\n",
    "                    print(f\"       📈 Correlation: {corr:.3f}\")\n",
    "                    print(f\"       📈 Valid pairs: {len(obs_valid)}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"       ⚠️  Insufficient valid data pairs ({len(obs_valid)})\")\n",
    "            else:\n",
    "                print(f\"       ❌ No valid simulation layers found\")\n",
    "        \n",
    "        # Create comprehensive visualization\n",
    "        if depth_results:\n",
    "            print(f\"\\n📈 Creating soil moisture profile evaluation...\")\n",
    "            \n",
    "            n_depths = len(depth_results)\n",
    "            fig, axes = plt.subplots(n_depths, 2, figsize=(15, 4*n_depths))\n",
    "            \n",
    "            if n_depths == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for i, (obs_depth, results) in enumerate(depth_results.items()):\n",
    "                # Time series plot\n",
    "                ax_ts = axes[i, 0]\n",
    "                ax_ts.plot(results['obs_valid'].index, results['obs_valid'], 'o-', \n",
    "                          label=f'Observed ({obs_depth:.4f}m)', \n",
    "                          color='black', alpha=0.7, markersize=2, linewidth=1)\n",
    "                ax_ts.plot(results['sim_valid'].index, results['sim_valid'], '-', \n",
    "                          label=f'Simulated (L{results[\"layer_idx\"]}, {results[\"sim_depth\"]}m)', \n",
    "                          color='blue', linewidth=2)\n",
    "                \n",
    "                ax_ts.set_title(f'Soil Moisture at {obs_depth:.4f}m depth', fontsize=11, fontweight='bold')\n",
    "                ax_ts.set_ylabel('Soil Moisture (m³/m³)', fontsize=10)\n",
    "                ax_ts.grid(True, alpha=0.3)\n",
    "                ax_ts.legend(fontsize=9)\n",
    "                \n",
    "                # Add metrics\n",
    "                metrics_text = (f\"RMSE: {results['rmse']:.3f}\\n\"\n",
    "                               f\"Bias: {results['bias']:+.3f}\\n\"\n",
    "                               f\"Corr: {results['corr']:.3f}\")\n",
    "                ax_ts.text(0.02, 0.95, metrics_text, transform=ax_ts.transAxes,\n",
    "                          bbox=dict(facecolor='white', alpha=0.8), fontsize=9, verticalalignment='top')\n",
    "                \n",
    "                # Scatter plot\n",
    "                ax_scatter = axes[i, 1]\n",
    "                ax_scatter.scatter(results['obs_valid'], results['sim_valid'], \n",
    "                                  alpha=0.6, c='blue', s=15)\n",
    "                \n",
    "                # 1:1 line\n",
    "                min_val = min(results['obs_valid'].min(), results['sim_valid'].min())\n",
    "                max_val = max(results['obs_valid'].max(), results['sim_valid'].max())\n",
    "                ax_scatter.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                               label='1:1 line', alpha=0.7)\n",
    "                \n",
    "                ax_scatter.set_xlabel('Observed SM (m³/m³)', fontsize=10)\n",
    "                ax_scatter.set_ylabel('Simulated SM (m³/m³)', fontsize=10)\n",
    "                ax_scatter.set_title(f'Obs vs Sim at {obs_depth:.4f}m', fontsize=11, fontweight='bold')\n",
    "                ax_scatter.grid(True, alpha=0.3)\n",
    "                ax_scatter.legend(fontsize=9)\n",
    "                ax_scatter.set_aspect('equal', adjustable='box')\n",
    "            \n",
    "            plt.suptitle(f'Soil Moisture Profile Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Summary performance assessment\n",
    "            print(f\"\\n🔬 Soil Moisture Profile Performance Summary:\")\n",
    "            \n",
    "            avg_rmse = np.mean([r['rmse'] for r in depth_results.values()])\n",
    "            avg_corr = np.mean([r['corr'] for r in depth_results.values()])\n",
    "            avg_bias = np.mean([r['bias'] for r in depth_results.values()])\n",
    "            \n",
    "            print(f\"   📊 Profile-averaged metrics:\")\n",
    "            print(f\"       Average RMSE: {avg_rmse:.3f} m³/m³\")\n",
    "            print(f\"       Average correlation: {avg_corr:.3f}\")\n",
    "            print(f\"       Average bias: {avg_bias:+.3f} m³/m³\")\n",
    "            \n",
    "            # Depth-dependent performance analysis\n",
    "            print(f\"\\n   🌱 Depth-Dependent Performance:\")\n",
    "            for obs_depth, results in depth_results.items():\n",
    "                if results['corr'] > 0.6 and results['rmse'] < 0.1:\n",
    "                    performance = \"Good\"\n",
    "                elif results['corr'] > 0.4 and results['rmse'] < 0.15:\n",
    "                    performance = \"Fair\"\n",
    "                else:\n",
    "                    performance = \"Poor\"\n",
    "                \n",
    "                print(f\"       {obs_depth:.4f}m: {performance} (r={results['corr']:.3f}, RMSE={results['rmse']:.3f})\")\n",
    "            \n",
    "            # Scientific interpretation\n",
    "            print(f\"\\n🎯 Scientific Interpretation:\")\n",
    "            \n",
    "            if avg_corr > 0.6:\n",
    "                print(f\"   ✅ Model captures temporal dynamics well\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Temporal dynamics need improvement\")\n",
    "            \n",
    "            if abs(avg_bias) < 0.05:\n",
    "                print(f\"   ✅ Minimal systematic bias in soil moisture\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Systematic bias detected ({avg_bias:+.3f} m³/m³)\")\n",
    "            \n",
    "            if avg_rmse < 0.1:\n",
    "                print(f\"   ✅ Reasonable magnitude errors\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Large magnitude errors suggest parameter issues\")\n",
    "            \n",
    "            # Process insights\n",
    "            depth_performance = {depth: results['corr'] for depth, results in depth_results.items()}\n",
    "            best_depth = max(depth_performance, key=depth_performance.get)\n",
    "            worst_depth = min(depth_performance, key=depth_performance.get)\n",
    "            \n",
    "            print(f\"\\n   🔍 Process Insights:\")\n",
    "            print(f\"       Best performance: {best_depth:.4f}m (r={depth_performance[best_depth]:.3f})\")\n",
    "            print(f\"       Worst performance: {worst_depth:.4f}m (r={depth_performance[worst_depth]:.3f})\")\n",
    "            \n",
    "            if best_depth < 0.1:\n",
    "                print(f\"       → Surface processes well represented\")\n",
    "            elif best_depth > 0.5:\n",
    "                print(f\"       → Deep soil processes well represented\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ❌ No valid depth comparisons possible\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"   ❌ Simulated soil moisture data not found in outputs\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Observed soil moisture data not found at {obs_sm_path}\")\n",
    "    print(\"   Cannot perform soil moisture evaluation\")\n",
    "    print(\"   Check soil moisture data processing step\")\n",
    "\n",
    "# Close evaluation dataset\n",
    "evaluation_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Summary and Next Steps\n",
    "\n",
    "## Summary: Point-Scale Snow and Soil Process Validation\n",
    "\n",
    "This tutorial demonstrated the complete CONFLUENCE workflow for point-scale hydrological modeling, establishing fundamental principles for reproducible computational hydrology research.\n",
    "\n",
    "### 🎯 Key Achievements\n",
    "\n",
    "- **✅ Reproducible Workflow Management**: Configuration-driven experiments with complete provenance tracking\n",
    "- **✅ Model-Agnostic Preprocessing**: Standardized data pipeline enabling true model physics comparisons  \n",
    "- **✅ Process-Based Simulation**: Explicit energy and water balance with SUMMA's modular physics\n",
    "- **✅ Multi-Variable Validation**: Co-located snow water equivalent and multi-depth soil moisture evaluation\n",
    "- **✅ Scientific Interpretation**: Performance assessment linked to underlying physical processes\n",
    "\n",
    "### 🔬 Scientific Process Validation\n",
    "\n",
    "- **Snow Physics**: Accumulation, metamorphism, and energy-balance driven melt processes\n",
    "- **Soil Hydrology**: Multi-depth moisture dynamics and vertical water redistribution\n",
    "- **Energy Balance**: Surface-atmosphere energy exchange and flux partitioning\n",
    "- **Temporal Dynamics**: Daily to seasonal process evolution and memory effects\n",
    "- **Physical Realism**: Mass and energy conservation with realistic state variable bounds\n",
    "\n",
    "### 🏗️ CONFLUENCE Framework Capabilities\n",
    "\n",
    "- **Modular Architecture**: Specialized managers for different workflow components\n",
    "- **Workflow Orchestration**: Automated step sequencing with restart capabilities\n",
    "- **Quality Assurance**: Built-in validation and error checking throughout pipeline\n",
    "- **Scalable Design**: Same framework supports point-scale through continental applications\n",
    "- **Research Continuity**: Foundation for distributed modeling and large-sample studies\n",
    "\n",
    "---\n",
    "\n",
    "## Looking Ahead: Tutorial 01b - Energy Flux Validation at FLUXNET Sites\n",
    "\n",
    "### 🌿 **Next Focus: Evapotranspiration and Energy Balance Processes**\n",
    "\n",
    "While Tutorial 01a focused on **snow and soil moisture dynamics** in a mountain environment, Tutorial 01b expands our process validation to **energy flux partitioning and evapotranspiration** using FLUXNET observations.\n",
    "\n",
    "### **Scientific Context for Tutorial 01b**\n",
    "\n",
    "**FLUXNET towers** provide exceptional validation opportunities for energy balance modeling through direct eddy covariance measurements of:\n",
    "\n",
    "- **Latent Heat Flux (LE)**: Direct measurement of evapotranspiration energy\n",
    "- **Sensible Heat Flux (H)**: Convective energy transfer to atmosphere  \n",
    "- **Net Radiation (Rn)**: Available energy driving surface processes\n",
    "- **Ground Heat Flux (G)**: Soil energy storage and conduction\n",
    "- **Carbon Fluxes (NEE)**: Ecosystem productivity and respiration\n",
    "\n",
    "### **Tutorial 01b Objectives**\n",
    "\n",
    "1. **Energy Balance Closure**: Validate SUMMA's energy partitioning against tower observations\n",
    "2. **ET Process Validation**: Test stomatal conductance, canopy interception, and soil evaporation\n",
    "3. **Vegetation Dynamics**: Evaluate LAI, phenology, and canopy process representations\n",
    "4. **Multi-Scale Temporal Analysis**: Assess performance from sub-daily to seasonal time scales\n",
    "5. **Cross-Biome Validation**: Demonstrate framework across forest, grassland, and agricultural sites\n",
    "\n",
    "### **CONFLUENCE Advantages for FLUXNET Analysis**\n",
    "\n",
    "- **Same Preprocessing Framework**: Model-agnostic pipeline serves both snow and flux validation\n",
    "- **Multi-Variable Evaluation**: Simultaneous assessment of energy, water, and carbon processes\n",
    "- **Process Attribution**: Link performance to specific model physics and parameter values\n",
    "- **Comparative Analysis**: Benchmarking across sites, seasons, and process representations\n",
    "\n",
    "### **Scientific Progression**\n",
    "\n",
    "| Tutorial | Process Focus | Validation Data | Environment | Model Complexity |\n",
    "|----------|---------------|-----------------|-------------|------------------|\n",
    "| **01a** | Snow & Soil Hydrology | SNOTEL (SWE, SM) | Mountain | Point-scale |\n",
    "| **01b** | Energy Balance & ET | FLUXNET (LE, H, Rn) | Multiple biomes | Point-scale |\n",
    "| **02** | Basin-scale Hydrology | Streamflow | Watershed | Spatial routing |\n",
    "| **02** | Domain-scale Hydrology | Streamflow | Large Domain | Thousands of basins + river segments |                                                               \n",
    "| **04** | Large-Sample Analysis | Multi-site | Global | Thousands of sites |\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to explore energy flux validation?** → **[Tutorial 01b: Point-Scale Energy Balance Validation](./01b_point_scale_fluxnet.ipynb)**\n",
    "\n",
    "The process validation foundation established here directly enables the energy flux analysis in Tutorial 01b, building toward comprehensive model evaluation across the full spectrum of hydrological processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scienv)",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
