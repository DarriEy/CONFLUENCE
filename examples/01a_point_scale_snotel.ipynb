{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 1: Point-Scale Workflow (SNOTEL Example)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to CONFLUENCE, a framework designed to advance reproducible computational hydrology through standardized, modular workflows that seamlessly scale from point-scale process validation to continental-scale large-sample studies. The CONFLUENCE philosophy centers on model-agnostic data preprocessing, configuration-driven experiments, and systematic workflow orchestration to ensure that hydrological research is transparent, reproducible, and scientifically rigorous. This tutorial series is organized into four progressive sections: **Section 01** focuses on point-scale modeling and vertical flux simulations to establish fundamental process understanding; **Section 02** expands to basin-scale modeling with various spatial discretization schemes and routing processes; **Section 03** advances to regional and multi-watershed simulations extending to continental scales; and **Section 04** demonstrates large-sample studies across extensive datasets for comparative hydrology research. This first notebook (Tutorial 01a) follows a systematic 5-step workflow: (1) experiment initialization and reproducible setup, (2) geospatial domain definition and spatial discretization, (3) input data preprocessing through the model-agnostic framework, (4) model instantiation and process-based simulation, and (5) model evaluation and process validation, establishing the foundational principles that will scale throughout the entire tutorial series.\n",
    "\n",
    "### The Scientific Importance of Point-Scale Modeling\n",
    "\n",
    "Point-scale modeling is the building block of distributed hydrological modeling, where vertical energy and water balance processes are simulated at a single location without the complexities of lateral flow routing. This approach is scientifically valuable for several reasons:\n",
    "\n",
    "1. **Process Understanding**: Point-scale simulations isolate vertical processes (precipitation, evapotranspiration, snowmelt, infiltration, and soil moisture dynamics), allowing researchers to evaluate model physics without confounding effects from spatial heterogeneity and routing processes.\n",
    "\n",
    "2. **Model Validation**: Single-point simulations provide controlled conditions for testing model assumptions and parameter sensitivity, serving as a prerequisite for successful distributed modeling applications.\n",
    "\n",
    "3. **Observational Constraints**: Point-scale modeling leverages high-quality, long-term observational datasets to constrain model parameters and validate process representations before scaling to larger, distributed domains.\n",
    "\n",
    "### Case Study: Paradise SNOTEL Station\n",
    "\n",
    "This tutorial demonstrates  point-scale simulations in CONFLUENCE using the Paradise SNOTEL station (ID: 602) in Washington State. Located at 1,630 m elevation in the Cascade Range, this site represents a transitional snow climate with observations of both Snow Water Equivilalent (SWE) and Soil Moisture (SM) at four depths.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. **Understand CONFLUENCE architecture**: Learn how the modular framework manages complex hydrological modeling workflows\n",
    "2. **Configure point-scale simulations**: Set up CONFLUENCE for single-point SUMMA simulations \n",
    "3. **Evaluate model performance**: Compare simulated and observed snow water equivalent and soil moisture using quantitative metrics\n",
    "\n",
    "This foundation in point-scale modeling prepares you for more complex distributed modeling applications while building confidence in model physics and parameter estimation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Experiment Initialization and Reproducible Workflow Setup\n",
    "\n",
    "## Scientific Context\n",
    "Reproducible hydrological research requires systematic organization of data, code, and results. Modern computational hydrology faces several challenges:\n",
    "\n",
    "- Provenance Tracking: Understanding how results were generated, which data were used, and what decisions were made\n",
    "- Experiment Scaling: Moving from single-site studies to large sample investigations across hundreds of watersheds\n",
    "- Collaborative Research: Enabling multiple researchers to build upon previous work\n",
    "- Long-term Maintenance: Ensuring experiments remain accessible and reproducible years later\n",
    "\n",
    "Point-scale modeling serves as the foundation for larger distributed studies. The organizational principles established here directly enable the large-sample hydrological studies we'll explore in Tutorial 4, where these same workflows scale across thousands of sites simultaneously.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE addresses reproducibility through three core principles:\n",
    "\n",
    "- Configuration-Driven Workflows: All experiment settings are stored in human-readable YAML files that serve as complete experiment documentation\n",
    "- Standardized Directory Structure: Consistent organization enables automated processing and easy navigation across studies\n",
    "- Modular Architecture: Specialized managers handle different workflow components, making the system maintainable and extensible\n",
    "\n",
    "The framework automatically creates detailed logs, maintains data provenance, and ensures that any experiment can be reproduced from its configuration file alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the libraries we'll need in this tutorial\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=== CONFLUENCE Tutorial - 1: Point-Scale Workflow - SNOTEL example ===\")\n",
    "print(f\"CONFLUENCE path: {confluence_path}\")\n",
    "print(f\"Tutorial started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Generation and Experiment Design\n",
    "The configuration file serves as the complete experimental protocol, documenting every decision. This approach ensures that experiments are:\n",
    "\n",
    "- Fully documented: Every setting is explicit and versioned\n",
    "- Easily modified: Parameter sensitivity studies require only config changes\n",
    "- Shareable: Colleagues can reproduce exact experiments\n",
    "- Scalable: The same structure works for single sites or continental studies\n",
    "- Repeatable: The same structure can be repeated to facilitate large sample studies\n",
    "\n",
    "## SNOTEL Point Scale Experiment Setup\n",
    "To get our point scale model setup we'll make appropriate changes to the default point scale config template in ../0_config_files/config_point_template.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths \n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data') \n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# Load template configuration for point-scale modeling\n",
    "config_template_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "\n",
    "# Read and customize configuration\n",
    "with open(config_template_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update paths and core experiment settings\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "config_dict['BOUNDING_BOX_COORDS'] = str('46.781/-121.751/46.799/-121.749')\n",
    "config_dict['POUR_POINT_COORDS'] = str('46.78/-121.75')\n",
    "config_dict['DOMAIN_DEFINITION_METHOD'] = \"point\"\n",
    "config_dict['DOWNLOAD_SNOTEL'] = 'True'\n",
    "config_dict['HYDROLOGICAL_MODEL'] = \"SUMMA\"\n",
    "config_dict['FORCING_DATASET'] = \"ERA5\"\n",
    "config_dict['SUPPLEMENT_FORCING'] = 'True'\n",
    "config_dict['EXPERIMENT_TIME_START'] = \"1981-01-01 01:00\"\n",
    "config_dict['EXPERIMENT_TIME_END'] = \"2019-12-31 23:00\"\n",
    "config_dict['DOMAIN_NAME'] = 'paradise'\n",
    "config_dict['EXPERIMENT_ID'] = 'point_scale_tutorial'\n",
    "\n",
    "# Save configuration \n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_notebook.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ Configuration saved to: {temp_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFLUENCE System Initialization\n",
    "Now we are ready to create an instantiation of CONFLUENCE using our new configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CONFLUENCE with the experiment configuration\n",
    "confluence = CONFLUENCE(temp_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure Creation and Organization\n",
    "We can now use CONFLUENCE to setup a project directory for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project structure\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "print(f\"\\n📁 Project root created: {project_dir}\")\n",
    "\n",
    "# Create spatial reference point (SNOTEL station location)\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "print(f\"📍 Pour point created: {pour_point_path}\")\n",
    "print(f\"   → Location: {config_dict['POUR_POINT_COORDS']} (Paradise SNOTEL)\")\n",
    "\n",
    "# Display the created directory structure\n",
    "print(f\"\\n=== Standardized Directory Structure ===\")\n",
    "\n",
    "def display_directory_tree(path, prefix=\"\", max_depth=2, current_depth=0):\n",
    "    \"\"\"Display directory tree with scientific context\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted([item for item in path.iterdir() if item.is_dir()])\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        # Add scientific context for key directories\n",
    "        if item.name == \"forcing\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Meteorological input data\")\n",
    "        elif item.name == \"observations\": \n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Validation datasets (SNOTEL, streamflow)\")\n",
    "        elif item.name == \"simulations\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Model output organized by experiment\")\n",
    "        elif item.name == \"attributes\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Geospatial characteristics (elevation, soil, land cover)\")\n",
    "        elif item.name == \"shapefiles\":\n",
    "            print(f\"{prefix}{'    ' if is_last else '│   '}    → Spatial domains and discretization\")\n",
    "            \n",
    "        if current_depth < max_depth - 1:\n",
    "            extension = \"    \" if is_last else \"│   \"\n",
    "            display_directory_tree(item, prefix + extension, max_depth, current_depth + 1)\n",
    "\n",
    "display_directory_tree(project_dir, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Geospatial Domain Definition and Spatial Discretization\n",
    "\n",
    "## Scientific Context\n",
    "Spatial representation is fundamental to hydrological modeling, determining how we conceptualize the landscape and partition it into computational units. The choice of spatial discretization profoundly affects:\n",
    "\n",
    "- Process Representation: How we capture spatial heterogeneity in climate, topography, vegetation, and soils\n",
    "- Model Complexity: The trade-off between process detail and computational efficiency\n",
    "- Scale Dependencies: How processes manifest differently at point, hillslope, and watershed scales\n",
    "- Validation Strategy: What observations are appropriate for model evaluation\n",
    "\n",
    "For point-scale modeling, we deliberately minimize spatial complexity to isolate vertical processes. This creates a controlled environment where energy and water balance physics can be evaluated without the confounding effects of lateral flow, spatial heterogeneity, or routing processes.\n",
    "\n",
    "The spatial representation we establish here contrasts with the distributed watersheds we'll explore in Tutorial 2, where complex topography drives spatial patterns in precipitation, radiation, and runoff generation.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE handles spatial domain definition through three components:\n",
    "\n",
    "- Attribute Acquisition: Systematic collection of the geospatial characteristics we need to configure our hydrological models (elevation, soil properties, land cover) using standardized datasets\n",
    "- Domain Delineation: Creation of the primary computational boundary (Grouped Response Units - GRUs)\n",
    "- Domain Discretization: Subdivision into Hydrologic Response Units (HRUs) based on landscape similarity\n",
    "\n",
    "For point-scale studies, this process creates a minimal spatial representation:\n",
    "\n",
    "- Bounding Box: 0.001° × 0.001° square centered on station coordinates\n",
    "- Single GRU: One computational unit representing the station footprint\n",
    "- Single HRU: No further subdivision needed for point-scale physics\n",
    "\n",
    "The same framework scales seamlessly from this minimal representation to complex distributed watersheds with hundreds of HRUs as we'll explore in Tutorials 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Geospatial Attribute Acquisition\n",
    "Attributes provide the physical characteristics needed to parameterize model physics. Even for point-scale modeling, we need elevation, soil properties, and vegetation characteristics to constrain energy and water balance processes.\n",
    "\n",
    "CONFLUENCE uses [gistool (Keshavaraz et al., 2025](https://github.com/CH-Earth/gistool) to subset and aquire the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Acquire geospatial attributes\n",
    "print(f\"\\n⬇️  Acquiring attributes through gistool (Model Agnostic Framework)...\")\n",
    "print(\"   → This may take several minutes \")\n",
    "\n",
    "# confluence.managers['data'].acquire_attributes()\n",
    "\n",
    "print(\"✅ Attribute acquisition complete\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Domain Delineation (GRU Creation)\n",
    "Domain delineation creates the primary computational boundary. For point-scale modeling, this is simply a geometric square around our station location, but the same process scales to complex watershed delineation for distributed modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating primary computational boundary...\")\n",
    "\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "print(f\"✅ Domain delineation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c: Domain Discretization (HRU Creation)\n",
    "Discretization subdivides GRUs into Hydrologic Response Units based on landscape similarity. For point-scale modeling, we maintain a 1:1 relationship (1 GRU = 1 HRU), but this step demonstrates the framework that enables more complex spatial representations which we will encounter in later notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Creating Hydrologic Response Units for model execution...\")\n",
    "\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "print(f\"✅ Domain discretization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data\n",
    "hru_path = confluence.project_dir / \"shapefiles\" / \"catchment\" / f\"{config_dict['DOMAIN_NAME']}_HRUs_{config_dict['DOMAIN_DISCRETIZATION']}.shp\"\n",
    "hru_gdf = gpd.read_file(hru_path)\n",
    "\n",
    "# Create simple plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot HRU\n",
    "hru_gdf.plot(ax=ax, facecolor='lightblue', edgecolor='blue', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add station point\n",
    "station_coords = config_dict['POUR_POINT_COORDS'].split('/')\n",
    "station_lat, station_lon = float(station_coords[0]), float(station_coords[1])\n",
    "ax.scatter(station_lon, station_lat, c='red', s=100, marker='*', \n",
    "          label=f'Paradise SNOTEL\\n({station_lat:.4f}, {station_lon:.4f})', zorder=5)\n",
    "\n",
    "# Styling\n",
    "ax.set_title(f'Point-Scale Domain: {config_dict[\"DOMAIN_NAME\"].title()}', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Longitude (°)', fontsize=12)\n",
    "ax.set_ylabel('Latitude (°)', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add annotation\n",
    "ax.text(0.02, 0.98, f\"Area: ~0.012 km²\\n1 GRU, 1 HRU\\nPoint-scale representation\", \n",
    "       transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "       bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Spatial visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Input Data Preprocessing and Model-Agnostic Framework\n",
    "## Scientific Context\n",
    "Input data preprocessing represents a critical but often overlooked component of hydrological modeling that profoundly affects model performance and scientific conclusions. Traditional approaches tightly couple data preprocessing with specific models, creating several scientific and practical challenges:\n",
    "\n",
    "- Model Comparison Barriers: Different preprocessing approaches make it difficult to determine whether performance differences arise from model physics or data preparation\n",
    "- Reproducibility Issues: Model-specific preprocessing pipelines are often poorly documented and difficult to reproduce\n",
    "- Research Inefficiency: Duplicated preprocessing effort across modeling studies\n",
    "- Benchmarking Limitations: Inability to evaluate models against consistent baselines\n",
    "\n",
    "The Model-Agnostic Framework (MAF) philosophy addresses these challenges by separating data preparation from model execution, creating a standardized pipeline that serves multiple modeling applications.\n",
    "\n",
    "## CONFLUENCE Implementation Philosophy\n",
    "CONFLUENCE implements a two-stage preprocessing architecture:\n",
    "Stage 1: Model-Agnostic Preprocessing\n",
    "\n",
    "- Standardized Data Sources: Consistent meteorological and geospatial datasets\n",
    "- Unified Spatial Framework: Common geospatial operations across all models\n",
    "- Quality-Controlled Outputs: Standardized formats with documented provenance\n",
    "- Reusable Products: Same preprocessed data serves multiple models and analyses\n",
    "\n",
    "Stage 2: Model-Specific Preprocessing\n",
    "\n",
    "- Format Translation: Convert standardized outputs to model-required formats\n",
    "- Model Configuration: Apply model-specific parameter assignments and settings\n",
    "- Initialization: Prepare model-specific initial conditions and control files\n",
    "\n",
    "This separation enables true model intercomparison studies, automated benchmarking, and scalable research workflows that maintain scientific rigor while maximizing efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Meteorological Forcing Data Acquisition\n",
    "Meteorological forcing drives all hydrological models, making standardized acquisition critical for reproducible research. CONFLUENCE leverages the Model-Agnostic Framework's [datatool (Keshavarz et al., 2025)](https://github.com/CH-Earth/datatool) to access quality-controlled, globally-consistent datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acquiring standardized meteorological forcing through datatool...\")\n",
    "\n",
    "# confluence.managers['data'].acquire_forcings()\n",
    "\n",
    "print(\"✅ Forcing data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Observational Data Processing\n",
    "Observational data provides the ground truth for model evaluation. CONFLUENCE systematically acquires and processes multiple observation types, creating standardized validation datasets that support comprehensive model assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute observational data processing\n",
    "print(f\"\\n📥 Processing observational datasets...\")\n",
    "confluence.managers['data'].process_observed_data()\n",
    "\n",
    "print(\"✅ Observational data processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Model-Agnostic Preprocessing Pipeline\n",
    "The model-agnostic preprocessing represents the core innovation of CONFLUENCE's data management philosophy. This stage creates standardized, model-independent data products that serve as the foundation for all subsequent modeling activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\n⚙️  Executing model-agnostic preprocessing...\")\n",
    "\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"✅ Model-agnostic preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3d: Model-Specific Preprocessing\n",
    "Model-specific preprocessing translates the standardized model-agnostic products into the formats and configurations required by individual models. This stage maintains the scientific benefits of standardized inputs while accommodating diverse model requirements.\n",
    "\n",
    "Remapping of the forcing data and zonal statistics calculations for the geospatial attributes is performed in one model-agnostic pre-processing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\n🔧 Executing SUMMA-specific preprocessing...\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"✅ Model-specific preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Instantiation and Process-Based Simulation\n",
    "## Scientific Context\n",
    "Model instantiation represents the critical transition from static data preparation to dynamic process simulation. This step transforms spatially-distributed inputs and temporally-varying forcing into evolving hydrological states through the explicit representation of physical processes.\n",
    "In process-based hydrological modeling, we solve coupled differential equations representing:\n",
    "\n",
    "- Energy Balance: Net radiation partitioning between sensible, latent, and ground heat fluxes\n",
    "- Water Balance: Precipitation partitioning among interception, infiltration, evapotranspiration, and runoff\n",
    "- Snow Physics: Accumulation, metamorphism, and melt processes with explicit energy considerations\n",
    "- Soil Hydrology: Infiltration, redistribution, and drainage through layered soil profiles\n",
    "- Vegetation Dynamics: Canopy interception, transpiration, and phenological controls\n",
    "\n",
    "The SUMMA (Structure for Unifying Multiple Modeling Alternatives) framework enables systematic evaluation of process representations, making it ideal for scientific hypothesis testing and model physics assessment.\n",
    "\n",
    "## CONFLUENCE Implementation\n",
    "CONFLUENCE manages model execution through several integrated components:\n",
    "\n",
    "- Workflow Orchestration: Automated sequencing of model initialization, spinup, and main simulation\n",
    "- Configuration Management: Translation of scientific decisions into model-specific control files\n",
    "- Execution Monitoring: Real-time tracking of model progress and error detection\n",
    "- Output Organization: Systematic storage and cataloging of simulation results\n",
    "- Quality Assurance: Automated checks for mass balance closure and physical realism\n",
    "\n",
    "This framework ensures that model execution is reproducible, traceable, and scientifically rigorous, while handling the computational complexity behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nRunning {confluence.config['HYDROLOGICAL_MODEL']} for point-scale simulation...\")\n",
    "\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\nPoint-scale model run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation and Process Validation\n",
    "## Scientific Context\n",
    "Model evaluation represents the critical test of whether our process-based simulation captures the essential physics of the real-world system. Effective evaluation requires:\n",
    "\n",
    "- Multi-Variable Assessment: Testing multiple aspects of the hydrological system to avoid equifinality and ensure robust process representation\n",
    "- Temporal Pattern Analysis: Evaluating both magnitude and timing of hydrological responses across seasonal cycles\n",
    "- Process-Specific Metrics: Using evaluation criteria that reflect the underlying physics being tested\n",
    "- Uncertainty Quantification: Understanding both observational and model uncertainty in performance assessment\n",
    "\n",
    "For point-scale modeling, we focus on direct process validation where observations closely match the spatial and temporal scales of model representation. The Paradise SNOTEL station provides co-located snow water equivalent and multi-depth soil moisture observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "sim_dir = confluence.project_dir / \"simulations\" / config_dict['EXPERIMENT_ID'] / \"SUMMA\"\n",
    "daily_output_path = sim_dir / f\"{config_dict['EXPERIMENT_ID']}_day.nc\"\n",
    "\n",
    "# Load and prepare the evaluation dataset\n",
    "ds = xr.open_dataset(daily_output_path)\n",
    "\n",
    "# Skip spinup period\n",
    "start_year = ds.time.dt.year.min().values + 1\n",
    "spinup_end = f\"{start_year}-01-01\"\n",
    "time_mask = ds.time >= pd.to_datetime(spinup_end)\n",
    "evaluation_data = ds.isel(time=time_mask)\n",
    "\n",
    "# Load observed SWE data\n",
    "obs_swe_path = confluence.project_dir / \"observations\" / \"snow\" / \"snotel\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_swe_processed.csv\"\n",
    "\n",
    "obs_swe = pd.read_csv(obs_swe_path, parse_dates=['Date'], dayfirst=True)\n",
    "obs_swe.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure proper datetime index\n",
    "if not isinstance(obs_swe.index, pd.DatetimeIndex):\n",
    "    obs_swe.index = pd.to_datetime(obs_swe.index)\n",
    "\n",
    "# Extract simulated SWE\n",
    "sim_swe = evaluation_data['scalarSWE'].to_pandas()\n",
    "\n",
    "# Find common period and align data\n",
    "start_date = max(obs_swe.index.min(), sim_swe.index.min())\n",
    "end_date = min(obs_swe.index.max(), sim_swe.index.max())\n",
    "\n",
    "# Resample to daily and filter to common period\n",
    "obs_daily = obs_swe.resample('D').mean().loc[start_date:end_date]\n",
    "sim_daily = sim_swe.resample('D').mean().loc[start_date:end_date]\n",
    "\n",
    "# Handle different column names for SWE\n",
    "if 'SWE' in obs_daily.columns:\n",
    "    obs_values = obs_daily['SWE']\n",
    "elif 'swe' in obs_daily.columns:\n",
    "    obs_values = obs_daily['swe']\n",
    "else:\n",
    "    # Use first column\n",
    "    obs_values = obs_daily.iloc[:, 0]\n",
    "\n",
    "# Convert sim_daily to Series if it's a DataFrame\n",
    "if isinstance(sim_daily, pd.DataFrame):\n",
    "    if len(sim_daily.columns) == 1:\n",
    "        sim_daily = sim_daily.iloc[:, 0]  # Extract the single column as Series\n",
    "\n",
    "# Remove NaN values for metrics calculation\n",
    "valid_mask = ~(obs_values.isna() | sim_daily.isna())\n",
    "obs_valid = obs_values[valid_mask]\n",
    "sim_valid = sim_daily[valid_mask]\n",
    "\n",
    "print(f\"\\n📊 Snow Water Equivalent Performance Metrics:\")\n",
    "\n",
    "# Basic statistics\n",
    "rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "bias = (sim_valid - obs_valid).mean()\n",
    "mae = np.abs(obs_valid - sim_valid).mean()\n",
    "corr = obs_valid.corr(sim_valid)\n",
    "\n",
    "# Percent bias\n",
    "pbias = 100 * bias / obs_valid.mean()\n",
    "\n",
    "# Nash-Sutcliffe Efficiency\n",
    "nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "\n",
    "# Kling-Gupta Efficiency\n",
    "kge_corr = obs_valid.corr(sim_valid)\n",
    "kge_bias = sim_valid.mean() / obs_valid.mean()\n",
    "kge_var = (sim_valid.std() / obs_valid.std())\n",
    "kge = 1 - np.sqrt((kge_corr - 1)**2 + (kge_bias - 1)**2 + (kge_var - 1)**2)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"   Root Mean Square Error (RMSE): {rmse:.2f} mm\")\n",
    "print(f\"   Mean Absolute Error (MAE): {mae:.2f} mm\")\n",
    "print(f\"   Bias: {bias:+.2f} mm ({pbias:+.1f}%)\")\n",
    "print(f\"   Correlation: {corr:.3f}\")\n",
    "print(f\"   Nash-Sutcliffe Efficiency: {nse:.3f}\")\n",
    "print(f\"   Kling-Gupta Efficiency: {kge:.3f}\")\n",
    "\n",
    "# Snow-specific metrics\n",
    "print(f\"\\n❄️  Snow-Specific Performance Assessment:\")\n",
    "\n",
    "# Peak SWE analysis\n",
    "obs_peak = obs_valid.max()\n",
    "sim_peak = sim_valid.max()\n",
    "peak_bias = sim_peak - obs_peak\n",
    "peak_pbias = 100 * peak_bias / obs_peak\n",
    "\n",
    "print(f\"       Peak SWE:\")\n",
    "print(f\"       Observed: {obs_peak:.1f} mm\")\n",
    "print(f\"       Simulated: {sim_peak:.1f} mm\")\n",
    "print(f\"       Bias: {peak_bias:+.1f} mm ({peak_pbias:+.1f}%)\")\n",
    "\n",
    "# Snow season timing\n",
    "obs_peak_date = obs_valid.idxmax()\n",
    "sim_peak_date = sim_valid.idxmax()\n",
    "timing_diff = (sim_peak_date - obs_peak_date).days\n",
    "\n",
    "print(f\"       Peak Timing:\")\n",
    "print(f\"       Observed peak: {obs_peak_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"       Simulated peak: {sim_peak_date.strftime('%B %d, %Y')}\")\n",
    "print(f\"       Timing difference: {timing_diff:+d} days\")\n",
    "\n",
    "# Snow season length\n",
    "snow_threshold = 10  # mm\n",
    "obs_snow_days = (obs_valid > snow_threshold).sum()\n",
    "sim_snow_days = (sim_valid > snow_threshold).sum()\n",
    "\n",
    "print(f\"       Snow Season (SWE > {snow_threshold} mm):\")\n",
    "print(f\"       Observed: {obs_snow_days} days\")\n",
    "print(f\"       Simulated: {sim_snow_days} days\")\n",
    "print(f\"       Difference: {sim_snow_days - obs_snow_days:+d} days\")\n",
    "\n",
    "# Create  visualization\n",
    "print(f\"\\n Creating SWE comparison visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(obs_daily.index, obs_values, 'o-', label='Observed', \n",
    "         color='black', alpha=0.7, markersize=3, linewidth=1)\n",
    "ax1.plot(sim_daily.index, sim_daily, '-', label='Simulated', \n",
    "         color='blue', linewidth=2)\n",
    "ax1.set_title('Snow Water Equivalent Time Series', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('SWE (mm)', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add performance metrics as text\n",
    "metrics_text = f'RMSE: {rmse:.1f} mm\\nBias: {bias:+.1f} mm\\nCorr: {corr:.3f}\\nNSE: {nse:.3f}'\n",
    "ax1.text(0.02, 0.95, metrics_text, transform=ax1.transAxes, \n",
    "         bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "\n",
    "# Scatter plot\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(obs_valid, sim_valid, alpha=0.6, c='blue', s=20)\n",
    "max_val = max(obs_valid.max(), sim_valid.max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'k--', label='1:1 line')\n",
    "ax2.set_xlabel('Observed SWE (mm)', fontsize=11)\n",
    "ax2.set_ylabel('Simulated SWE (mm)', fontsize=11)\n",
    "ax2.set_title('Observed vs. Simulated SWE', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Seasonal cycle\n",
    "ax3 = axes[1, 0]\n",
    "obs_monthly = obs_values.groupby(obs_values.index.month).mean()\n",
    "sim_monthly = sim_daily.groupby(sim_daily.index.month).mean()\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax3.plot(range(1, 13), obs_monthly, 'o-', label='Observed', color='black', linewidth=2)\n",
    "ax3.plot(range(1, 13), sim_monthly, 'o-', label='Simulated', color='blue', linewidth=2)\n",
    "ax3.set_xticks(range(1, 13))\n",
    "ax3.set_xticklabels(months, rotation=45)\n",
    "ax3.set_ylabel('Mean SWE (mm)', fontsize=11)\n",
    "ax3.set_title('Seasonal Cycle', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Residuals over time\n",
    "ax4 = axes[1, 1]\n",
    "residuals = sim_valid - obs_valid\n",
    "ax4.scatter(obs_valid.index, residuals, alpha=0.6, c='red', s=15)\n",
    "ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.axhline(y=residuals.std(), color='red', linestyle='--', alpha=0.5, label=f'±1σ ({residuals.std():.1f} mm)')\n",
    "ax4.axhline(y=-residuals.std(), color='red', linestyle='--', alpha=0.5)\n",
    "ax4.set_ylabel('Residuals (Sim - Obs) [mm]', fontsize=11)\n",
    "ax4.set_title('Model Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle(f'Snow Water Equivalent Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b: Soil Moisture Profile Evaluation\n",
    "Soil moisture evaluation tests the model's representation of vadose zone processes, including infiltration, drainage, and vertical redistribution. Multi-depth observations provide unprecedented validation opportunities for soil physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observed soil moisture data\n",
    "obs_sm_path = confluence.project_dir / \"observations\" / \"soil_moisture\" / \"ismn\" / \"processed\" / f\"{config_dict['DOMAIN_NAME']}_sm_processed.csv\"\n",
    "\n",
    "# Load observed data\n",
    "obs_sm = pd.read_csv(obs_sm_path, parse_dates=['timestamp'])\n",
    "obs_sm.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Ensure proper datetime index\n",
    "if not isinstance(obs_sm.index, pd.DatetimeIndex):\n",
    "    obs_sm.index = pd.to_datetime(obs_sm.index)\n",
    "\n",
    "# Identify observed depth columns\n",
    "obs_depth_cols = [col for col in obs_sm.columns if col.startswith('sm_')]\n",
    "\n",
    "# Extract observed depth values\n",
    "obs_depths = []\n",
    "for depth_col in obs_depth_cols:\n",
    "    # Extract depth from column name (e.g., 'sm_0.0508_0.0508' -> 0.0508)\n",
    "    depth_str = depth_col.split('_')[1]\n",
    "    obs_depths.append(float(depth_str))\n",
    "\n",
    "# Extract simulated soil moisture\n",
    "\n",
    "sim_sm = evaluation_data['mLayerVolFracLiq']\n",
    "sim_depths = evaluation_data['mLayerDepth']\n",
    "    \n",
    "# Calculate representative layer depths\n",
    "mean_layer_depths = sim_depths.mean(dim='time')\n",
    "valid_layers = mean_layer_depths > 0  # Filter out invalid layers\n",
    "    \n",
    "# Find common period\n",
    "start_date = max(obs_sm.index.min(), pd.to_datetime(sim_sm.time.min().values))\n",
    "end_date = min(obs_sm.index.max(), pd.to_datetime(sim_sm.time.max().values))\n",
    "    \n",
    "# Filter to common period\n",
    "obs_period = obs_sm.loc[start_date:end_date]\n",
    "sim_time_mask = (sim_sm.time >= start_date) & (sim_sm.time <= end_date)\n",
    "sim_period = sim_sm.isel(time=sim_time_mask)\n",
    "sim_depths_period = sim_depths.isel(time=sim_time_mask)\n",
    "    \n",
    "n_depths = len(obs_depth_cols)\n",
    "depth_results = {}\n",
    "\n",
    "# Analyze each observed depth\n",
    "for i, (depth_col, obs_depth) in enumerate(zip(obs_depth_cols, obs_depths)):\n",
    "    print(f\"\\n     Depth {i+1}: {obs_depth:.4f}m ({depth_col})\")\n",
    "    \n",
    "    # Find closest simulated layer\n",
    "    mean_depths = sim_depths_period.mean(dim='time')\n",
    "    valid_mask = mean_depths > 0\n",
    "    \n",
    "    if valid_mask.sum() > 0:\n",
    "        valid_mean_depths = mean_depths.where(valid_mask)\n",
    "        depth_differences = np.abs(valid_mean_depths - obs_depth)\n",
    "        closest_layer_idx = depth_differences.argmin().values\n",
    "        closest_layer_depth = valid_mean_depths[closest_layer_idx].values\n",
    "        \n",
    "        \n",
    "        # Extract data for this layer\n",
    "        obs_layer = obs_period[depth_col]\n",
    "        sim_layer = sim_period.isel(midToto=closest_layer_idx, hru=0)\n",
    "        \n",
    "        # Convert to pandas for easier handling\n",
    "        sim_layer_ts = sim_layer.to_pandas()\n",
    "        \n",
    "        # Resample to daily and align\n",
    "        obs_daily = obs_layer.resample('D').mean()\n",
    "        sim_daily = sim_layer_ts.resample('D').mean()\n",
    "        \n",
    "        # Remove invalid values (negative soil moisture indicates missing data)\n",
    "        sim_daily = sim_daily.where(sim_daily > -100)\n",
    "        \n",
    "        # Find valid paired data\n",
    "        valid_data_mask = ~(obs_daily.isna() | sim_daily.isna())\n",
    "        obs_valid = obs_daily[valid_data_mask]\n",
    "        sim_valid = sim_daily[valid_data_mask]\n",
    "        \n",
    "        if len(obs_valid) > 10:  # Require minimum data for meaningful evaluation\n",
    "            # Calculate performance metrics\n",
    "            rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "            bias = (sim_valid - obs_valid).mean()\n",
    "            mae = np.abs(obs_valid - sim_valid).mean()\n",
    "            corr = obs_valid.corr(sim_valid)\n",
    "            \n",
    "            # Store results\n",
    "            depth_results[obs_depth] = {\n",
    "                'obs_valid': obs_valid,\n",
    "                'sim_valid': sim_valid,\n",
    "                'rmse': rmse,\n",
    "                'bias': bias,\n",
    "                'mae': mae,\n",
    "                'corr': corr,\n",
    "                'layer_idx': closest_layer_idx,\n",
    "                'sim_depth': closest_layer_depth\n",
    "            }\n",
    "            \n",
    "            print(f\"       RMSE: {rmse:.3f} m³/m³\")\n",
    "            print(f\"       Bias: {bias:+.3f} m³/m³\")\n",
    "            print(f\"       Correlation: {corr:.3f}\")\n",
    "            print(f\"       Valid pairs: {len(obs_valid)}\")\n",
    "            \n",
    "# Create  visualization\n",
    "print(f\"\\n  Creating soil moisture profile evaluation...\")\n",
    "\n",
    "n_depths = len(depth_results)\n",
    "fig, axes = plt.subplots(n_depths, 2, figsize=(15, 4*n_depths))\n",
    "\n",
    "if n_depths == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (obs_depth, results) in enumerate(depth_results.items()):\n",
    "    # Time series plot\n",
    "    ax_ts = axes[i, 0]\n",
    "    ax_ts.plot(results['obs_valid'].index, results['obs_valid'], 'o-', \n",
    "              label=f'Observed ({obs_depth:.4f}m)', \n",
    "              color='black', alpha=0.7, markersize=2, linewidth=1)\n",
    "    ax_ts.plot(results['sim_valid'].index, results['sim_valid'], '-', \n",
    "              label=f'Simulated (L{results[\"layer_idx\"]}, {results[\"sim_depth\"]}m)', \n",
    "              color='blue', linewidth=2)\n",
    "    \n",
    "    ax_ts.set_title(f'Soil Moisture at {obs_depth:.4f}m depth', fontsize=11, fontweight='bold')\n",
    "    ax_ts.set_ylabel('Soil Moisture (m³/m³)', fontsize=10)\n",
    "    ax_ts.grid(True, alpha=0.3)\n",
    "    ax_ts.legend(fontsize=9)\n",
    "    \n",
    "    # Add metrics\n",
    "    metrics_text = (f\"RMSE: {results['rmse']:.3f}\\n\"\n",
    "                   f\"Bias: {results['bias']:+.3f}\\n\"\n",
    "                   f\"Corr: {results['corr']:.3f}\")\n",
    "    ax_ts.text(0.02, 0.95, metrics_text, transform=ax_ts.transAxes,\n",
    "              bbox=dict(facecolor='white', alpha=0.8), fontsize=9, verticalalignment='top')\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax_scatter = axes[i, 1]\n",
    "    ax_scatter.scatter(results['obs_valid'], results['sim_valid'], \n",
    "                      alpha=0.6, c='blue', s=15)\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(results['obs_valid'].min(), results['sim_valid'].min())\n",
    "    max_val = max(results['obs_valid'].max(), results['sim_valid'].max())\n",
    "    ax_scatter.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                   label='1:1 line', alpha=0.7)\n",
    "    \n",
    "    ax_scatter.set_xlabel('Observed SM (m³/m³)', fontsize=10)\n",
    "    ax_scatter.set_ylabel('Simulated SM (m³/m³)', fontsize=10)\n",
    "    ax_scatter.set_title(f'Obs vs Sim at {obs_depth:.4f}m', fontsize=11, fontweight='bold')\n",
    "    ax_scatter.grid(True, alpha=0.3)\n",
    "    ax_scatter.legend(fontsize=9)\n",
    "    ax_scatter.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.suptitle(f'Soil Moisture Profile Evaluation - {config_dict[\"DOMAIN_NAME\"].title()}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary performance assessment\n",
    "print(f\"\\n  Soil Moisture Profile Performance Summary:\")\n",
    "\n",
    "avg_rmse = np.mean([r['rmse'] for r in depth_results.values()])\n",
    "avg_corr = np.mean([r['corr'] for r in depth_results.values()])\n",
    "avg_bias = np.mean([r['bias'] for r in depth_results.values()])\n",
    "\n",
    "print(f\"       Profile-averaged metrics:\")\n",
    "print(f\"       Average RMSE: {avg_rmse:.3f} m³/m³\")\n",
    "print(f\"       Average correlation: {avg_corr:.3f}\")\n",
    "print(f\"       Average bias: {avg_bias:+.3f} m³/m³\")\n",
    "\n",
    "# Close evaluation dataset\n",
    "evaluation_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tutorial Summary and Next Steps\n",
    "\n",
    "## Summary: Point-Scale Snow and Soil Process Evaluation\n",
    "\n",
    "This tutorial demonstrated the complete CONFLUENCE workflow for point-scale hydrological modeling, establishing fundamental principles for reproducible computational hydrology research. Through the Paradise SNOTEL case study, we illustrated how standardized workflows can bridge the gap between complex modeling frameworks and practical scientific applications while maintaining scientific rigor and reproducibility.\n",
    "\n",
    "### Key Methods\n",
    "The tutorial successfully demonstrated CONFLUENCE's core capabilities through five integrated workflow components. Reproducible workflow management was achieved through configuration-driven experiments that provide complete provenance tracking and ensure experimental transparency. The model-agnostic preprocessing pipeline creates standardized data products that enable true model physics comparisons by separating data preparation from model-specific requirements. Process-based simulation was implemented using SUMMA's modular physics to explicitly represent energy and water balance processes at the point scale. Multi-variable validation leveraged co-located snow water equivalent and multi-depth soil moisture observations to provide comprehensive model evaluation. Finally, scientific interpretation linked quantitative performance metrics to underlying physical processes, moving beyond statistical assessment to process understanding.\n",
    "\n",
    "### Scientific Process Validation\n",
    "The tutorial validated key hydrological processes across multiple temporal scales and physical domains. Snow physics evaluation included accumulation, metamorphism, and energy-balance driven melt processes, with assessment of both magnitude and timing accuracy. Soil hydrology validation examined multi-depth moisture dynamics and vertical water redistribution through four soil layers, testing infiltration and drainage representations. Energy balance processes were implicitly evaluated through successful simulation of temperature-driven phase changes and moisture dynamics. Temporal dynamics were assessed from daily to seasonal time scales, capturing both rapid response and longer-term memory effects. Physical realism was maintained through mass and energy conservation checks and realistic state variable bounds.\n",
    "\n",
    "### CONFLUENCE Framework Demonstration\n",
    "This tutorial showcased CONFLUENCE's modular architecture through specialized managers that handle distinct workflow components while maintaining system integration. Workflow orchestration capabilities were demonstrated through automated step sequencing. The scalable design principle was established, showing how the same framework structure supports applications from point-scale studies through continental-scale analyses. Research continuity was ensured by creating a foundation that directly enables progression to distributed modeling and large-sample comparative studies in subsequent tutorials.\n",
    "\n",
    "### Next Focus: Evapotranspiration and Energy Balance Processes\n",
    "\n",
    "**Ready to explore energy flux validation?** → **[Tutorial 01b: Point-Scale Energy Balance Validation](./01b_point_scale_fluxnet.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scienv)",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
