{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial: Distributed Basin Workflow with Delineation\n",
    "\n",
    "This notebook demonstrates the distributed modeling approach using the delineation method. We'll use the same Bow River at Banff location but create a distributed model with multiple GRUs (Grouped Response Units).\n",
    "\n",
    "## Key Differences from Lumped Model\n",
    "\n",
    "- **Domain Method**: `delineate` instead of `lumped`\n",
    "- **Stream Threshold**: 5000 (creates more sub-basins)\n",
    "- **Multiple GRUs**: Each sub-basin becomes a GRU\n",
    "- **Routing**: mizuRoute connects the GRUs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand watershed delineation with stream networks\n",
    "2. Create a distributed model with multiple GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Import main CONFLUENCE class\n",
    "from CONFLUENCE import CONFLUENCE\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize CONFLUENCE\n",
    "First, let's set up our directories and load the configuration. We'll modify the configuration from Tutorial 1 to create a distributed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/work/comphyd_lab/data/CONFLUENCE_data')  # ‚Üê User should modify this path\n",
    "\n",
    "# Load template configuration\n",
    "config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_template.yaml'\n",
    "\n",
    "# Read config file\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update core paths\n",
    "config_dict['CONFLUENCE_CODE_DIR'] = str(CONFLUENCE_CODE_DIR)\n",
    "config_dict['CONFLUENCE_DATA_DIR'] = str(CONFLUENCE_DATA_DIR)\n",
    "\n",
    "# Modify for distributed delineation\n",
    "config_dict['DOMAIN_NAME'] = 'Bow_at_Banff_distributed'\n",
    "config_dict['EXPERIMENT_ID'] = 'distributed_tutorial'\n",
    "config_dict['DOMAIN_DEFINITION_METHOD'] = 'delineate'  # Changed from 'lumped'\n",
    "config_dict['STREAM_THRESHOLD'] = 5000  # Higher threshold for fewer sub-basins\n",
    "config_dict['DOMAIN_DISCRETIZATION'] = 'GRUs'  # Keep as GRUs\n",
    "config_dict['SPATIAL_MODE'] = 'Distributed'  # Changed from 'Lumped'\n",
    "\n",
    "# Save updated config to a temporary file\n",
    "temp_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_distributed.yaml'\n",
    "with open(temp_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f)\n",
    "\n",
    "# Initialize CONFLUENCE\n",
    "confluence = CONFLUENCE(temp_config_path)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=== Directory Configuration ===\")\n",
    "print(f\"Code Directory: {CONFLUENCE_CODE_DIR}\")\n",
    "print(f\"Data Directory: {CONFLUENCE_DATA_DIR}\")\n",
    "print(\"\\n=== Key Configuration Settings ===\")\n",
    "print(f\"Domain Name: {confluence.config['DOMAIN_NAME']}\")\n",
    "print(f\"Pour Point: {confluence.config['POUR_POINT_COORDS']}\")\n",
    "print(f\"Domain Method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"Stream Threshold: {confluence.config['STREAM_THRESHOLD']}\")\n",
    "print(f\"Spatial Mode: {confluence.config['SPATIAL_MODE']}\")\n",
    "print(f\"Model: {confluence.config['HYDROLOGICAL_MODEL']}\")\n",
    "print(f\"Simulation Period: {confluence.config['EXPERIMENT_TIME_START']} to {confluence.config['EXPERIMENT_TIME_END']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Setup - Organizing the Modeling Workflow\n",
    "\n",
    "First, we'll establish a well-organized project structure, similar to what we did in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Project Initialization\n",
    "print(\"=== Step 1: Project Initialization ===\")\n",
    "\n",
    "# Setup project\n",
    "project_dir = confluence.managers['project'].setup_project()\n",
    "\n",
    "# Create pour point\n",
    "pour_point_path = confluence.managers['project'].create_pour_point()\n",
    "\n",
    "# List created directories\n",
    "print(\"\\nCreated directories:\")\n",
    "for item in sorted(project_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}\")\n",
    "\n",
    "print(\"\\nNote: The pour point location is identical to the lumped model.\")\n",
    "print(\"The difference is in how we subdivide the watershed above this point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geospatial Domain Definition - Data Acquisition and Preparation\n",
    "\n",
    "We'll reuse some of the geospatial data from the lumped model tutorial, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if we can reuse data from the lumped model\n",
    "lumped_dem_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped' / 'attributes' / 'elevation' / 'dem'\n",
    "lumped_forcing_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped' / 'forcing' / 'raw_data'\n",
    "can_reuse = lumped_dem_path.exists()\n",
    "can_reuse_forcing = lumped_forcing_path.exists()\n",
    "\n",
    "if can_reuse or can_reuse_forcing:\n",
    "    import shutil\n",
    "    \n",
    "    # Create a function to copy files with name substitution\n",
    "    def copy_with_name_substitution(src_path, dst_path, old_str='_lumped', new_str='_distributed'):\n",
    "        if not src_path.exists():\n",
    "            return False\n",
    "            \n",
    "        # Create destination directory if it doesn't exist\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if src_path.is_dir():\n",
    "            # Copy entire directory\n",
    "            if not dst_path.exists():\n",
    "                dst_path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "            # Copy all files with name substitution\n",
    "            for src_file in src_path.glob('**/*'):\n",
    "                if src_file.is_file():\n",
    "                    # Create relative path\n",
    "                    rel_path = src_file.relative_to(src_path)\n",
    "                    # Create new filename with substitution\n",
    "                    new_name = src_file.name.replace(old_str, new_str)\n",
    "                    # Create destination path\n",
    "                    dst_file = dst_path / rel_path.parent / new_name\n",
    "                    # Create parent directories if they don't exist\n",
    "                    dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    # Copy the file\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "            return True\n",
    "        elif src_path.is_file():\n",
    "            # Copy single file with name substitution\n",
    "            new_name = dst_path.name.replace(old_str, new_str)\n",
    "            dst_file = dst_path.parent / new_name\n",
    "            dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src_path, dst_file)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    print(\"Found existing geospatial data from lumped model. Copying and renaming files...\")\n",
    "    \n",
    "    # Copy and rename DEM and other attribute data\n",
    "    if can_reuse:\n",
    "        # Define paths\n",
    "        src_attr_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped' / 'attributes'\n",
    "        dst_attr_path = project_dir / 'attributes'\n",
    "        \n",
    "        # Copy attributes with name substitution\n",
    "        copied = copy_with_name_substitution(src_attr_path, dst_attr_path, '_lumped', '_distributed')\n",
    "        if copied:\n",
    "            print(\"‚úì Copied and renamed attribute files from lumped model\")\n",
    "    \n",
    "    # Copy and rename forcing data\n",
    "    if can_reuse_forcing:\n",
    "        # Define paths\n",
    "        src_forcing_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped' / 'forcing' / 'raw_data'\n",
    "        dst_forcing_path = project_dir / 'forcing' / 'raw_data'\n",
    "        \n",
    "        # Copy forcing data with name substitution\n",
    "        copied = copy_with_name_substitution(src_forcing_path, dst_forcing_path, '_lumped', '_distributed')\n",
    "        if copied:\n",
    "            print(\"‚úì Copied and renamed forcing data from lumped model\")\n",
    "            \n",
    "    print(\"The distributed model will use these copied files as a starting point.\")\n",
    "else:\n",
    "    print(\"No existing data found from the lumped model. Will acquire all data from scratch.\")\n",
    "\n",
    "    # Step 2: Geospatial Domain Definition - Data Acquisition\n",
    "    print(\"\\n=== Step 2: Geospatial Domain Definition - Data Acquisition ===\")\n",
    "    \n",
    "    # Acquire attributes\n",
    "    print(\"Acquiring geospatial attributes (DEM, soil, land cover)...\")\n",
    "    confluence.managers['data'].acquire_attributes()\n",
    "\n",
    "    # Acquire forcings\n",
    "    print(f\"\\nAcquiring forcing data: {confluence.config['FORCING_DATASET']}\")\n",
    "    confluence.managers['data'].acquire_forcings()\n",
    "    \n",
    "print(\"\\n‚úì Geospatial attributes acquired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Domain Definition - Delineation with Stream Network\n",
    "\n",
    "This is where the main difference occurs - we'll create multiple sub-basins connected by a stream network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Geospatial Domain Definition - Delineation\n",
    "print(\"=== Step 3: Geospatial Domain Definition - Delineation ===\")\n",
    "\n",
    "# Define domain\n",
    "print(f\"Delineating distributed watershed...\")\n",
    "print(f\"Method: {confluence.config['DOMAIN_DEFINITION_METHOD']}\")\n",
    "print(f\"Stream threshold: {confluence.config['STREAM_THRESHOLD']}\")\n",
    "print(\"\\nThis will create multiple sub-basins connected by a stream network.\")\n",
    "\n",
    "watershed_path = confluence.managers['domain'].define_domain()\n",
    "\n",
    "# Check outputs\n",
    "basin_path = project_dir / 'shapefiles' / 'river_basins'\n",
    "network_path = project_dir / 'shapefiles' / 'river_network'\n",
    "\n",
    "if basin_path.exists():\n",
    "    basin_files = list(basin_path.glob('*.shp'))\n",
    "    print(f\"\\n‚úì Created basin shapefiles: {len(basin_files)}\")\n",
    "    \n",
    "if network_path.exists():\n",
    "    network_files = list(network_path.glob('*.shp'))\n",
    "    print(f\"‚úì Created river network shapefiles: {len(network_files)}\")\n",
    "    \n",
    "    # Load and check number of basins\n",
    "    if basin_files:\n",
    "        gdf = gpd.read_file(basin_files[0])\n",
    "        print(f\"\\nNumber of sub-basins (GRUs): {len(gdf)}\")\n",
    "        print(f\"Total area: {gdf.geometry.area.sum() / 1e6:.2f} km¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Distributed Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the delineated domain with stream network\n",
    "basin_files = list((project_dir / 'shapefiles' / 'river_basins').glob('*.shp'))\n",
    "network_files = list((project_dir / 'shapefiles' / 'river_network').glob('*.shp'))\n",
    "    \n",
    "if basin_files and network_files:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Load data\n",
    "    basins = gpd.read_file(basin_files[0])\n",
    "    rivers = gpd.read_file(network_files[0])\n",
    "    \n",
    "    # Plot basins with different colors\n",
    "    basins.plot(ax=ax, column='GRU_ID', cmap='viridis', \n",
    "               alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Plot river network\n",
    "    rivers.plot(ax=ax, color='blue', linewidth=2)\n",
    "    \n",
    "    # Add pour point\n",
    "    pour_point = gpd.read_file(pour_point_path)\n",
    "    pour_point.plot(ax=ax, color='red', markersize=150, marker='o', zorder=5)\n",
    "    \n",
    "    ax.set_title(f'Distributed Domain: {len(basins)} Sub-basins', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Add colorbar for GRU IDs\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                               norm=plt.Normalize(vmin=basins['GRU_ID'].min(), \n",
    "                                                 vmax=basins['GRU_ID'].max()))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('GRU ID', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Geospatial Domain Definition - Discretization\n",
    "\n",
    "Now we'll create Hydrologic Response Units (HRUs) based on the Grouped Response Units (GRUs) we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Geospatial Domain Definition - Discretization\n",
    "print(\"=== Step 4: Geospatial Domain Definition - Discretization ===\")\n",
    "\n",
    "# Discretize domain\n",
    "print(f\"Creating HRUs based on GRUs...\")\n",
    "print(f\"Method: {confluence.config['DOMAIN_DISCRETIZATION']}\")\n",
    "print(\"For this tutorial: 1 GRU = 1 HRU (simplest case)\")\n",
    "\n",
    "hru_path = confluence.managers['domain'].discretize_domain()\n",
    "\n",
    "# Check the created HRU shapefile\n",
    "catchment_path = project_dir / 'shapefiles' / 'catchment'\n",
    "if catchment_path.exists():\n",
    "    hru_files = list(catchment_path.glob('*.shp'))\n",
    "    print(f\"\\n‚úì Created HRU shapefiles: {len(hru_files)}\")\n",
    "    \n",
    "    if hru_files:\n",
    "        hru_gdf = gpd.read_file(hru_files[0])\n",
    "        print(f\"\\nHRU Statistics:\")\n",
    "        print(f\"Number of HRUs: {len(hru_gdf)}\")\n",
    "        print(f\"Number of GRUs: {hru_gdf['GRU_ID'].nunique()}\")\n",
    "        print(f\"Total area: {hru_gdf.geometry.area.sum() / 1e6:.2f} km¬≤\")\n",
    "        \n",
    "        # Show HRU distribution\n",
    "        hru_counts = hru_gdf.groupby('GRU_ID').size()\n",
    "        print(f\"\\nHRUs per GRU:\")\n",
    "        for gru_id, count in hru_counts.items():\n",
    "            print(f\"  GRU {gru_id}: {count} HRUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Agnostic Data Processing - Observed Data\n",
    "\n",
    "The observed streamflow data will be the same for both the lumped and distributed models since they use the same pour point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Model Agnostic Data Processing - Observed Data\n",
    "print(\"=== Step 5: Model Agnostic Data Processing - Observed Data ===\")\n",
    "\n",
    "# Check if we can reuse data from the lumped model\n",
    "lumped_obs_path = CONFLUENCE_DATA_DIR / 'domain_Bow_at_Banff_lumped' / 'observations' / 'streamflow' / 'preprocessed'\n",
    "can_reuse_obs = lumped_obs_path.exists() and list(lumped_obs_path.glob('*.csv'))\n",
    "\n",
    "if can_reuse_obs:\n",
    "    print(\"Found existing observed data from lumped model. Reusing...\")\n",
    "    # We can proceed, but CONFLUENCE will handle the reuse internally\n",
    "\n",
    "# Process observed data\n",
    "print(\"Processing observed streamflow data...\")\n",
    "confluence.managers['data'].process_observed_data()\n",
    "\n",
    "# Visualize observed streamflow data\n",
    "obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "if obs_path.exists():\n",
    "    obs_df = pd.read_csv(obs_path)\n",
    "    obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(obs_df['datetime'], obs_df['discharge_cms'], \n",
    "            linewidth=1.5, color='blue', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "    ax.set_title(f'Observed Streamflow - Bow River at Banff (WSC Station: {confluence.config[\"STATION_ID\"]})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    ax.text(0.02, 0.95, f'Mean: {obs_df[\"discharge_cms\"].mean():.1f} m¬≥/s\\nMax: {obs_df[\"discharge_cms\"].max():.1f} m¬≥/s', \n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Agnostic Data Processing - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Model Agnostic Data Processing - Preprocessing\n",
    "print(\"=== Step 7: Model Agnostic Data Processing - Preprocessing ===\")\n",
    "\n",
    "# Run model-agnostic preprocessing\n",
    "print(\"\\nRunning model-agnostic preprocessing...\")\n",
    "confluence.managers['data'].run_model_agnostic_preprocessing()\n",
    "\n",
    "print(\"\\n‚úì Model-agnostic preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model-Specific Processing - Preprocessing\n",
    "\n",
    "Now we prepare inputs specific to our chosen hydrological model (SUMMA in this case), set up for a distributed configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Model Specific Processing and Initialization\n",
    "print(\"=== Step 8: Model Specific Processing and Initialization ===\")\n",
    "\n",
    "# Preprocess models\n",
    "print(f\"Preparing {confluence.config['HYDROLOGICAL_MODEL']} input files...\")\n",
    "print(f\"Note: For distributed mode with {confluence.config['HYDROLOGICAL_MODEL']}, this includes generating:\")\n",
    "print(f\"  - Model parameter files for each GRU\")\n",
    "print(f\"  - Routing configuration for river network\")\n",
    "\n",
    "confluence.managers['model'].preprocess_models()\n",
    "\n",
    "print(\"\\n‚úì Model-specific preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Run the Distributed Model\n",
    "\n",
    "Now we execute the SUMMA model in distributed mode with routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Run the Distributed Model\n",
    "print(\"=== Step 9: Run the Distributed Model ===\")\n",
    "\n",
    "# Run the model\n",
    "print(f\"Running distributed {confluence.config['HYDROLOGICAL_MODEL']} model...\")\n",
    "print(f\"Number of GRUs: (check from previous output)\")\n",
    "print(\"Note: This will take longer than the lumped model due to multiple units.\")\n",
    "\n",
    "confluence.managers['model'].run_models()\n",
    "\n",
    "print(\"\\n‚úì Model execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Visualize Distributed Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Visualize Observed vs. Simulated Streamflow for Distributed Model\n",
    "print(\"=== Step 14: Visualizing Model Results (Distributed) ===\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Load and plot simulation results\n",
    "sim_path = project_dir / 'simulations' / confluence.config['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "sim_files = list(sim_path.glob('*.nc'))\n",
    "\n",
    "if not sim_files:\n",
    "    print(\"No mizuRoute simulation results found. Check if model execution was successful.\")\n",
    "    print(f\"Expected path: {sim_path}\")\n",
    "    \n",
    "    # Check for alternative locations\n",
    "    alt_sim_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "        f\"domain_{config_dict['DOMAIN_NAME']}/simulations/{config_dict['EXPERIMENT_ID']}/mizuRoute/*.nc\"))\n",
    "    \n",
    "    if alt_sim_paths:\n",
    "        sim_files = alt_sim_paths\n",
    "        print(f\"Found alternative simulation data at: {sim_files[0]}\")\n",
    "    else:\n",
    "        print(\"No simulation results found anywhere. Visualization cannot proceed.\")\n",
    "\n",
    "if sim_files:\n",
    "    try:\n",
    "        # Load simulation data\n",
    "        print(f\"Loading simulation data from: {sim_files[0]}\")\n",
    "        sim_data = xr.open_dataset(sim_files[0])\n",
    "        \n",
    "        # Load observation data\n",
    "        obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "        \n",
    "        if not obs_path.exists():\n",
    "            print(f\"Warning: Observation data not found at expected path: {obs_path}\")\n",
    "            print(\"Checking for alternative locations...\")\n",
    "            alt_obs_paths = list(Path(config_dict['CONFLUENCE_DATA_DIR']).glob(\n",
    "                f\"domain_{config_dict['DOMAIN_NAME']}/observations/streamflow/preprocessed/*_streamflow_processed.csv\"))\n",
    "            \n",
    "            if alt_obs_paths:\n",
    "                obs_path = alt_obs_paths[0]\n",
    "                print(f\"Found alternative observation data at: {obs_path}\")\n",
    "            else:\n",
    "                print(\"No observation data found. Only simulations will be displayed.\")\n",
    "        \n",
    "        if obs_path.exists():\n",
    "            print(f\"Loading observation data from: {obs_path}\")\n",
    "            obs_df = pd.read_csv(obs_path)\n",
    "            obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "            obs_df.set_index('datetime', inplace=True)\n",
    "            print(f\"Observation period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "        else:\n",
    "            obs_df = None\n",
    "            \n",
    "        # Find the segment ID for the outlet\n",
    "        reach_id = int(confluence.config.get('SIM_REACH_ID', 0))\n",
    "        print(f\"Using reach ID for outlet: {reach_id}\")\n",
    "        \n",
    "        if 'reachID' in sim_data.variables:\n",
    "            # Find the index of our target reach\n",
    "            reach_indices = np.where(sim_data.reachID.values == reach_id)[0]\n",
    "            \n",
    "            if len(reach_indices) > 0:\n",
    "                reach_idx = reach_indices[0]\n",
    "                print(f\"Found reach ID {reach_id} at index {reach_idx}\")\n",
    "                \n",
    "                # Extract simulated flow at outlet\n",
    "                if 'IRFroutedRunoff' in sim_data.variables:\n",
    "                    print(\"Extracting IRFroutedRunoff variable\")\n",
    "                    \n",
    "                    # Extract flow at the outlet segment\n",
    "                    if 'seg' in sim_data.dims:\n",
    "                        sim_flow = sim_data.IRFroutedRunoff.sel(seg=reach_idx).to_series()\n",
    "                    else:\n",
    "                        sim_flow = sim_data.IRFroutedRunoff.isel(reachID=reach_idx).to_series()\n",
    "                    \n",
    "                    sim_df = pd.DataFrame(sim_flow)\n",
    "                    sim_df.columns = ['discharge_cms']\n",
    "                    \n",
    "                    # Determine common time period if observations exist\n",
    "                    if obs_df is not None:\n",
    "                        # Align to daily timestep for comparison\n",
    "                        obs_daily = obs_df.resample('D').mean()\n",
    "                        sim_daily = sim_df.resample('D').mean()\n",
    "                        \n",
    "                        # Find overlapping time period\n",
    "                        start_date = max(obs_daily.index.min(), sim_daily.index.min())\n",
    "                        end_date = min(obs_daily.index.max(), sim_daily.index.max())\n",
    "                        \n",
    "                        # Advance start date by 1 month to skip initial spinup\n",
    "                        start_date = start_date + pd.DateOffset(months=1)\n",
    "                        \n",
    "                        print(f\"Common data period (after skipping 1 month spinup): {start_date} to {end_date}\")\n",
    "                        \n",
    "                        # Filter to common period\n",
    "                        obs_period = obs_daily.loc[start_date:end_date]\n",
    "                        sim_period = sim_daily.loc[start_date:end_date]\n",
    "                        \n",
    "                        # Calculate performance metrics\n",
    "                        rmse = np.sqrt(((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).mean())\n",
    "                        \n",
    "                        # Calculate Nash-Sutcliffe Efficiency (NSE)\n",
    "                        mean_obs = obs_period['discharge_cms'].mean()\n",
    "                        numerator = ((obs_period['discharge_cms'] - sim_period['discharge_cms'])**2).sum()\n",
    "                        denominator = ((obs_period['discharge_cms'] - mean_obs)**2).sum()\n",
    "                        nse = 1 - (numerator / denominator)\n",
    "                        \n",
    "                        # Calculate Percent Bias (PBIAS)\n",
    "                        pbias = 100 * (sim_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "                        \n",
    "                        # Calculate Kling-Gupta Efficiency (KGE)\n",
    "                        r = obs_period['discharge_cms'].corr(sim_period['discharge_cms'])  # Correlation\n",
    "                        alpha = sim_period['discharge_cms'].std() / obs_period['discharge_cms'].std()  # Relative variability\n",
    "                        beta = sim_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()  # Bias ratio\n",
    "                        kge = 1 - ((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)**0.5\n",
    "                        \n",
    "                        print(f\"Performance metrics:\")\n",
    "                        print(f\"  - RMSE: {rmse:.2f} m¬≥/s\")\n",
    "                        print(f\"  - NSE: {nse:.2f}\")\n",
    "                        print(f\"  - PBIAS: {pbias:.2f}%\")\n",
    "                        print(f\"  - KGE: {kge:.2f}\")\n",
    "                        \n",
    "                        # Create figure with two subplots for time series and flow duration curve\n",
    "                        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 16))\n",
    "                        fig.suptitle(f\"Distributed Model Results - {confluence.config['DOMAIN_NAME'].replace('_', ' ').title()}\", \n",
    "                                     fontsize=16, fontweight='bold')\n",
    "                        \n",
    "                        # Plot time series\n",
    "                        ax1.plot(obs_period.index, obs_period['discharge_cms'], \n",
    "                                 'b-', label='Observed', linewidth=1.5, alpha=0.7)\n",
    "                        ax1.plot(sim_period.index, sim_period['discharge_cms'], \n",
    "                                 'r-', label='Simulated (Distributed)', linewidth=1.5, alpha=0.7)\n",
    "                        \n",
    "                        # Add calibration/evaluation period shading if configured\n",
    "                        if 'CALIBRATION_PERIOD' in confluence.config and 'EVALUATION_PERIOD' in confluence.config:\n",
    "                            cal_start = pd.Timestamp(confluence.config.get('CALIBRATION_PERIOD').split(',')[0].strip())\n",
    "                            cal_end = pd.Timestamp(confluence.config.get('CALIBRATION_PERIOD').split(',')[1].strip())\n",
    "                            eval_start = pd.Timestamp(confluence.config.get('EVALUATION_PERIOD').split(',')[0].strip())\n",
    "                            eval_end = pd.Timestamp(confluence.config.get('EVALUATION_PERIOD').split(',')[1].strip())\n",
    "                            \n",
    "                            # Only shade if within the plot range\n",
    "                            if cal_start <= end_date and cal_end >= start_date:\n",
    "                                valid_cal_start = max(cal_start, start_date)\n",
    "                                valid_cal_end = min(cal_end, end_date)\n",
    "                                ax1.axvspan(valid_cal_start, valid_cal_end, alpha=0.2, color='gray', label='Calibration Period')\n",
    "                            \n",
    "                            if eval_start <= end_date and eval_end >= start_date:\n",
    "                                valid_eval_start = max(eval_start, start_date)\n",
    "                                valid_eval_end = min(eval_end, end_date)\n",
    "                                ax1.axvspan(valid_eval_start, valid_eval_end, alpha=0.2, color='lightblue', label='Evaluation Period')\n",
    "                        \n",
    "                        ax1.set_xlabel('Date', fontsize=12)\n",
    "                        ax1.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "                        ax1.set_title('Streamflow Comparison', fontsize=14)\n",
    "                        ax1.legend(loc='upper right', fontsize=10)\n",
    "                        ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "                        ax1.set_facecolor('#f0f0f0')\n",
    "                        \n",
    "                        # Format x-axis\n",
    "                        ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "                        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                        \n",
    "                        # Add metrics as text\n",
    "                        ax1.text(0.02, 0.95, \n",
    "                                 f\"RMSE: {rmse:.2f} m¬≥/s\\nNSE: {nse:.2f}\\nPBIAS: {pbias:.2f}%\\nKGE: {kge:.2f}\",\n",
    "                                 transform=ax1.transAxes, \n",
    "                                 fontsize=12,\n",
    "                                 bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "                        \n",
    "                        # Plot flow duration curve\n",
    "                        # Sort values in descending order\n",
    "                        obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "                        sim_sorted = sim_period['discharge_cms'].sort_values(ascending=False)\n",
    "                        \n",
    "                        # Calculate exceedance probabilities\n",
    "                        obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "                        sim_ranks = np.arange(1., len(sim_sorted) + 1) / len(sim_sorted)\n",
    "                        \n",
    "                        # Plot Flow Duration Curves\n",
    "                        ax2.loglog(obs_ranks * 100, obs_sorted, 'b-', label='Observed', linewidth=2)\n",
    "                        ax2.loglog(sim_ranks * 100, sim_sorted, 'r-', label='Simulated', linewidth=2)\n",
    "                        \n",
    "                        ax2.set_xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "                        ax2.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "                        ax2.set_title('Flow Duration Curve', fontsize=14)\n",
    "                        ax2.legend(loc='best', fontsize=10)\n",
    "                        ax2.grid(True, which='both', linestyle=':', alpha=0.6)\n",
    "                        ax2.set_facecolor('#f0f0f0')\n",
    "                        \n",
    "                        # Add flow regime regions\n",
    "                        ax2.axvspan(0, 20, alpha=0.2, color='blue', label='High Flows')\n",
    "                        ax2.axvspan(20, 70, alpha=0.2, color='green', label='Medium Flows')\n",
    "                        ax2.axvspan(70, 100, alpha=0.2, color='red', label='Low Flows')\n",
    "                        \n",
    "                        # Save the plot to file\n",
    "                        plot_folder = project_dir / \"plots\" / \"results\"\n",
    "                        plot_folder.mkdir(parents=True, exist_ok=True)\n",
    "                        plot_filename = plot_folder / f\"{confluence.config['EXPERIMENT_ID']}_streamflow_comparison.png\"\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.subplots_adjust(top=0.93)\n",
    "                        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "                        print(f\"Plot saved to: {plot_filename}\")\n",
    "                        \n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        # If no observations, just plot simulation\n",
    "                        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "                        ax.plot(sim_df.index, sim_df['discharge_cms'], \n",
    "                                color='red', linewidth=1.5, label='Simulated (Distributed)')\n",
    "                        \n",
    "                        ax.set_xlabel('Date', fontsize=12)\n",
    "                        ax.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "                        ax.set_title(f'Distributed Model Results - {confluence.config[\"DOMAIN_NAME\"].replace(\"_\", \" \").title()}', \n",
    "                                    fontsize=14, fontweight='bold')\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        ax.legend(fontsize=10)\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\"Error: IRFroutedRunoff variable not found in simulation output\")\n",
    "                    print(f\"Available variables: {list(sim_data.variables)}\")\n",
    "            else:\n",
    "                print(f\"Error: Could not find reach ID {reach_id} in simulation output\")\n",
    "                print(f\"Available reach IDs: {sim_data.reachID.values}\")\n",
    "        else:\n",
    "            print(\"Error: reachID variable not found in simulation output\")\n",
    "            print(f\"Available variables: {list(sim_data.variables)}\")\n",
    "        \n",
    "        # Close the dataset\n",
    "        sim_data.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing simulation results: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "else:\n",
    "    print(\"No simulation results found. Check model execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Optimization and Analysis (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Compare Lumped vs Distributed Results (Optional)\n",
    "\n",
    "If you've completed the lumped model tutorial, we can compare results between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Compare Lumped vs. Distributed Model Results\n",
    "print(\"=== Step 15: Comparing Lumped and Distributed Model Results ===\")\n",
    "\n",
    "# Import necessary libraries if not already imported\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Set paths for both lumped and distributed model results\n",
    "lumped_domain = 'Bow_at_Banff_lumped_tutorial'\n",
    "lumped_sim_path = CONFLUENCE_DATA_DIR / f'domain_{lumped_domain}' / 'simulations' / 'tutorial_run' / 'SUMMA'\n",
    "dist_sim_path = project_dir / 'simulations' / confluence.config['EXPERIMENT_ID'] / 'mizuRoute'\n",
    "\n",
    "# Check if paths exist and find simulation files\n",
    "lumped_sim_files = str(lumped_sim_path / 'tutorial_run_timestep.nc')\n",
    "dist_sim_files = list(dist_sim_path.glob('*.nc')) if dist_sim_path.exists() else []\n",
    "\n",
    "# Check what result files are available\n",
    "if not lumped_sim_files and not dist_sim_files:\n",
    "    print(\"Neither lumped nor distributed model results found. Run both models first for comparison.\")\n",
    "elif not lumped_sim_files:\n",
    "    print(\"Lumped model results not found. Complete Tutorial 2 first for comparison.\")\n",
    "elif not dist_sim_files:\n",
    "    print(\"Distributed simulation results not found. Run the distributed model first.\")\n",
    "else:\n",
    "    print(\"Found both lumped and distributed model results. Creating comparison plot...\")\n",
    "    \n",
    "    try:\n",
    "        # Load lumped simulation data\n",
    "        print(f\"Loading lumped model results from: {lumped_sim_files[0]}\")\n",
    "        lumped_data = xr.open_dataset(lumped_sim_files[0])\n",
    "        \n",
    "        # Load distributed simulation data\n",
    "        print(f\"Loading distributed model results from: {dist_sim_files[0]}\")\n",
    "        dist_data = xr.open_dataset(dist_sim_files[0])\n",
    "        \n",
    "        # Load observation data\n",
    "        obs_path = project_dir / 'observations' / 'streamflow' / 'preprocessed' / f\"{confluence.config['DOMAIN_NAME']}_streamflow_processed.csv\"\n",
    "        \n",
    "        if not obs_path.exists():\n",
    "            print(f\"Observation data not found at: {obs_path}\")\n",
    "            # Try to find observations in the lumped domain directory\n",
    "            alt_obs_path = CONFLUENCE_DATA_DIR / f'domain_{lumped_domain}' / 'observations' / 'streamflow' / 'preprocessed' / f\"{lumped_domain}_streamflow_processed.csv\"\n",
    "            if alt_obs_path.exists():\n",
    "                obs_path = alt_obs_path\n",
    "                print(f\"Found alternative observation data at: {obs_path}\")\n",
    "            else:\n",
    "                print(\"No observation data found. Only simulations will be compared.\")\n",
    "        \n",
    "        obs_df = None\n",
    "        if obs_path.exists():\n",
    "            print(f\"Loading observation data from: {obs_path}\")\n",
    "            obs_df = pd.read_csv(obs_path)\n",
    "            obs_df['datetime'] = pd.to_datetime(obs_df['datetime'])\n",
    "            obs_df.set_index('datetime', inplace=True)\n",
    "            print(f\"Observation period: {obs_df.index.min()} to {obs_df.index.max()}\")\n",
    "        \n",
    "        # Define reach IDs for each model\n",
    "        lumped_reach_id = int(config_dict.get('SIM_REACH_ID', 0))\n",
    "        dist_reach_id = int(confluence.config.get('SIM_REACH_ID', 0))\n",
    "        \n",
    "        # Extract flows based on available structure\n",
    "        lumped_flow = None\n",
    "        dist_flow = None\n",
    "        \n",
    "        # Extract lumped flow\n",
    "        print(f\"Extracting lumped model flow for reach ID: {lumped_reach_id}\")\n",
    "        if 'reachID' in lumped_data.variables and 'IRFroutedRunoff' in lumped_data.variables:\n",
    "            reach_indices = np.where(lumped_data.reachID.values == lumped_reach_id)[0]\n",
    "            if len(reach_indices) > 0:\n",
    "                reach_idx = reach_indices[0]\n",
    "                if 'seg' in lumped_data.dims:\n",
    "                    lumped_flow = lumped_data.IRFroutedRunoff.sel(seg=reach_idx).to_series()\n",
    "                else:\n",
    "                    lumped_flow = lumped_data.IRFroutedRunoff.isel(reachID=reach_idx).to_series()\n",
    "                \n",
    "                lumped_df = pd.DataFrame(lumped_flow)\n",
    "                lumped_df.columns = ['discharge_cms']\n",
    "            else:\n",
    "                print(f\"Warning: Reach ID {lumped_reach_id} not found in lumped model output\")\n",
    "                print(f\"Available reach IDs: {lumped_data.reachID.values}\")\n",
    "        else:\n",
    "            print(\"Warning: Required variables not found in lumped model output\")\n",
    "            print(f\"Available variables: {list(lumped_data.variables)}\")\n",
    "        \n",
    "        # Extract distributed flow\n",
    "        print(f\"Extracting distributed model flow for reach ID: {dist_reach_id}\")\n",
    "        if 'reachID' in dist_data.variables and 'IRFroutedRunoff' in dist_data.variables:\n",
    "            reach_indices = np.where(dist_data.reachID.values == dist_reach_id)[0]\n",
    "            if len(reach_indices) > 0:\n",
    "                reach_idx = reach_indices[0]\n",
    "                if 'seg' in dist_data.dims:\n",
    "                    dist_flow = dist_data.IRFroutedRunoff.sel(seg=reach_idx).to_series()\n",
    "                else:\n",
    "                    dist_flow = dist_data.IRFroutedRunoff.isel(reachID=reach_idx).to_series()\n",
    "                \n",
    "                dist_df = pd.DataFrame(dist_flow)\n",
    "                dist_df.columns = ['discharge_cms']\n",
    "            else:\n",
    "                print(f\"Warning: Reach ID {dist_reach_id} not found in distributed model output\")\n",
    "                print(f\"Available reach IDs: {dist_data.reachID.values}\")\n",
    "        else:\n",
    "            print(\"Warning: Required variables not found in distributed model output\")\n",
    "            print(f\"Available variables: {list(dist_data.variables)}\")\n",
    "        \n",
    "        # Proceed only if both flows are extracted\n",
    "        if lumped_flow is not None and dist_flow is not None:\n",
    "            # Resample to daily for comparison\n",
    "            lumped_daily = lumped_df.resample('D').mean()\n",
    "            dist_daily = dist_df.resample('D').mean()\n",
    "            \n",
    "            # Determine common time period\n",
    "            if obs_df is not None:\n",
    "                obs_daily = obs_df.resample('D').mean()\n",
    "                start_date = max(obs_daily.index.min(), lumped_daily.index.min(), dist_daily.index.min())\n",
    "                end_date = min(obs_daily.index.max(), lumped_daily.index.max(), dist_daily.index.max())\n",
    "            else:\n",
    "                start_date = max(lumped_daily.index.min(), dist_daily.index.min())\n",
    "                end_date = min(lumped_daily.index.max(), dist_daily.index.max())\n",
    "            \n",
    "            # Advance start date by 1 month to skip spinup\n",
    "            start_date = start_date + pd.DateOffset(months=1)\n",
    "            \n",
    "            print(f\"Common comparison period (after skipping 1 month spinup): {start_date} to {end_date}\")\n",
    "            \n",
    "            # Filter to common period\n",
    "            lumped_period = lumped_daily.loc[start_date:end_date]\n",
    "            dist_period = dist_daily.loc[start_date:end_date]\n",
    "            \n",
    "            if obs_df is not None:\n",
    "                obs_period = obs_daily.loc[start_date:end_date]\n",
    "                \n",
    "                # Calculate metrics for both models\n",
    "                metrics = []\n",
    "                \n",
    "                # Lumped model metrics\n",
    "                lumped_rmse = np.sqrt(((obs_period['discharge_cms'] - lumped_period['discharge_cms'])**2).mean())\n",
    "                lumped_nse = 1 - (((obs_period['discharge_cms'] - lumped_period['discharge_cms'])**2).sum() / \n",
    "                                 ((obs_period['discharge_cms'] - obs_period['discharge_cms'].mean())**2).sum())\n",
    "                lumped_pbias = 100 * (lumped_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "                \n",
    "                # Calculate Kling-Gupta Efficiency for lumped model\n",
    "                r_lumped = obs_period['discharge_cms'].corr(lumped_period['discharge_cms'])\n",
    "                alpha_lumped = lumped_period['discharge_cms'].std() / obs_period['discharge_cms'].std()\n",
    "                beta_lumped = lumped_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()\n",
    "                kge_lumped = 1 - ((r_lumped - 1)**2 + (alpha_lumped - 1)**2 + (beta_lumped - 1)**2)**0.5\n",
    "                \n",
    "                metrics.append({\n",
    "                    'model': 'Lumped',\n",
    "                    'RMSE': f\"{lumped_rmse:.2f} m¬≥/s\",\n",
    "                    'NSE': f\"{lumped_nse:.3f}\",\n",
    "                    'PBIAS': f\"{lumped_pbias:.2f}%\",\n",
    "                    'KGE': f\"{kge_lumped:.3f}\"\n",
    "                })\n",
    "                \n",
    "                # Distributed model metrics\n",
    "                dist_rmse = np.sqrt(((obs_period['discharge_cms'] - dist_period['discharge_cms'])**2).mean())\n",
    "                dist_nse = 1 - (((obs_period['discharge_cms'] - dist_period['discharge_cms'])**2).sum() / \n",
    "                               ((obs_period['discharge_cms'] - obs_period['discharge_cms'].mean())**2).sum())\n",
    "                dist_pbias = 100 * (dist_period['discharge_cms'].sum() - obs_period['discharge_cms'].sum()) / obs_period['discharge_cms'].sum()\n",
    "                \n",
    "                # Calculate Kling-Gupta Efficiency for distributed model\n",
    "                r_dist = obs_period['discharge_cms'].corr(dist_period['discharge_cms'])\n",
    "                alpha_dist = dist_period['discharge_cms'].std() / obs_period['discharge_cms'].std()\n",
    "                beta_dist = dist_period['discharge_cms'].mean() / obs_period['discharge_cms'].mean()\n",
    "                kge_dist = 1 - ((r_dist - 1)**2 + (alpha_dist - 1)**2 + (beta_dist - 1)**2)**0.5\n",
    "                \n",
    "                metrics.append({\n",
    "                    'model': 'Distributed',\n",
    "                    'RMSE': f\"{dist_rmse:.2f} m¬≥/s\",\n",
    "                    'NSE': f\"{dist_nse:.3f}\",\n",
    "                    'PBIAS': f\"{dist_pbias:.2f}%\",\n",
    "                    'KGE': f\"{kge_dist:.3f}\"\n",
    "                })\n",
    "                \n",
    "                # Print metrics table\n",
    "                print(\"\\nPerformance Metrics Comparison:\")\n",
    "                metrics_df = pd.DataFrame(metrics).set_index('model')\n",
    "                print(metrics_df)\n",
    "            \n",
    "            # Create figure\n",
    "            fig = plt.figure(figsize=(15, 15))\n",
    "            gs = gridspec.GridSpec(3, 1, height_ratios=[2, 1, 1])\n",
    "            \n",
    "            # Timeseries plot\n",
    "            ax1 = fig.add_subplot(gs[0])\n",
    "            \n",
    "            # Plot observations if available\n",
    "            if obs_df is not None:\n",
    "                ax1.plot(obs_period.index, obs_period['discharge_cms'], \n",
    "                        color='black', linewidth=2, label='Observed', zorder=3)\n",
    "            \n",
    "            # Plot lumped model results\n",
    "            ax1.plot(lumped_period.index, lumped_period['discharge_cms'], \n",
    "                    color='#1f77b4', linewidth=1.5, alpha=0.8, label='Lumped Model', zorder=2)\n",
    "            \n",
    "            # Plot distributed model results\n",
    "            ax1.plot(dist_period.index, dist_period['discharge_cms'], \n",
    "                    color='#ff7f0e', linewidth=1.5, alpha=0.8, label='Distributed Model', zorder=1)\n",
    "            \n",
    "            # Add calibration/evaluation period shading if configured\n",
    "            if 'CALIBRATION_PERIOD' in confluence.config and 'EVALUATION_PERIOD' in confluence.config:\n",
    "                cal_start = pd.Timestamp(confluence.config.get('CALIBRATION_PERIOD').split(',')[0].strip())\n",
    "                cal_end = pd.Timestamp(confluence.config.get('CALIBRATION_PERIOD').split(',')[1].strip())\n",
    "                eval_start = pd.Timestamp(confluence.config.get('EVALUATION_PERIOD').split(',')[0].strip())\n",
    "                eval_end = pd.Timestamp(confluence.config.get('EVALUATION_PERIOD').split(',')[1].strip())\n",
    "                \n",
    "                # Only shade if within the plot range\n",
    "                if cal_start <= end_date and cal_end >= start_date:\n",
    "                    valid_cal_start = max(cal_start, start_date)\n",
    "                    valid_cal_end = min(cal_end, end_date)\n",
    "                    ax1.axvspan(valid_cal_start, valid_cal_end, alpha=0.2, color='gray', label='Calibration Period')\n",
    "                \n",
    "                if eval_start <= end_date and eval_end >= start_date:\n",
    "                    valid_eval_start = max(eval_start, start_date)\n",
    "                    valid_eval_end = min(eval_end, end_date)\n",
    "                    ax1.axvspan(valid_eval_start, valid_eval_end, alpha=0.2, color='lightblue', label='Evaluation Period')\n",
    "            \n",
    "            ax1.set_xlabel('Date', fontsize=12)\n",
    "            ax1.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "            ax1.set_title('Lumped vs Distributed Model Comparison', fontsize=14, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend(fontsize=10, loc='upper right')\n",
    "            \n",
    "            # Format x-axis to show years\n",
    "            ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            \n",
    "            # Add metrics if observations are available\n",
    "            if obs_df is not None:\n",
    "                # Add metrics table as text\n",
    "                metrics_text = \"Performance Metrics:\\n\"\n",
    "                metrics_text += f\"Lumped Model:    RMSE: {lumped_rmse:.2f} m¬≥/s    NSE: {lumped_nse:.3f}    KGE: {kge_lumped:.3f}\\n\"\n",
    "                metrics_text += f\"Distributed Model:    RMSE: {dist_rmse:.2f} m¬≥/s    NSE: {dist_nse:.3f}    KGE: {kge_dist:.3f}\"\n",
    "                \n",
    "                ax1.text(0.01, 0.02, metrics_text, transform=ax1.transAxes,\n",
    "                        bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
    "                        fontsize=10, verticalalignment='bottom')\n",
    "            \n",
    "            # Flow Duration Curve\n",
    "            ax2 = fig.add_subplot(gs[1])\n",
    "            \n",
    "            if obs_df is not None:\n",
    "                # Sort values in descending order\n",
    "                obs_sorted = obs_period['discharge_cms'].sort_values(ascending=False)\n",
    "                lumped_sorted = lumped_period['discharge_cms'].sort_values(ascending=False)\n",
    "                dist_sorted = dist_period['discharge_cms'].sort_values(ascending=False)\n",
    "                \n",
    "                # Calculate exceedance probabilities\n",
    "                obs_ranks = np.arange(1., len(obs_sorted) + 1) / len(obs_sorted)\n",
    "                lumped_ranks = np.arange(1., len(lumped_sorted) + 1) / len(lumped_sorted)\n",
    "                dist_ranks = np.arange(1., len(dist_sorted) + 1) / len(dist_sorted)\n",
    "                \n",
    "                # Plot Flow Duration Curves\n",
    "                ax2.semilogy(obs_ranks * 100, obs_sorted, 'k-', label='Observed', linewidth=2)\n",
    "                ax2.semilogy(lumped_ranks * 100, lumped_sorted, '-', color='#1f77b4', label='Lumped Model', linewidth=1.5)\n",
    "                ax2.semilogy(dist_ranks * 100, dist_sorted, '-', color='#ff7f0e', label='Distributed Model', linewidth=1.5)\n",
    "                \n",
    "                ax2.set_xlabel('Exceedance Probability (%)', fontsize=12)\n",
    "                ax2.set_ylabel('Discharge (m¬≥/s)', fontsize=12)\n",
    "                ax2.set_title('Flow Duration Curve', fontsize=14)\n",
    "                ax2.legend(loc='best', fontsize=10)\n",
    "                ax2.grid(True, which='both', alpha=0.3)\n",
    "                \n",
    "                # Add flow regime regions\n",
    "                ax2.axvspan(0, 20, alpha=0.2, color='blue')\n",
    "                ax2.axvspan(20, 70, alpha=0.2, color='green')\n",
    "                ax2.axvspan(70, 100, alpha=0.2, color='red')\n",
    "                \n",
    "                # Add text labels for flow regions\n",
    "                ax2.text(10, ax2.get_ylim()[1] * 0.8, 'High Flows', fontsize=10, ha='center')\n",
    "                ax2.text(45, ax2.get_ylim()[1] * 0.1, 'Medium Flows', fontsize=10, ha='center')\n",
    "                ax2.text(85, ax2.get_ylim()[1] * 0.02, 'Low Flows', fontsize=10, ha='center')\n",
    "            \n",
    "            # Error analysis\n",
    "            ax3 = fig.add_subplot(gs[2])\n",
    "            \n",
    "            if obs_df is not None:\n",
    "                # Calculate errors for both models\n",
    "                lumped_error = lumped_period['discharge_cms'] - obs_period['discharge_cms']\n",
    "                dist_error = dist_period['discharge_cms'] - obs_period['discharge_cms']\n",
    "                \n",
    "                # Plot errors\n",
    "                ax3.plot(lumped_period.index, lumped_error, '-', color='#1f77b4', label='Lumped Model Error', alpha=0.7)\n",
    "                ax3.plot(dist_period.index, dist_error, '-', color='#ff7f0e', label='Distributed Model Error', alpha=0.7)\n",
    "                \n",
    "                # Add zero line\n",
    "                ax3.axhline(y=0, color='k', linestyle='-', linewidth=0.8)\n",
    "                \n",
    "                ax3.set_xlabel('Date', fontsize=12)\n",
    "                ax3.set_ylabel('Error (m¬≥/s)', fontsize=12)\n",
    "                ax3.set_title('Model Error (Simulated - Observed)', fontsize=14)\n",
    "                ax3.legend(fontsize=10)\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Format x-axis to match the first plot\n",
    "                ax3.xaxis.set_major_locator(mdates.YearLocator())\n",
    "                ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            \n",
    "            # Save and show the plot\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            plot_folder = project_dir / \"plots\" / \"results\"\n",
    "            plot_folder.mkdir(parents=True, exist_ok=True)\n",
    "            plot_filename = plot_folder / 'lumped_vs_distributed_comparison.png'\n",
    "            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to: {plot_filename}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # Close datasets\n",
    "            lumped_data.close()\n",
    "            dist_data.close()\n",
    "        else:\n",
    "            print(\"Error: Failed to extract flow data from one or both models.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during comparison: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run the complete workflow in one step\n",
    "# (Uncomment to use this instead of the step-by-step approach)\n",
    "\n",
    "# confluence.run_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
