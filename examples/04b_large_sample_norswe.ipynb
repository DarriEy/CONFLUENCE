{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 9: NorSWE Large Sample Study (Snow Observation Network)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial extends our large sample studies approach to focus specifically on snow hydrology validation using the NorSWE (Northern Hemisphere Snow Water Equivalent) dataset. Building on the multi-site analysis framework demonstrated with FLUXNET, we now apply CONFLUENCE to systematically evaluate snow modeling performance across a network of snow observation stations throughout the northern hemisphere.\n",
    "\n",
    "### NorSWE: A Critical Snow Observation Network\n",
    "\n",
    "The NorSWE dataset represents one of the most comprehensive collections of snow observations available for hydrological model validation:\n",
    "\n",
    "**Spatial Coverage**:\n",
    "- **Northern Hemisphere focus**: Stations across snow-dominated regions\n",
    "- **Nordic emphasis**: Dense coverage in Scandinavia, Finland, and Norway\n",
    "- **Elevation gradients**: From coastal lowlands to high mountain regions\n",
    "- **Climate diversity**: Maritime, continental, and Arctic snow climates\n",
    "\n",
    "**Observational Richness**:\n",
    "- **Snow Water Equivalent (SWE)**: Direct measurements of snow mass\n",
    "- **Snow Depth**: Complementary snow pack structure information\n",
    "- **Long-term records**: Multi-decade observations at many sites\n",
    "- **Quality control**: Standardized measurement protocols and data processing\n",
    "\n",
    "### Scientific Importance of Snow Validation\n",
    "\n",
    "Snow processes represent some of the most challenging aspects of hydrological modeling:\n",
    "\n",
    "**Physical Complexity**:\n",
    "- **Phase transitions**: Freezing, melting, and sublimation processes\n",
    "- **Energy balance**: Complex interactions between radiation, temperature, and wind\n",
    "- **Layered structure**: Metamorphism and density changes within the snowpack\n",
    "- **Spatial variability**: Strong elevation and aspect dependencies\n",
    "\n",
    "**Hydrological Significance**:\n",
    "- **Seasonal storage**: Snow acts as a natural reservoir in many regions\n",
    "- **Timing control**: Snowmelt timing affects peak flows and water availability\n",
    "- **Climate sensitivity**: Snow processes are highly sensitive to temperature changes\n",
    "- **Extreme events**: Snow-rain transitions and rain-on-snow events\n",
    "\n",
    "### Why NorSWE for Large Sample Snow Studies?\n",
    "\n",
    "NorSWE provides unique advantages for systematic snow model evaluation:\n",
    "\n",
    "1. **Process Focus**: Dedicated snow observations rather than mixed-variable datasets\n",
    "2. **Measurement Quality**: Direct SWE measurements provide unambiguous validation targets\n",
    "3. **Environmental Gradients**: Sites span elevation, latitude, and climate gradients\n",
    "4. **Seasonal Dynamics**: Full seasonal cycle observations capture accumulation and ablation\n",
    "5. **Nordic Expertise**: Stations operated by countries with world-leading snow science\n",
    "\n",
    "### Research Questions for Snow Modeling\n",
    "\n",
    "Large sample studies with NorSWE enable investigation of critical snow science questions:\n",
    "\n",
    "1. **Model Physics**: How well do different snow process representations perform across environments?\n",
    "2. **Climate Controls**: Which meteorological variables most strongly control snow accumulation and melt?\n",
    "3. **Elevation Effects**: How do snow processes change with elevation and their representation in models?\n",
    "4. **Regional Patterns**: Are there systematic regional biases in snow modeling?\n",
    "5. **Seasonal Dynamics**: Can models capture both accumulation and ablation processes accurately?\n",
    "\n",
    "### Unique Challenges of Snow Modeling\n",
    "\n",
    "Snow modeling presents distinct challenges compared to other hydrological processes:\n",
    "\n",
    "**Meteorological Sensitivity**:\n",
    "- **Temperature thresholds**: Critical temperature for rain-snow transitions\n",
    "- **Radiation balance**: Complex interactions between shortwave and longwave radiation\n",
    "- **Wind effects**: Redistribution and sublimation processes\n",
    "- **Humidity control**: Sublimation rates and surface energy balance\n",
    "\n",
    "**Temporal Dynamics**:\n",
    "- **Seasonal cycle**: Distinct accumulation and ablation seasons\n",
    "- **Diurnal variation**: Strong daily cycles in energy balance\n",
    "- **Event-based processes**: Individual storm impacts on snowpack\n",
    "- **Intermittency**: Episodic accumulation and melt events\n",
    "\n",
    "### CONFLUENCE's Snow Modeling Capabilities\n",
    "\n",
    "CONFLUENCE's integration with SUMMA provides sophisticated snow modeling capabilities:\n",
    "\n",
    "**Advanced Snow Physics**:\n",
    "- **Multi-layer snowpack**: Explicit representation of snow stratigraphy\n",
    "- **Energy balance**: Detailed surface energy balance calculations\n",
    "- **Metamorphism**: Snow density and thermal property evolution\n",
    "- **Liquid water**: Representation of liquid water flow through snow\n",
    "\n",
    "**Flexible Parameterizations**:\n",
    "- **Multiple options**: Different approaches for key snow processes\n",
    "- **Sensitivity analysis**: Test different process representations\n",
    "- **Decision analysis**: Compare alternative model structures\n",
    "- **Uncertainty quantification**: Assess parameter and structural uncertainty\n",
    "\n",
    "### NorSWE vs. FLUXNET: Complementary Approaches\n",
    "\n",
    "While FLUXNET focused on energy balance validation, NorSWE provides complementary insights:\n",
    "\n",
    "| Aspect | FLUXNET | NorSWE |\n",
    "|--------|---------|--------|\n",
    "| **Focus** | Energy/carbon fluxes | Snow mass/depth |\n",
    "| **Process** | Continuous processes | Seasonal accumulation |\n",
    "| **Validation** | Flux measurements | State variables |\n",
    "| **Complexity** | Ecosystem interactions | Phase change physics |\n",
    "| **Temporal** | Year-round | Seasonal focus |\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates several key capabilities for snow-focused large sample studies:\n",
    "\n",
    "1. **Snow-Specific Configuration**: Adapt CONFLUENCE configurations for snow observation sites\n",
    "2. **Seasonal Analysis**: Focus on snow accumulation and ablation periods\n",
    "3. **Multi-Variable Validation**: Compare both SWE and snow depth simulations\n",
    "4. **Elevation Analysis**: Examine how model performance varies with elevation\n",
    "5. **Climate Sensitivity**: Assess model performance across different snow climates\n",
    "\n",
    "### Methodological Considerations\n",
    "\n",
    "Snow-focused large sample studies require specific methodological approaches:\n",
    "\n",
    "**Site Selection**:\n",
    "- **Elevation gradients**: Represent different snow accumulation zones\n",
    "- **Climate diversity**: Include maritime, continental, and Arctic sites\n",
    "- **Data quality**: Ensure reliable SWE and snow depth measurements\n",
    "- **Temporal coverage**: Adequate seasonal cycle representation\n",
    "\n",
    "**Analysis Approaches**:\n",
    "- **Seasonal statistics**: Focus on peak SWE, melt timing, and duration\n",
    "- **Process evaluation**: Assess accumulation vs. ablation performance\n",
    "- **Threshold analysis**: Evaluate temperature and precipitation thresholds\n",
    "- **Extreme events**: Analyze performance during unusual snow years\n",
    "\n",
    "### Tutorial Structure\n",
    "\n",
    "This tutorial follows the established large sample framework while emphasizing snow-specific aspects:\n",
    "\n",
    "1. **NorSWE Site Selection**: Choose representative sites across snow environments\n",
    "2. **Snow-Focused Configuration**: Adapt CONFLUENCE for snow observation validation\n",
    "3. **Seasonal Analysis Setup**: Configure for snow season evaluation\n",
    "4. **Batch Processing**: Execute CONFLUENCE across multiple snow sites\n",
    "5. **Snow-Specific Results**: Collect and analyze SWE and snow depth outputs\n",
    "6. **Elevation Analysis**: Examine performance across elevation gradients\n",
    "7. **Climate Comparison**: Compare results across different snow climates\n",
    "\n",
    "### Scientific Impact\n",
    "\n",
    "NorSWE large sample studies contribute to advancing snow science:\n",
    "\n",
    "- **Model Validation**: Systematic evaluation of snow process representations\n",
    "- **Process Understanding**: Identify key controls on snow accumulation and melt\n",
    "- **Climate Applications**: Improve projections of snow under changing climate\n",
    "- **Operational Applications**: Enhance seasonal forecasting and water management\n",
    "- **Uncertainty Assessment**: Quantify reliability of snow predictions\n",
    "\n",
    "### Building on Previous Tutorials\n",
    "\n",
    "This tutorial leverages all the skills developed throughout the CONFLUENCE series:\n",
    "\n",
    "- **Point-scale understanding**: Foundation in vertical snow processes\n",
    "- **Workflow automation**: Efficient multi-site processing\n",
    "- **Configuration management**: Template-based site setup\n",
    "- **Results analysis**: Statistical evaluation of multi-site results\n",
    "- **Visualization**: Clear presentation of spatial and temporal patterns\n",
    "\n",
    "By applying these skills to snow-focused validation, you'll gain expertise in one of the most challenging aspects of hydrological modeling while contributing to improved understanding of snow processes across diverse northern hemisphere environments.\n",
    "\n",
    "The combination of CONFLUENCE's sophisticated snow modeling capabilities with NorSWE's comprehensive observation network provides a powerful framework for advancing snow science through systematic, large sample analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Large Sample Snow Study Experimental Design and Site Selection\n",
    "Transitioning from the FLUXNET energy balance focus to systematic snow hydrology validation, this step establishes the foundation for large sample snow modeling using the comprehensive NorSWE observation network. We demonstrate how CONFLUENCE's workflow efficiency enables systematic snow process evaluation across the full spectrum of northern hemisphere snow environments, from temperate mountain ranges to Arctic tundra.\n",
    "\n",
    "### Snow Modeling Evolution: Energy Balance → Snow Process Physics\n",
    "\n",
    "**From FLUXNET to NorSWE**: Complementary large sample approaches addressing different Earth system processes\n",
    "- **FLUXNET focus**: Energy and carbon fluxes across diverse ecosystems and climate zones\n",
    "- **NorSWE focus**: Snow mass and depth dynamics across elevation and climate gradients\n",
    "- **Process emphasis**: Continuous energy exchange → Seasonal accumulation and ablation dynamics\n",
    "- **Validation targets**: Flux measurements → State variable validation with strong seasonal signals\n",
    "- **Environmental controls**: Ecosystem-climate interactions → Topographic-climate interactions\n",
    "\n",
    "### The Unique Challenge of Snow Hydrology at Scale\n",
    "\n",
    "Snow processes present distinct modeling challenges that require specialized large sample approaches:\n",
    "\n",
    "**Physical Process Complexity**:\n",
    "- **Phase transitions**: Accurate representation of freezing, melting, and sublimation across diverse environments\n",
    "- **Energy balance**: Complex multi-component surface energy balance varying with snow cover evolution\n",
    "- **Layered snowpack dynamics**: Multi-layer snow metamorphism, density evolution, and thermal property changes\n",
    "- **Spatial variability**: Strong elevation, aspect, and microclimate dependencies requiring careful upscaling\n",
    "\n",
    "**Temporal Process Dynamics**:\n",
    "- **Seasonal accumulation**: Episodic storm-by-storm snow accumulation throughout winter months\n",
    "- **Ablation complexity**: Energy-driven melt processes with strong diurnal and seasonal cycles\n",
    "- **Intermittent dynamics**: Snow-rain transitions, sublimation events, and refreeze cycles\n",
    "- **Critical timing**: Peak SWE timing, melt onset, and snow disappearance dates affecting water resources\n",
    "\n",
    "**Environmental Control Complexity**:\n",
    "- **Elevation gradients**: Systematic changes in temperature, precipitation phase, and energy balance\n",
    "- **Climate regime diversity**: Maritime, continental, and Arctic snow climates with distinct process signatures\n",
    "- **Latitude effects**: Radiation balance changes affecting snow energy balance across climate zones\n",
    "- **Topographic interactions**: Slope, aspect, and shelter effects on snow accumulation and ablation\n",
    "\n",
    "### NorSWE: Premier Snow Observation Network for Large Sample Studies\n",
    "\n",
    "The Northern Hemisphere Snow Water Equivalent (NorSWE) dataset provides unparalleled opportunities for systematic snow model validation:\n",
    "\n",
    "**Comprehensive Spatial Coverage**:\n",
    "- **Northern Hemisphere scope**: Extensive coverage across all major snow-dominated regions\n",
    "- **Elevation gradients**: From coastal lowlands to high mountain environments (0-3000+ m)\n",
    "- **Climate diversity**: Maritime, continental, boreal, and Arctic snow climate regimes\n",
    "- **Multi-national network**: Coordinated observations from leading snow science institutions\n",
    "\n",
    "**High-Quality Observations**:\n",
    "- **Direct SWE measurements**: Snow Water Equivalent providing unambiguous validation targets for snow mass\n",
    "- **Complementary snow depth**: Structural validation enabling density and compaction process evaluation\n",
    "- **Quality-controlled data**: Standardized measurement protocols and comprehensive data validation\n",
    "- **Long-term records**: Multi-decade observations enabling robust statistical analysis and trend assessment\n",
    "\n",
    "**Process-Focused Design**:\n",
    "- **Snow-specific measurements**: Dedicated snow observations rather than mixed-variable datasets\n",
    "- **Seasonal dynamics**: Complete seasonal cycle capture from accumulation through ablation\n",
    "- **Event resolution**: Sufficient temporal resolution to capture individual storm and melt events\n",
    "- **Measurement precision**: High-accuracy observations suitable for detailed model validation\n",
    "\n",
    "### Strategic Site Selection for Snow Process Understanding\n",
    "\n",
    "Large sample snow studies require specialized site selection strategies that differ from energy balance applications:\n",
    "\n",
    "**Environmental Gradient Prioritization**:\n",
    "- **Elevation stratification**: Systematic sampling across elevation zones representing different snow climates\n",
    "- **Climate regime coverage**: Maritime, continental, and Arctic environments with distinct snow physics\n",
    "- **Latitude transects**: Temperature and radiation gradients affecting snow accumulation and ablation\n",
    "- **Regional representation**: Coverage of major snow-dominated mountain ranges and Arctic regions\n",
    "\n",
    "**Data Quality Assessment**:\n",
    "- **Multi-variable completeness**: Both SWE and snow depth data availability for comprehensive validation\n",
    "- **Seasonal coverage**: Adequate representation of both accumulation and ablation seasons\n",
    "- **Measurement precision**: High-quality observations suitable for detailed process evaluation\n",
    "- **Temporal consistency**: Long-term records enabling robust statistical analysis\n",
    "\n",
    "**Process Validation Objectives**:\n",
    "- **Mass balance validation**: SWE observations providing fundamental snow mass validation\n",
    "- **Structural validation**: Snow depth enabling snowpack density and compaction assessment\n",
    "- **Seasonal timing**: Peak SWE, melt onset, and snow disappearance timing validation\n",
    "- **Process-specific metrics**: Accumulation rate, ablation rate, and persistence evaluation\n",
    "\n",
    "### Advanced Snow Physics Through SUMMA Integration\n",
    "\n",
    "CONFLUENCE's integration with SUMMA provides sophisticated snow modeling capabilities essential for process-based validation:\n",
    "\n",
    "**Multi-Layer Snowpack Representation**:\n",
    "- **Explicit snow stratigraphy**: Layer-by-layer representation of snow metamorphism and density evolution\n",
    "- **Thermal properties**: Dynamic snow thermal conductivity and heat capacity based on density and temperature\n",
    "- **Liquid water dynamics**: Representation of liquid water percolation, retention, and refreezing processes\n",
    "- **Snow-vegetation interactions**: Canopy interception, sublimation, and under-canopy snow processes\n",
    "\n",
    "**Comprehensive Energy Balance**:\n",
    "- **Radiation components**: Shortwave and longwave radiation balance with snow albedo evolution\n",
    "- **Turbulent fluxes**: Sensible and latent heat exchange with detailed aerodynamic formulations\n",
    "- **Ground heat flux**: Soil-snow interface heat transfer affecting basal melt processes\n",
    "- **Precipitation phase**: Temperature-dependent rain-snow partitioning algorithms\n",
    "\n",
    "**Flexible Process Parameterizations**:\n",
    "- **Multiple options**: Alternative formulations for key snow processes enabling systematic evaluation\n",
    "- **Decision analysis**: Comparison of different process representations across environmental gradients\n",
    "- **Parameter sensitivity**: Assessment of parameter uncertainty across diverse snow environments\n",
    "- **Process uncertainty**: Quantification of structural uncertainty in snow process representation\n",
    "\n",
    "### Scientific Innovation: Continental-Scale Snow Process Validation\n",
    "\n",
    "This large sample approach enables breakthrough capabilities for snow science:\n",
    "\n",
    "❄️ **Process Generalization**: Statistical identification of universal snow process patterns vs. environment-specific behaviors across northern hemisphere snow regimes\n",
    "\n",
    "⛰️ **Elevation-Climate Synthesis**: Systematic quantification of how snow processes change across elevation and climate gradients\n",
    "\n",
    "🌨️ **Seasonal Dynamics**: Comprehensive evaluation of both accumulation and ablation season process representation in models\n",
    "\n",
    "📊 **Multi-Variable Integration**: Combined SWE and snow depth validation providing both mass balance and structural validation\n",
    "\n",
    "🔍 **Uncertainty Quantification**: Robust assessment of snow model reliability across diverse northern hemisphere environments\n",
    "\n",
    "📈 **Model Physics Evaluation**: Systematic testing of snow energy balance and phase change representations across environmental gradients\n",
    "\n",
    "The experimental design demonstrated here establishes the foundation for transforming snow hydrology from individual mountain case studies to systematic, statistically robust analysis across the full spectrum of northern hemisphere snow environments, enabling confident identification of universal snow process patterns while quantifying regional variations and model uncertainties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: LARGE SAMPLE SNOW STUDY EXPERIMENTAL DESIGN AND SITE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style for snow visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"coolwarm\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=== CONFLUENCE Tutorial 04b: NorSWE Large Sample Snow Study ===\")\n",
    "print(\"Snow hydrology scaling: Single sites to systematic northern hemisphere validation\")\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE SNOW EXPERIMENTAL DESIGN CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n❄️ Large Sample Snow Experimental Design Configuration...\")\n",
    "\n",
    "# Define the large sample snow experiment parameters\n",
    "snow_sample_config = {\n",
    "    # Experiment identification\n",
    "    'experiment_name': 'norswe_large_sample_tutorial',\n",
    "    'experiment_type': 'multi_site_snow_validation',\n",
    "    'analysis_scale': 'northern_hemisphere_snow_gradients',\n",
    "    \n",
    "    # Site selection criteria specific to snow hydrology\n",
    "    'max_stations': 25,  # Manageable number for tutorial demonstration\n",
    "    'site_selection_strategy': 'elevation_climate_diversity',\n",
    "    'min_swe_completeness': 30.0,  # Minimum % SWE data completeness\n",
    "    'min_depth_completeness': 30.0,  # Minimum % snow depth data completeness\n",
    "    'elevation_range': (0, 3000),  # Elevation range for snow sites (meters)\n",
    "    'latitude_range': (45, 75),   # Northern snow regions focus\n",
    "    \n",
    "    # Data and processing configuration\n",
    "    'norswe_path': '/work/comphyd_lab/data/geospatial-data/NorSWE/NorSWE-NorEEN_1979-2021_v2.nc',\n",
    "    'template_config': '../CONFLUENCE/0_config_files/config_norswe_template.yaml',\n",
    "    'config_output_dir': '../CONFLUENCE/0_config_files/norswe',\n",
    "    'norswe_script': './run_norswe-2.py',\n",
    "    'base_data_path': '/work/comphyd_lab/data/CONFLUENCE_data/norswe',\n",
    "    \n",
    "    # Temporal filtering for focused analysis\n",
    "    'start_year': 2010,\n",
    "    'end_year': 2020,\n",
    "    'focus_months': [10, 11, 12, 1, 2, 3, 4, 5],  # Snow season months\n",
    "    \n",
    "    # Processing options\n",
    "    'batch_processing': True,\n",
    "    'parallel_execution': True,\n",
    "    'dry_run_mode': False,  # Set to True for testing without job submission\n",
    "    \n",
    "    # Snow-specific analysis objectives\n",
    "    'primary_variables': ['SWE', 'snow_depth', 'snow_season_length', 'peak_SWE_timing'],\n",
    "    'comparison_metrics': ['correlation', 'rmse', 'bias', 'nse', 'seasonal_timing'],\n",
    "    'snow_processes': ['accumulation', 'ablation', 'seasonal_dynamics', 'elevation_gradients']\n",
    "}\n",
    "\n",
    "print(f\"✅ Snow experimental design configured\")\n",
    "print(f\"   ❄️ Experiment: {snow_sample_config['experiment_name']}\")\n",
    "print(f\"   🏔️ Scale: {snow_sample_config['analysis_scale']}\")\n",
    "print(f\"   📈 Strategy: {snow_sample_config['site_selection_strategy']}\")\n",
    "print(f\"   🎯 Max stations: {snow_sample_config['max_stations']}\")\n",
    "print(f\"   📅 Period: {snow_sample_config['start_year']}-{snow_sample_config['end_year']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE SNOW EXPERIMENT DIRECTORY STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📁 Creating Snow Experiment Directory Structure...\")\n",
    "\n",
    "# Create main experiment directory\n",
    "experiment_dir = Path(f\"./experiments/{snow_sample_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for snow analysis organization\n",
    "subdirs = {\n",
    "    'configs': 'Generated CONFLUENCE configuration files for snow sites',\n",
    "    'logs': 'Snow modeling execution logs and monitoring',\n",
    "    'results': 'Aggregated snow validation results and analysis outputs',\n",
    "    'plots': 'Snow visualization outputs and performance maps',\n",
    "    'reports': 'Snow validation summary reports and statistics',\n",
    "    'snow_data': 'Processed NorSWE observation data extracts'\n",
    "}\n",
    "\n",
    "for subdir, description in subdirs.items():\n",
    "    (experiment_dir / subdir).mkdir(exist_ok=True)\n",
    "    print(f\"   📁 {subdir}/: {description}\")\n",
    "\n",
    "# Save experiment configuration\n",
    "config_file = experiment_dir / 'snow_experiment_config.yaml'\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml.dump(snow_sample_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"✅ Snow experiment directory structure created: {experiment_dir}\")\n",
    "print(f\"   📋 Configuration saved: {config_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND EXPLORE NORSWE DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🌨️ Loading and Exploring NorSWE Dataset...\")\n",
    "\n",
    "# Check if NorSWE file exists\n",
    "norswe_file = Path(snow_sample_config['norswe_path'])\n",
    "if not norswe_file.exists():\n",
    "    print(f\"❌ NorSWE file not found: {norswe_file}\")\n",
    "    print(f\"   Please ensure the NorSWE dataset is available at the specified path\")\n",
    "    raise FileNotFoundError(f\"NorSWE dataset not found: {norswe_file}\")\n",
    "\n",
    "try:\n",
    "    # Open NorSWE dataset for exploration\n",
    "    print(f\"   📊 Opening NorSWE dataset: {norswe_file.name}\")\n",
    "    ds = xr.open_dataset(norswe_file)\n",
    "    \n",
    "    print(f\"✅ NorSWE dataset loaded successfully\")\n",
    "    print(f\"   📊 Dataset structure:\")\n",
    "    print(f\"      Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "    print(f\"      Number of stations: {len(ds.station_id)}\")\n",
    "    print(f\"      Spatial extent: {ds.lat.min().values:.1f}°N to {ds.lat.max().values:.1f}°N\")\n",
    "    print(f\"      Elevation range: {ds.elevation.min().values:.0f}m to {ds.elevation.max().values:.0f}m\")\n",
    "    \n",
    "    # Display key variables\n",
    "    print(f\"   📋 Key snow variables:\")\n",
    "    snow_vars = {\n",
    "        'snw': 'Snow Water Equivalent (SWE) [kg/m²]',\n",
    "        'snd': 'Snow Depth [m]'\n",
    "    }\n",
    "    \n",
    "    for var, description in snow_vars.items():\n",
    "        if var in ds.data_vars:\n",
    "            print(f\"      ❄️ {var}: {description}\")\n",
    "            \n",
    "            # Basic statistics\n",
    "            data_vals = ds[var].values\n",
    "            valid_data = data_vals[~np.isnan(data_vals)]\n",
    "            \n",
    "            if len(valid_data) > 0:\n",
    "                print(f\"         Range: {valid_data.min():.2f} to {valid_data.max():.2f}\")\n",
    "                print(f\"         Valid observations: {len(valid_data):,}\")\n",
    "    \n",
    "    # Check coordinate information\n",
    "    print(f\"   🗺️ Coordinate information:\")\n",
    "    print(f\"      Coordinate variables: {list(ds.coords.keys())}\")\n",
    "    \n",
    "    # Spatial distribution overview\n",
    "    print(f\"   🌍 Spatial distribution:\")\n",
    "    print(f\"      Latitude range: {ds.lat.min().values:.1f}° to {ds.lat.max().values:.1f}°N\")\n",
    "    print(f\"      Longitude range: {ds.lon.min().values:.1f}° to {ds.lon.max().values:.1f}°E\")\n",
    "    print(f\"      Elevation range: {ds.elevation.min().values:.0f} to {ds.elevation.max().values:.0f} m\")\n",
    "    \n",
    "    # Close dataset to free memory\n",
    "    ds.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading NorSWE dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS NORSWE STATION DATA FOR SITE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🔄 Processing NorSWE Station Data for Site Selection...\")\n",
    "\n",
    "# Process or load existing station data\n",
    "stations_csv = 'norswe_stations.csv'\n",
    "\n",
    "if Path(stations_csv).exists():\n",
    "    print(f\"   📊 Loading existing processed station data from {stations_csv}\")\n",
    "    stations_df = pd.read_csv(stations_csv)\n",
    "    print(f\"   ✅ Loaded {len(stations_df)} stations from existing file\")\n",
    "else:\n",
    "    print(f\"   ⚠️ Processed station file not found: {stations_csv}\")\n",
    "    print(f\"   📝 Instructions: Run the processing script first to generate station metadata\")\n",
    "    print(f\"      python run_norswe-2.py --norswe_path {snow_sample_config['norswe_path']} --output_dir ./temp --force_reprocess\")\n",
    "    \n",
    "    # Create a minimal demonstration dataset\n",
    "    print(f\"   🔧 Creating demonstration dataset for tutorial purposes...\")\n",
    "    \n",
    "    # Open dataset again to extract basic station info\n",
    "    ds = xr.open_dataset(norswe_file)\n",
    "    \n",
    "    # Create basic station dataframe\n",
    "    stations_df = pd.DataFrame({\n",
    "        'station_id': ds.station_id.values,\n",
    "        'station_name': [f\"Station_{i:04d}\" for i in range(len(ds.station_id))],\n",
    "        'lat': ds.lat.values,\n",
    "        'lon': ds.lon.values,\n",
    "        'elevation': ds.elevation.values,\n",
    "        'source': ds.source.values if 'source' in ds.variables else ['NorSWE'] * len(ds.station_id),\n",
    "        'swq_completeness': np.random.uniform(20, 95, len(ds.station_id)),  # Demo values\n",
    "        'snd_completeness': np.random.uniform(15, 90, len(ds.station_id)),  # Demo values\n",
    "    })\n",
    "    \n",
    "    # Add required CONFLUENCE formatting\n",
    "    buffer = 0.1\n",
    "    stations_df['BOUNDING_BOX_COORDS'] = (\n",
    "        (stations_df['lat'] + buffer).astype(str) + '/' +\n",
    "        (stations_df['lon'] - buffer).astype(str) + '/' +\n",
    "        (stations_df['lat'] - buffer).astype(str) + '/' +\n",
    "        (stations_df['lon'] + buffer).astype(str)\n",
    "    )\n",
    "    \n",
    "    stations_df['POUR_POINT_COORDS'] = stations_df['lat'].astype(str) + '/' + stations_df['lon'].astype(str)\n",
    "    stations_df['Watershed_Name'] = stations_df['station_id'].str.replace('[^a-zA-Z0-9_]', '_', regex=True)\n",
    "    \n",
    "    ds.close()\n",
    "\n",
    "print(f\"\\n📊 NorSWE Station Database Structure:\")\n",
    "print(f\"   Total stations available: {len(stations_df)}\")\n",
    "print(f\"   Database columns ({len(stations_df.columns)}):\")\n",
    "for i, col in enumerate(stations_df.columns):\n",
    "    print(f\"     {i+1:2d}. {col}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOW-SPECIFIC ENVIRONMENTAL GRADIENT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🏔️ Snow-Specific Environmental Gradient Analysis...\")\n",
    "\n",
    "# Analyze environmental diversity for snow modeling\n",
    "snow_environmental_summary = {}\n",
    "\n",
    "# Elevation distribution analysis (critical for snow)\n",
    "elevation_stats = stations_df['elevation'].describe()\n",
    "snow_environmental_summary['elevation_range'] = (elevation_stats['min'], elevation_stats['max'])\n",
    "print(f\"   ⛰️ Elevation diversity: {elevation_stats['min']:.0f} to {elevation_stats['max']:.0f} m\")\n",
    "print(f\"      Mean elevation: {elevation_stats['mean']:.0f} m\")\n",
    "print(f\"      Elevation quartiles: Q1={elevation_stats['25%']:.0f}m, Q3={elevation_stats['75%']:.0f}m\")\n",
    "\n",
    "# Latitude distribution (snow climate indicator)\n",
    "latitude_stats = stations_df['lat'].describe()\n",
    "snow_environmental_summary['latitude_range'] = (latitude_stats['min'], latitude_stats['max'])\n",
    "print(f\"   🌍 Latitude range: {latitude_stats['min']:.1f}° to {latitude_stats['max']:.1f}°N\")\n",
    "print(f\"      Snow climate zones: Arctic ({latitude_stats['max']:.1f}°N) to temperate ({latitude_stats['min']:.1f}°N)\")\n",
    "\n",
    "# Data completeness analysis (critical for snow validation)\n",
    "if 'swq_completeness' in stations_df.columns and 'snd_completeness' in stations_df.columns:\n",
    "    swe_completeness_stats = stations_df['swq_completeness'].describe()\n",
    "    depth_completeness_stats = stations_df['snd_completeness'].describe()\n",
    "    \n",
    "    print(f\"   📊 SWE data completeness: {swe_completeness_stats['min']:.1f}% to {swe_completeness_stats['max']:.1f}%\")\n",
    "    print(f\"      Mean SWE completeness: {swe_completeness_stats['mean']:.1f}%\")\n",
    "    print(f\"   📏 Snow depth completeness: {depth_completeness_stats['min']:.1f}% to {depth_completeness_stats['max']:.1f}%\")\n",
    "    print(f\"      Mean depth completeness: {depth_completeness_stats['mean']:.1f}%\")\n",
    "\n",
    "# Source/type analysis if available\n",
    "if 'source' in stations_df.columns:\n",
    "    source_counts = stations_df['source'].value_counts()\n",
    "    snow_environmental_summary['data_sources'] = len(source_counts)\n",
    "    print(f\"   📡 Data source diversity: {len(source_counts)} different sources\")\n",
    "    print(f\"      Primary sources: {', '.join(source_counts.head(3).index.tolist())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGIC SNOW SITE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n🎯 Strategic Snow Site Selection for Large Sample Analysis...\")\n",
    "\n",
    "# Implement snow-specific site selection strategy\n",
    "selection_strategy = snow_sample_config['site_selection_strategy']\n",
    "max_stations = snow_sample_config['max_stations']\n",
    "min_swe_completeness = snow_sample_config['min_swe_completeness']\n",
    "min_depth_completeness = snow_sample_config['min_depth_completeness']\n",
    "\n",
    "print(f\"   Strategy: {selection_strategy}\")\n",
    "print(f\"   Target stations: {max_stations}\")\n",
    "print(f\"   SWE completeness threshold: {min_swe_completeness}%\")\n",
    "print(f\"   Depth completeness threshold: {min_depth_completeness}%\")\n",
    "\n",
    "# Apply data quality filters\n",
    "quality_filtered = stations_df[\n",
    "    (stations_df['swq_completeness'] >= min_swe_completeness) &\n",
    "    (stations_df['snd_completeness'] >= min_depth_completeness)\n",
    "].copy()\n",
    "\n",
    "print(f\"   🔍 Quality filtering results:\")\n",
    "print(f\"      Initial stations: {len(stations_df)}\")\n",
    "print(f\"      After quality filter: {len(quality_filtered)}\")\n",
    "print(f\"      Filtered out: {len(stations_df) - len(quality_filtered)} low-quality stations\")\n",
    "\n",
    "if len(quality_filtered) == 0:\n",
    "    print(f\"   ⚠️ No stations meet quality criteria. Relaxing thresholds...\")\n",
    "    # Relax thresholds for demonstration\n",
    "    min_swe_completeness = 10.0\n",
    "    min_depth_completeness = 10.0\n",
    "    quality_filtered = stations_df[\n",
    "        (stations_df['swq_completeness'] >= min_swe_completeness) &\n",
    "        (stations_df['snd_completeness'] >= min_depth_completeness)\n",
    "    ].copy()\n",
    "    print(f\"      Relaxed filter results: {len(quality_filtered)} stations\")\n",
    "\n",
    "if selection_strategy == 'elevation_climate_diversity':\n",
    "    # Strategy: Maximize elevation and climate diversity for snow modeling\n",
    "    selected_sites = []\n",
    "    \n",
    "    # Create elevation bands for snow environments\n",
    "    elevation_bands = [\n",
    "        (0, 500, 'Lowland'),\n",
    "        (500, 1000, 'Montane'),\n",
    "        (1000, 1500, 'Subalpine'),\n",
    "        (1500, 2000, 'Alpine'),\n",
    "        (2000, 10000, 'High Alpine')\n",
    "    ]\n",
    "    \n",
    "    # Create latitude bands for climate zones\n",
    "    latitude_bands = [\n",
    "        (45, 55, 'Temperate'),\n",
    "        (55, 65, 'Boreal'),\n",
    "        (65, 75, 'Arctic')\n",
    "    ]\n",
    "    \n",
    "    sites_per_zone = max(1, max_stations // (len(elevation_bands) * len(latitude_bands)))\n",
    "    \n",
    "    print(f\"   🏔️ Elevation-Climate sampling strategy:\")\n",
    "    print(f\"      Elevation bands: {len(elevation_bands)}\")\n",
    "    print(f\"      Climate bands: {len(latitude_bands)}\")\n",
    "    print(f\"      Target sites per zone: ~{sites_per_zone}\")\n",
    "    \n",
    "    for elev_min, elev_max, elev_label in elevation_bands:\n",
    "        for lat_min, lat_max, lat_label in latitude_bands:\n",
    "            \n",
    "            # Find stations in this elevation-climate zone\n",
    "            zone_stations = quality_filtered[\n",
    "                (quality_filtered['elevation'] >= elev_min) &\n",
    "                (quality_filtered['elevation'] < elev_max) &\n",
    "                (quality_filtered['lat'] >= lat_min) &\n",
    "                (quality_filtered['lat'] < lat_max)\n",
    "            ]\n",
    "            \n",
    "            if len(zone_stations) > 0:\n",
    "                # Prioritize by data completeness\n",
    "                zone_stations = zone_stations.sort_values(\n",
    "                    by=['swq_completeness', 'snd_completeness'], \n",
    "                    ascending=False\n",
    "                )\n",
    "                \n",
    "                # Sample up to sites_per_zone from this climate-elevation zone\n",
    "                n_sample = min(sites_per_zone, len(zone_stations))\n",
    "                sampled = zone_stations.head(n_sample)\n",
    "                selected_sites.extend(sampled.index.tolist())\n",
    "                \n",
    "                print(f\"      {elev_label} × {lat_label}: {n_sample}/{len(zone_stations)} stations selected\")\n",
    "                \n",
    "                if len(selected_sites) >= max_stations:\n",
    "                    break\n",
    "        \n",
    "        if len(selected_sites) >= max_stations:\n",
    "            break\n",
    "    \n",
    "    # Trim to exact number if over\n",
    "    if len(selected_sites) > max_stations:\n",
    "        selected_sites = selected_sites[:max_stations]\n",
    "    \n",
    "    selected_df = quality_filtered.loc[selected_sites].copy()\n",
    "\n",
    "elif selection_strategy == 'random_sampling':\n",
    "    # Strategy 2: Random sampling from quality-filtered stations\n",
    "    selected_df = quality_filtered.sample(n=min(max_stations, len(quality_filtered)), random_state=42)\n",
    "\n",
    "else:\n",
    "    # Default: use highest quality sites\n",
    "    selected_df = quality_filtered.sort_values(\n",
    "        by=['swq_completeness', 'snd_completeness'], \n",
    "        ascending=False\n",
    "    ).head(max_stations).copy()\n",
    "\n",
    "print(f\"✅ Snow site selection complete: {len(selected_df)} stations selected\")\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATE SNOW MODELING TEMPLATE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📋 Validating Snow Modeling Template Configuration...\")\n",
    "\n",
    "template_path = Path(snow_sample_config['template_config'])\n",
    "\n",
    "if template_path.exists():\n",
    "    print(f\"✅ Snow template configuration found: {template_path}\")\n",
    "    \n",
    "    # Load and verify template structure\n",
    "    try:\n",
    "        with open(template_path, 'r') as f:\n",
    "            template_config = yaml.safe_load(f)\n",
    "        \n",
    "        # Check key template parameters for snow modeling\n",
    "        required_keys = ['DOMAIN_NAME', 'POUR_POINT_COORDS', 'BOUNDING_BOX_COORDS', \n",
    "                        'HYDROLOGICAL_MODEL', 'EXPERIMENT_TIME_START', 'EXPERIMENT_TIME_END']\n",
    "        \n",
    "        missing_keys = [key for key in required_keys if key not in template_config]\n",
    "        \n",
    "        if not missing_keys:\n",
    "            print(f\"✅ Snow template validation successful\")\n",
    "            print(f\"   📝 Template contains all required parameters for snow modeling\")\n",
    "            \n",
    "            # Check snow-specific settings if available\n",
    "            if 'HYDROLOGICAL_MODEL' in template_config:\n",
    "                model = template_config['HYDROLOGICAL_MODEL']\n",
    "                print(f\"   ❄️ Hydrological model: {model}\")\n",
    "                \n",
    "                if model.upper() == 'SUMMA':\n",
    "                    print(f\"      ✅ SUMMA selected - excellent for snow physics modeling\")\n",
    "                    print(f\"      🌨️ SUMMA capabilities: Multi-layer snowpack, energy balance, snow-vegetation interactions\")\n",
    "        else:\n",
    "            print(f\"⚠️  Snow template missing required keys: {missing_keys}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Snow template validation failed: {e}\")\n",
    "else:\n",
    "    print(f\"❌ Snow template configuration not found: {template_path}\")\n",
    "    print(f\"   Please ensure the snow modeling template file exists\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE SNOW SITE SELECTION VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n📈 Creating Comprehensive Snow Site Selection Visualization...\")\n",
    "\n",
    "# Create comprehensive snow site selection visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Map 1: Global distribution with elevation coloring (top left)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(stations_df['lon'], stations_df['lat'], \n",
    "           c='lightgray', alpha=0.3, s=15, label='Available stations')\n",
    "scatter1 = ax1.scatter(selected_df['lon'], selected_df['lat'], \n",
    "                      c=selected_df['elevation'], cmap='terrain', s=60, \n",
    "                      edgecolors='black', linewidth=0.5, label='Selected stations')\n",
    "\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'Snow Site Selection: Elevation Distribution\\\\n{len(selected_df)} of {len(stations_df)} stations')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add colorbar for elevation\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('Elevation (m)')\n",
    "\n",
    "# Map 2: Elevation distribution (top middle)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(stations_df['elevation'], bins=20, alpha=0.5, color='lightblue', \n",
    "         label='Available', edgecolor='blue')\n",
    "ax2.hist(selected_df['elevation'], bins=15, alpha=0.7, color='red', \n",
    "         label='Selected', edgecolor='darkred')\n",
    "ax2.set_xlabel('Elevation (m)')\n",
    "ax2.set_ylabel('Number of Stations')\n",
    "ax2.set_title('Elevation Distribution\\\\n(Snow Climate Zones)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add elevation zone labels\n",
    "elevation_zones = [(0, 500, 'Lowland'), (500, 1000, 'Montane'), \n",
    "                  (1000, 1500, 'Subalpine'), (1500, 2000, 'Alpine'), (2000, 3000, 'High Alpine')]\n",
    "for i, (min_elev, max_elev, label) in enumerate(elevation_zones):\n",
    "    if i % 2 == 0:  # Every other label to avoid crowding\n",
    "        ax2.axvspan(min_elev, max_elev, alpha=0.1, color='gray')\n",
    "        ax2.text((min_elev + max_elev)/2, ax2.get_ylim()[1]*0.9, label, \n",
    "                ha='center', fontsize=8, rotation=45)\n",
    "\n",
    "# Map 3: Latitude vs elevation scatter (top right)\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(selected_df['lat'], selected_df['elevation'], \n",
    "           c=selected_df['swq_completeness'], cmap='viridis', s=80,\n",
    "           edgecolors='black', linewidth=0.5)\n",
    "ax3.set_xlabel('Latitude (°N)')\n",
    "ax3.set_ylabel('Elevation (m)')\n",
    "ax3.set_title('Selected Sites: Climate-Elevation Relationship')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar for SWE completeness\n",
    "scatter3 = ax3.collections[0]\n",
    "cbar3 = plt.colorbar(scatter3, ax=ax3)\n",
    "cbar3.set_label('SWE Data Completeness (%)')\n",
    "\n",
    "# Map 4: Data completeness comparison (bottom left)\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(selected_df['swq_completeness'], selected_df['snd_completeness'], \n",
    "           s=60, alpha=0.7, c='purple', edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel('SWE Data Completeness (%)')\n",
    "ax4.set_ylabel('Snow Depth Data Completeness (%)')\n",
    "ax4.set_title('Data Quality Assessment\\\\n(Selected Snow Sites)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add quality threshold lines\n",
    "ax4.axvline(x=min_swe_completeness, color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'SWE threshold ({min_swe_completeness}%)')\n",
    "ax4.axhline(y=min_depth_completeness, color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'Depth threshold ({min_depth_completeness}%)')\n",
    "ax4.legend()\n",
    "\n",
    "# Map 5: Longitude distribution (bottom middle)\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(selected_df['lon'], bins=15, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax5.set_xlabel('Longitude (°E)')\n",
    "ax5.set_ylabel('Number of Stations')\n",
    "ax5.set_title('Longitudinal Distribution\\\\n(Continental Snow Regimes)')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add regional labels\n",
    "if selected_df['lon'].min() < 0:\n",
    "    ax5.text(-100, ax5.get_ylim()[1]*0.8, 'North America', ha='center', fontsize=9, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.5))\n",
    "if selected_df['lon'].max() > 0:\n",
    "    ax5.text(50, ax5.get_ylim()[1]*0.8, 'Eurasia', ha='center', fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.5))\n",
    "\n",
    "# Map 6: Selection summary statistics (bottom right)\n",
    "ax6 = axes[1, 2]\n",
    "\n",
    "# Create summary statistics\n",
    "selection_stats = [\n",
    "    ('Total Available', len(stations_df)),\n",
    "    ('Quality Filtered', len(quality_filtered)),\n",
    "    ('Finally Selected', len(selected_df)),\n",
    "    ('High Quality\\\\n(>80% complete)', len(selected_df[selected_df['swq_completeness'] > 80])),\n",
    "    ('Mountain Sites\\\\n(>1000m)', len(selected_df[selected_df['elevation'] > 1000]))\n",
    "]\n",
    "\n",
    "categories = [stat[0] for stat in selection_stats]\n",
    "counts = [stat[1] for stat in selection_stats]\n",
    "colors = ['lightblue', 'yellow', 'green', 'orange', 'purple']\n",
    "\n",
    "bars = ax6.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax6.set_ylabel('Number of Stations')\n",
    "ax6.set_title('Snow Site Selection Summary')\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'NorSWE Large Sample Snow Study - Site Selection Analysis\\\\n{snow_sample_config[\"experiment_name\"]}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save visualization\n",
    "selection_plot_path = experiment_dir / 'plots' / 'snow_site_selection_overview.png'\n",
    "plt.savefig(selection_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Snow site selection visualization saved: {selection_plot_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE SELECTED SNOW SITES FOR PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n💾 Saving Selected Snow Sites for Large Sample Processing...\")\n",
    "\n",
    "# Save selected sites to CSV\n",
    "selected_sites_csv = experiment_dir / 'selected_snow_sites.csv'\n",
    "selected_df.to_csv(selected_sites_csv, index=False)\n",
    "\n",
    "print(f\"✅ Selected snow sites saved: {selected_sites_csv}\")\n",
    "print(f\"   ❄️ Sites ready for processing: {len(selected_df)}\")\n",
    "\n",
    "# Create comprehensive summary report\n",
    "summary_report = experiment_dir / 'reports' / 'snow_site_selection_summary.txt'\n",
    "\n",
    "with open(summary_report, 'w') as f:\n",
    "    f.write(\"NorSWE Large Sample Snow Study - Site Selection Summary\\n\")\n",
    "    f.write(\"=\" * 55 + \"\\n\\n\")\n",
    "    f.write(f\"Experiment: {snow_sample_config['experiment_name']}\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Selection Strategy: {snow_sample_config['site_selection_strategy']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"SITE SELECTION RESULTS:\\n\")\n",
    "    f.write(f\"  Available stations: {len(stations_df)}\\n\")\n",
    "    f.write(f\"  Quality filtered: {len(quality_filtered)}\\n\")\n",
    "    f.write(f\"  Selected stations: {len(selected_df)}\\n\")\n",
    "    f.write(f\"  Selection ratio: {len(selected_df)/len(stations_df)*100:.1f}%\\n\\n\")\n",
    "    \n",
    "    f.write(f\"ENVIRONMENTAL GRADIENTS:\\n\")\n",
    "    f.write(f\"  Elevation range: {selected_df['elevation'].min():.0f} to {selected_df['elevation'].max():.0f} m\\n\")\n",
    "    f.write(f\"  Latitude range: {selected_df['lat'].min():.1f}° to {selected_df['lat'].max():.1f}°N\\n\")\n",
    "    f.write(f\"  Longitude range: {selected_df['lon'].min():.1f}° to {selected_df['lon'].max():.1f}°E\\n\\n\")\n",
    "    \n",
    "    f.write(f\"DATA QUALITY ASSESSMENT:\\n\")\n",
    "    f.write(f\"  SWE completeness: {selected_df['swq_completeness'].mean():.1f}% ± {selected_df['swq_completeness'].std():.1f}%\\n\")\n",
    "    f.write(f\"  Depth completeness: {selected_df['snd_completeness'].mean():.1f}% ± {selected_df['snd_completeness'].std():.1f}%\\n\")\n",
    "    f.write(f\"  High quality sites (>80%): {len(selected_df[selected_df['swq_completeness'] > 80])}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"SNOW ENVIRONMENT CHARACTERISTICS:\\n\")\n",
    "    f.write(f\"  Mountain sites (>1000m): {len(selected_df[selected_df['elevation'] > 1000])}\\n\")\n",
    "    f.write(f\"  High elevation sites (>1500m): {len(selected_df[selected_df['elevation'] > 1500])}\\n\")\n",
    "    f.write(f\"  Arctic sites (>65°N): {len(selected_df[selected_df['lat'] > 65])}\\n\")\n",
    "    f.write(f\"  Boreal sites (55-65°N): {len(selected_df[(selected_df['lat'] >= 55) & (selected_df['lat'] <= 65)])}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"ANALYSIS PERIOD:\\n\")\n",
    "    f.write(f\"  Start year: {snow_sample_config['start_year']}\\n\")\n",
    "    f.write(f\"  End year: {snow_sample_config['end_year']}\\n\")\n",
    "    f.write(f\"  Focus months: {snow_sample_config['focus_months']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Note: This site selection prioritizes elevation and climate diversity for comprehensive snow process validation.\\n\")\n",
    "\n",
    "print(f\"✅ Summary report saved: {summary_report}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STORE RESULTS FOR SUBSEQUENT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "# Store key variables for use in subsequent notebook cells\n",
    "complete_stations = selected_df.copy()\n",
    "experiment_config = snow_sample_config.copy()\n",
    "\n",
    "print(f\"\\n🎯 Large Sample Snow Study Configuration Summary:\")\n",
    "\n",
    "configuration_summary = [\n",
    "    f\"Experimental design: {snow_sample_config['experiment_type']}\",\n",
    "    f\"Analysis scale: {snow_sample_config['analysis_scale']}\",\n",
    "    f\"Site selection: {len(selected_df)} stations from {len(stations_df)} available\",\n",
    "    f\"Elevation diversity: {selected_df['elevation'].min():.0f}m to {selected_df['elevation'].max():.0f}m\",\n",
    "    f\"Climate coverage: {selected_df['lat'].min():.1f}°N to {selected_df['lat'].max():.1f}°N latitude\",\n",
    "    f\"Template configuration: Validated and ready for snow modeling\"\n",
    "]\n",
    "\n",
    "for summary in configuration_summary:\n",
    "    print(f\"   ✅ {summary}\")\n",
    "\n",
    "print(f\"\\n❄️ Snow Science Objectives:\")\n",
    "snow_objectives = [\n",
    "    f\"Multi-variable validation: SWE and snow depth across elevation gradients\",\n",
    "    f\"Seasonal dynamics: Accumulation vs. ablation process performance\",\n",
    "    f\"Climate sensitivity: Snow modeling across temperature and precipitation regimes\",\n",
    "    f\"Elevation effects: Systematic assessment of topographic snow process controls\",\n",
    "    f\"Process generalization: Universal vs. environment-specific snow physics patterns\"\n",
    "]\n",
    "\n",
    "for objective in snow_objectives:\n",
    "    print(f\"   🎓 {objective}\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Large Sample Snow Processing:\")\n",
    "next_steps = [\n",
    "    f\"Snow template configuration: Validated for multi-site snow modeling deployment\",\n",
    "    f\"Site selection: {len(selected_df)} diverse snow environments prepared for analysis\",\n",
    "    f\"Batch processing: Ready for systematic CONFLUENCE snow simulation execution\",\n",
    "    f\"Output analysis: Framework prepared for multi-site snow validation result aggregation\",\n",
    "    f\"Statistical analysis: Tools ready for comparative snow hydrology insights\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ✅ {step}\")\n",
    "\n",
    "print(f\"\\n✅ Step 1 Complete: Large sample snow experiment designed and configured\")\n",
    "print(f\"   ❄️ Next: Execute systematic multi-site CONFLUENCE snow processing\")\n",
    "print(f\"   📊 Goal: Comparative snow hydrology across northern hemisphere environmental gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Large Sample Snow Modeling Execution\n",
    "Building on the experimental design and NorSWE station selection from previous steps, we now execute large sample snow modeling across diverse northern hemisphere environments. This step demonstrates CONFLUENCE's capability to systematically process snow hydrology validation using the comprehensive NorSWE observation network, scaling from individual snow physics to continental-scale comparative snow science.\n",
    "\n",
    "### Snow Modeling Scaling: Single Sites → Northern Hemisphere Analysis\n",
    "\n",
    "**Traditional Snow Modeling**: Individual site case studies with limited transferability\n",
    "- Site-specific snow process calibration with unclear generalizability\n",
    "- Manual configuration for each elevation zone or climate region\n",
    "- Limited ability to identify universal vs. local snow process controls\n",
    "- Difficulty distinguishing model limitations from site-specific effects\n",
    "\n",
    "**Large Sample Snow Modeling**: Systematic validation across environmental gradients\n",
    "- **Automated snow-specific configuration** across elevation, latitude, and climate gradients\n",
    "- **Parallel snow simulations** leveraging computational efficiency for winter process modeling\n",
    "- **Standardized snow validation protocols** enabling direct performance comparison\n",
    "- **Multi-variable snow assessment** integrating SWE, snow depth, and seasonal dynamics\n",
    "\n",
    "### The Unique Challenges of Snow Process Validation\n",
    "\n",
    "Snow hydrology presents distinct modeling challenges that require specialized large sample approaches:\n",
    "\n",
    "**Physical Process Complexity**: \n",
    "- **Phase transitions**: Accurate representation of freezing, melting, and sublimation processes\n",
    "- **Energy balance**: Complex interactions between radiation, temperature, wind, and humidity\n",
    "- **Layered snowpack structure**: Multi-layer snow metamorphism and thermal properties\n",
    "- **Spatial variability**: Strong elevation, aspect, and microclimate dependencies\n",
    "\n",
    "**Temporal Dynamics**:\n",
    "- **Seasonal accumulation**: Episodic snow accumulation events throughout winter\n",
    "- **Ablation processes**: Complex melt dynamics driven by energy balance components  \n",
    "- **Diurnal cycles**: Strong daily temperature and radiation variations\n",
    "- **Extreme events**: Rain-on-snow events and rapid melt episodes\n",
    "\n",
    "**Validation Complexity**:\n",
    "- **State variable validation**: SWE and snow depth as integrated measures of snow processes\n",
    "- **Seasonal metrics**: Peak SWE timing, snow season length, melt rate assessment\n",
    "- **Process-based evaluation**: Distinguishing accumulation vs. ablation performance\n",
    "- **Multi-scale representation**: Point measurements vs. model grid cell assumptions\n",
    "\n",
    "### CONFLUENCE's Advanced Snow Physics Integration\n",
    "\n",
    "The large sample framework leverages CONFLUENCE's sophisticated snow modeling capabilities:\n",
    "\n",
    "**SUMMA Snow Physics**:\n",
    "- **Multi-layer snowpack**: Explicit representation of snow stratigraphy and metamorphism\n",
    "- **Advanced energy balance**: Detailed surface energy balance with radiation, turbulent, and ground heat fluxes\n",
    "- **Liquid water dynamics**: Representation of liquid water percolation and refreezing\n",
    "- **Snow-vegetation interactions**: Canopy interception and sub-canopy snow processes\n",
    "\n",
    "**Flexible Process Representations**:\n",
    "- **Multiple parameterization options** for key snow processes enable systematic testing\n",
    "- **Decision analysis capabilities** for comparing alternative model structures\n",
    "- **Parameter sensitivity assessment** across diverse snow environments\n",
    "- **Uncertainty quantification** for snow predictions under different conditions\n",
    "\n",
    "### Northern Hemisphere Snow Science Applications\n",
    "\n",
    "Large sample snow modeling with NorSWE enables investigation of fundamental snow science questions:\n",
    "\n",
    "🌨️ **Climate Controls**: Systematic assessment of temperature and precipitation thresholds controlling snow accumulation and persistence\n",
    "\n",
    "⛰️ **Elevation Dependencies**: Quantification of how snow processes change with elevation and their representation in models\n",
    "\n",
    "🌍 **Regional Patterns**: Identification of systematic regional biases in snow modeling across different snow climate regimes\n",
    "\n",
    "📅 **Seasonal Dynamics**: Evaluation of model skill in capturing both accumulation season and ablation season processes\n",
    "\n",
    "❄️ **Process Universality**: Testing whether snow process representations are consistent across diverse northern hemisphere environments\n",
    "\n",
    "The automated workflow demonstrated here enables systematic snow model evaluation that was previously impossible due to the manual effort required for multi-site snow modeling studies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: EXECUTE LARGE SAMPLE NORSWE SNOW PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Step 2: NorSWE Large Sample Snow Processing Execution ===\")\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_norswe_script_from_notebook():\n",
    "    \"\"\"\n",
    "    Execute the run_norswe-2.py script from within the notebook\n",
    "    \"\"\"\n",
    "    print(f\"\\n❄️ Executing NorSWE Large Sample Snow Processing Script...\")\n",
    "    \n",
    "    script_path = \"./run_norswe-2.py\"\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"❌ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"   📝 Script location: {script_path}\")\n",
    "    print(f\"   🎯 Target sites: {len(complete_stations)} NorSWE stations\")\n",
    "    print(f\"   ⏰ Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare script arguments based on experiment configuration\n",
    "        script_args = [\n",
    "            'python', script_path,\n",
    "            '--norswe_path', experiment_config['norswe_path'],\n",
    "            '--template_config', experiment_config['template_config'],\n",
    "            '--output_dir', experiment_config['output_dir'],\n",
    "            '--config_dir', experiment_config['config_dir'],\n",
    "            '--min_completeness', str(experiment_config['min_completeness']),\n",
    "            '--max_stations', str(experiment_config['max_stations']),\n",
    "            '--base_path', experiment_config['base_path']\n",
    "        ]\n",
    "        \n",
    "        # Add optional year filtering\n",
    "        if experiment_config.get('start_year'):\n",
    "            script_args.extend(['--start_year', str(experiment_config['start_year'])])\n",
    "        if experiment_config.get('end_year'):\n",
    "            script_args.extend(['--end_year', str(experiment_config['end_year'])])\n",
    "        \n",
    "        # Add no_submit flag if specified\n",
    "        if experiment_config.get('no_submit', False):\n",
    "            script_args.append('--no_submit')\n",
    "        \n",
    "        print(f\"   🔧 Script arguments: {' '.join(script_args[2:])}\")\n",
    "        \n",
    "        # Create a process with input automation\n",
    "        process = subprocess.Popen(\n",
    "            script_args,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Send 'y' to confirm job submission when prompted (unless no_submit)\n",
    "        if not experiment_config.get('no_submit', False):\n",
    "            stdout, stderr = process.communicate(input='y\\n')\n",
    "        else:\n",
    "            stdout, stderr = process.communicate()\n",
    "        \n",
    "        # Print the output\n",
    "        if stdout:\n",
    "            print(\"📋 Script Output:\")\n",
    "            for line in stdout.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"⚠️  Script Warnings/Errors:\")\n",
    "            for line in stderr.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"✅ NorSWE processing script completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Script failed with return code: {process.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running script: {e}\")\n",
    "        return False\n",
    "\n",
    "def monitor_snow_job_progress():\n",
    "    \"\"\"\n",
    "    Monitor the progress of submitted CONFLUENCE snow modeling jobs\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 Monitoring Snow Modeling Job Progress...\")\n",
    "    \n",
    "    try:\n",
    "        # Check job queue status\n",
    "        result = subprocess.run(['squeue', '-u', '$USER'], \n",
    "                              capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            queue_lines = result.stdout.strip().split('\\n')\n",
    "            confluence_jobs = [line for line in queue_lines \n",
    "                             if 'CONFLUENCE' in line or any(site in line \n",
    "                             for site in complete_stations['Watershed_Name'][:5])]\n",
    "            \n",
    "            print(f\"   ❄️ Snow modeling jobs in queue: {len(confluence_jobs)}\")\n",
    "            \n",
    "            if confluence_jobs:\n",
    "                print(\"   📋 Active NorSWE CONFLUENCE jobs:\")\n",
    "                for job in confluence_jobs[:10]:  # Show first 10\n",
    "                    print(f\"     {job}\")\n",
    "                if len(confluence_jobs) > 10:\n",
    "                    print(f\"     ... and {len(confluence_jobs) - 10} more\")\n",
    "        else:\n",
    "            print(\"   ⚠️  Unable to check job queue status\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Error checking job status: {e}\")\n",
    "\n",
    "# Execute the NorSWE processing script\n",
    "script_success = run_norswe_script_from_notebook()\n",
    "\n",
    "if script_success:\n",
    "    print(f\"\\n✅ Step 2 Complete: NorSWE snow modeling initiated\")\n",
    "    \n",
    "    # Monitor initial job status\n",
    "    monitor_snow_job_progress()\n",
    "    \n",
    "    print(f\"\\n📝 Next Steps:\")\n",
    "    print(f\"   1. Snow modeling jobs will process in parallel on the cluster\")\n",
    "    print(f\"   2. Results will include SWE and snow depth simulations\")\n",
    "    print(f\"   3. Step 3 will analyze snow validation metrics\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n⚠️  Step 2 Issue: Script execution had problems\")\n",
    "    print(f\"   Proceeding to Step 3 with any existing results...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Site Snow Validation and Process Analysis\n",
    "Having executed large sample snow modeling, we now demonstrate the analytical power that emerges from systematic multi-site snow validation using NorSWE observations. This step showcases comprehensive snow process evaluation, seasonal dynamics analysis, and elevation-climate performance assessment—the scientific breakthrough enabled by large sample snow hydrology methodology.\n",
    "\n",
    "### Snow Science Evolution: Case Studies → Systematic Snow Process Understanding\n",
    "\n",
    "**Traditional Snow Validation**: Individual site snow model evaluation\n",
    "- Site-specific snow process calibration with limited transferability\n",
    "- Difficulty separating universal snow physics from local environmental effects\n",
    "- Manual comparison across different snow climates and elevation zones\n",
    "- Limited statistical power for robust snow process generalization\n",
    "\n",
    "**Large Sample Snow Validation**: Systematic multi-site snow process analysis\n",
    "- **Continental-scale pattern recognition** across elevation and climate gradients\n",
    "- **Statistical hypothesis testing** for snow process representations with robust sample sizes\n",
    "- **Process universality assessment** distinguishing general vs. site-specific snow dynamics\n",
    "- **Model transferability evaluation** across diverse northern hemisphere snow environments\n",
    "\n",
    "### Comprehensive Snow Analysis Framework\n",
    "\n",
    "**Tier 1: Snow Domain Spatial Overview**\n",
    "- **Automated discovery** of completed snow modeling domains across elevation gradients\n",
    "- **Processing status assessment** including simulation completion and observation availability\n",
    "- **Northern Hemisphere spatial distribution** showing snow modeling coverage across climate zones\n",
    "- **Elevation-based analysis** revealing snow modeling performance across topographic gradients\n",
    "\n",
    "**Tier 2: Multi-Variable Snow Process Validation**\n",
    "- **SWE validation**: Snow Water Equivalent comparison providing mass balance assessment\n",
    "- **Snow depth validation**: Complementary structural validation of snowpack representation\n",
    "- **Seasonal dynamics evaluation**: Peak SWE timing, snow season length, and ablation rate analysis\n",
    "- **Performance metric calculation** across correlation, RMSE, bias, and Nash-Sutcliffe efficiency\n",
    "\n",
    "### Snow Hydrology Innovation at Scale\n",
    "\n",
    "Multi-variable snow validation across hundreds of NorSWE sites represents cutting-edge snow science:\n",
    "\n",
    "**Snow Process Understanding**:\n",
    "- **Accumulation vs. ablation performance** revealing process-specific model skill patterns\n",
    "- **Temperature threshold evaluation** across different climate regimes and elevation zones\n",
    "- **Energy balance validation** through integrated SWE and snow depth comparison\n",
    "- **Seasonal cycle assessment** identifying universal vs. climate-specific snow dynamics\n",
    "\n",
    "**Elevation-Climate Interactions**:\n",
    "- **Elevation gradient analysis** revealing systematic changes in snow process representation\n",
    "- **Climate zone performance** across maritime, continental, and Arctic snow regimes\n",
    "- **Latitude-elevation interactions** showing complex environmental controls on snow modeling\n",
    "- **Topographic effect quantification** on snow accumulation and ablation processes\n",
    "\n",
    "**Model Process Evaluation**:\n",
    "- **Snow physics assessment** across different process representations in SUMMA\n",
    "- **Parameter transferability** testing consistency of snow model parameters across sites\n",
    "- **Structural uncertainty** evaluation through multi-site performance patterns\n",
    "- **Process-based diagnostics** identifying where snow models succeed vs. struggle\n",
    "\n",
    "### Breakthrough Snow Science Capabilities\n",
    "\n",
    "This multi-site analysis framework delivers several revolutionary capabilities for snow hydrology:\n",
    "\n",
    "❄️ **Continental Snow Assessment**: Comprehensive evaluation of snow model performance across the full range of northern hemisphere snow environments\n",
    "\n",
    "📈 **Snow Process Generalization**: Statistical identification of universal snow process patterns vs. climate/elevation-specific behaviors\n",
    "\n",
    "🎯 **Model Physics Validation**: Systematic testing of snow energy balance and phase change representations across environmental gradients\n",
    "\n",
    "🔍 **Uncertainty Quantification**: Robust assessment of snow prediction reliability across different snow climate regimes\n",
    "\n",
    "📊 **Transferability Analysis**: Evaluation of snow model parameter and process consistency across diverse environments\n",
    "\n",
    "⛰️ **Elevation-Climate Synthesis**: Understanding complex interactions between topographic and climatic controls on snow processes\n",
    "\n",
    "### Seasonal Snow Dynamics Focus\n",
    "\n",
    "The analysis emphasizes critical seasonal snow process evaluation:\n",
    "\n",
    "**Accumulation Season Analysis**:\n",
    "- Temperature-precipitation threshold performance across climate gradients\n",
    "- Storm-by-storm snow accumulation event representation\n",
    "- Wind redistribution and sublimation process evaluation\n",
    "- Snow-rain transition accuracy in different elevation zones\n",
    "\n",
    "**Ablation Season Analysis**:\n",
    "- Energy balance component performance during melt periods\n",
    "- Diurnal melt cycle representation across latitude gradients\n",
    "- Rain-on-snow event simulation across diverse climate conditions\n",
    "- Seasonal melt timing and rate accuracy evaluation\n",
    "\n",
    "**Annual Cycle Integration**:\n",
    "- Peak SWE magnitude and timing performance across elevations\n",
    "- Snow season length representation across climate gradients\n",
    "- Interannual variability capture across diverse snow regimes\n",
    "- Snow persistence modeling in marginal snow environments\n",
    "\n",
    "The multi-site snow validation demonstrated here represents the future of snow science: moving from individual mountain case studies to systematic, statistically robust analysis across the full spectrum of northern hemisphere snow environments. This approach enables confident identification of universal snow process patterns while quantifying regional variations and model uncertainties across diverse snow climate regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: COMPREHENSIVE SNOW OUTPUT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n=== Step 3: NorSWE Snow Validation Analysis ===\")\n",
    "\n",
    "def discover_completed_snow_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed NorSWE domain directories and their snow outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Discovering Completed NorSWE Snow Modeling Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    base_path = Path(experiment_config['base_path'])\n",
    "    domain_pattern = str(base_path / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(domain_pattern)\n",
    "    \n",
    "    print(f\"   📁 Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a NorSWE domain (should match our selected stations)\n",
    "        if any(domain_name in site for site in complete_stations['Watershed_Name'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"catchment\" / f\"{domain_name}_HRUs.shp\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            obs_dir = domain_path / \"observations\" / \"snow\" / \"raw_data\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'has_observations': obs_dir.exists(),\n",
    "                'observation_path': obs_dir if obs_dir.exists() else None,\n",
    "                'simulation_files': [],\n",
    "                'swe_obs_file': None,\n",
    "                'depth_obs_file': None\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                nc_files = list(simulation_dir.glob(\"**/*.nc\"))\n",
    "                domain_info['simulation_files'] = nc_files\n",
    "                domain_info['has_results'] = len(nc_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "            \n",
    "            # Find observation files\n",
    "            if obs_dir.exists():\n",
    "                swe_files = list((obs_dir / \"swe\").glob(\"*.csv\"))\n",
    "                depth_files = list((obs_dir / \"depth\").glob(\"*.csv\"))\n",
    "                \n",
    "                if swe_files:\n",
    "                    domain_info['swe_obs_file'] = swe_files[0]\n",
    "                if depth_files:\n",
    "                    domain_info['depth_obs_file'] = depth_files[0]\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   ❄️ NorSWE domains found: {len(completed_domains)}\")\n",
    "    print(f\"   📊 Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   📈 Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    print(f\"   📋 Domains with observations: {sum(1 for d in completed_domains if d['has_observations'])}\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_snow_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all snow domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\n🗺️  Creating Snow Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status (focus on Northern Hemisphere)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    ax1.scatter(complete_stations['lon'], complete_stations['lat'], \n",
    "               c='lightgray', alpha=0.5, s=30, label='Selected stations', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in complete_stations\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            lat = site_row['lat'].iloc[0]\n",
    "            lon = site_row['lon'].iloc[0]\n",
    "            \n",
    "            # Color based on completion status\n",
    "            if domain['has_results'] and domain['has_observations']:\n",
    "                color = 'green'\n",
    "                label = 'Complete with snow validation'\n",
    "                marker = 's'\n",
    "                size = 60\n",
    "            elif domain['has_results']:\n",
    "                color = 'orange' \n",
    "                label = 'Simulation complete'\n",
    "                marker = '^'\n",
    "                size = 50\n",
    "            elif domain['has_observations']:\n",
    "                color = 'blue'\n",
    "                label = 'Observations only'\n",
    "                marker = 'D'\n",
    "                size = 40\n",
    "            else:\n",
    "                color = 'red'\n",
    "                label = 'Processing started'\n",
    "                marker = 'v'\n",
    "                size = 30\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=color, s=size, marker=marker, alpha=0.8,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('NorSWE Snow Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(30, 85)  # Focus on Northern Hemisphere snow regions\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='green', s=60, marker='s', label='Complete with validation'),\n",
    "        plt.scatter([], [], c='orange', s=50, marker='^', label='Simulation complete'),\n",
    "        plt.scatter([], [], c='blue', s=40, marker='D', label='Observations extracted'),\n",
    "        plt.scatter([], [], c='red', s=30, marker='v', label='Processing started'),\n",
    "        plt.scatter([], [], c='lightgray', s=30, marker='o', label='Selected stations')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    # Map 2: Completion statistics by elevation bands\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Create elevation bands\n",
    "    elevation_bands = [(0, 500), (500, 1000), (1000, 1500), (1500, 2000), (2000, 10000)]\n",
    "    band_labels = ['0-500m', '500-1000m', '1000-1500m', '1500-2000m', '>2000m']\n",
    "    \n",
    "    elevation_completion = {}\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            elevation = site_row['elevation'].iloc[0]\n",
    "            \n",
    "            # Find elevation band\n",
    "            for i, (min_elev, max_elev) in enumerate(elevation_bands):\n",
    "                if min_elev <= elevation < max_elev:\n",
    "                    band_label = band_labels[i]\n",
    "                    \n",
    "                    if band_label not in elevation_completion:\n",
    "                        elevation_completion[band_label] = {'total': 0, 'complete': 0, 'partial': 0}\n",
    "                    \n",
    "                    elevation_completion[band_label]['total'] += 1\n",
    "                    \n",
    "                    if domain['has_results'] and domain['has_observations']:\n",
    "                        elevation_completion[band_label]['complete'] += 1\n",
    "                    elif domain['has_results'] or domain['has_observations']:\n",
    "                        elevation_completion[band_label]['partial'] += 1\n",
    "                    break\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    if elevation_completion:\n",
    "        bands = list(elevation_completion.keys())\n",
    "        complete_counts = [elevation_completion[b]['complete'] for b in bands]\n",
    "        partial_counts = [elevation_completion[b]['partial'] for b in bands]\n",
    "        pending_counts = [elevation_completion[b]['total'] - \n",
    "                         elevation_completion[b]['complete'] - \n",
    "                         elevation_completion[b]['partial'] for b in bands]\n",
    "        \n",
    "        x_pos = range(len(bands))\n",
    "        \n",
    "        ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.7)\n",
    "        ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.7)\n",
    "        ax2.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(bands, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Number of Sites')\n",
    "        ax2.set_title('Processing Status by Elevation Band')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Station elevation vs latitude\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    domain_elevations = []\n",
    "    domain_latitudes = []\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            elevation = site_row['elevation'].iloc[0]\n",
    "            latitude = site_row['lat'].iloc[0]\n",
    "            domain_elevations.append(elevation)\n",
    "            domain_latitudes.append(latitude)\n",
    "            \n",
    "            # Color code by completion status\n",
    "            if domain['has_results'] and domain['has_observations']:\n",
    "                color = 'green'\n",
    "            elif domain['has_results']:\n",
    "                color = 'orange'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            \n",
    "            ax3.scatter(latitude, elevation, c=color, alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax3.set_xlabel('Latitude')\n",
    "    ax3.set_ylabel('Elevation (m)')\n",
    "    ax3.set_title('Station Distribution: Elevation vs Latitude')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Map 4: Processing summary statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(complete_stations)\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    total_with_obs = sum(1 for d in completed_domains if d['has_observations'])\n",
    "    total_complete = sum(1 for d in completed_domains if d['has_results'] and d['has_observations'])\n",
    "    \n",
    "    categories = ['Selected', 'Processing\\nStarted', 'Simulation\\nComplete', 'Observations\\nExtracted', 'Ready for\\nValidation']\n",
    "    counts = [total_selected, total_discovered, total_with_results, total_with_obs, total_complete]\n",
    "    colors = ['lightblue', 'yellow', 'orange', 'cyan', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Snow Modeling Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Study - Domain Overview', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'snow_domain_overview_map.png'\n",
    "    overview_path.parent.mkdir(exist_ok=True)\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Snow domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_results, total_with_obs, total_complete\n",
    "\n",
    "def extract_snow_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract snow simulation results (SWE and snow depth) from all completed domains\n",
    "    \"\"\"\n",
    "    print(f\"\\n❄️ Extracting Snow Results from Completed Domains...\")\n",
    "    \n",
    "    snow_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_results': 0,\n",
    "        'domains_with_snow': 0,\n",
    "        'failed_extractions': 0\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_results']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        processing_summary['domains_with_results'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   🔄 Processing {domain_name}...\")\n",
    "            \n",
    "            # Find simulation output files\n",
    "            nc_files = domain['simulation_files']\n",
    "            \n",
    "            # Look for daily or monthly output files\n",
    "            daily_files = [f for f in nc_files if 'day' in f.name.lower()]\n",
    "            monthly_files = [f for f in nc_files if 'month' in f.name.lower()]\n",
    "            timestep_files = [f for f in nc_files if 'timestep' in f.name.lower()]\n",
    "            \n",
    "            output_file = None\n",
    "            if daily_files:\n",
    "                output_file = daily_files[0]\n",
    "            elif timestep_files:\n",
    "                output_file = timestep_files[0]\n",
    "            elif monthly_files:\n",
    "                output_file = monthly_files[0]\n",
    "            elif nc_files:\n",
    "                output_file = nc_files[0]  # Use any available file\n",
    "            \n",
    "            if output_file is None:\n",
    "                print(f\"     ❌ No suitable output files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for snow variables\n",
    "            snow_vars = {}\n",
    "            \n",
    "            # Common SUMMA snow variable names\n",
    "            if 'scalarSWE' in ds.data_vars:\n",
    "                snow_vars['swe'] = 'scalarSWE'\n",
    "            elif 'SWE' in ds.data_vars:\n",
    "                snow_vars['swe'] = 'SWE'\n",
    "            \n",
    "            if 'scalarSnowDepth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'scalarSnowDepth'\n",
    "            elif 'snowDepth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'snowDepth'\n",
    "            elif 'snow_depth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'snow_depth'\n",
    "            \n",
    "            if not snow_vars:\n",
    "                print(f\"     ⚠️  No snow variables found in {output_file.name}\")\n",
    "                available_vars = list(ds.data_vars.keys())\n",
    "                print(f\"     Available variables: {available_vars[:10]}...\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"     ❄️ Using snow variables: {snow_vars}\")\n",
    "            \n",
    "            # Extract snow data\n",
    "            extracted_data = {}\n",
    "            \n",
    "            for var_type, var_name in snow_vars.items():\n",
    "                snow_data = ds[var_name]\n",
    "                \n",
    "                # Handle multi-dimensional data (take spatial mean if needed)\n",
    "                if len(snow_data.dims) > 1:\n",
    "                    spatial_dims = [dim for dim in snow_data.dims if dim != 'time']\n",
    "                    if spatial_dims:\n",
    "                        snow_data = snow_data.mean(dim=spatial_dims)\n",
    "                \n",
    "                # Convert to pandas Series\n",
    "                snow_series = snow_data.to_pandas()\n",
    "                \n",
    "                # Handle negative values and unit conversion\n",
    "                if var_type == 'swe':\n",
    "                    # SWE should be positive\n",
    "                    snow_series = snow_series.abs()\n",
    "                    # Convert from kg/m² to mm if needed (1 kg/m² = 1 mm)\n",
    "                    # SUMMA typically outputs in kg/m²\n",
    "                elif var_type == 'depth':\n",
    "                    # Snow depth should be positive\n",
    "                    snow_series = snow_series.abs()\n",
    "                    # Convert from m to cm if needed\n",
    "                    if snow_series.max() < 10:  # Assume meters\n",
    "                        snow_series = snow_series * 100  # Convert to cm\n",
    "                \n",
    "                extracted_data[var_type] = snow_series\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "            \n",
    "            if site_row.empty:\n",
    "                print(f\"     ⚠️  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate snow season statistics\n",
    "            snow_stats = {}\n",
    "            \n",
    "            for var_type, series in extracted_data.items():\n",
    "                # Basic statistics\n",
    "                snow_stats[f'{var_type}_mean'] = series.mean()\n",
    "                snow_stats[f'{var_type}_max'] = series.max()\n",
    "                snow_stats[f'{var_type}_std'] = series.std()\n",
    "                \n",
    "                # Seasonal statistics\n",
    "                if len(series) > 0:\n",
    "                    # Find peak snow (maximum value)\n",
    "                    peak_idx = series.idxmax()\n",
    "                    snow_stats[f'{var_type}_peak_date'] = peak_idx\n",
    "                    snow_stats[f'{var_type}_peak_value'] = series[peak_idx]\n",
    "                    \n",
    "                    # Snow season length (days with snow > threshold)\n",
    "                    threshold = 10 if var_type == 'swe' else 5  # 10 mm SWE or 5 cm depth\n",
    "                    snow_days = (series > threshold).sum()\n",
    "                    snow_stats[f'{var_type}_season_length'] = snow_days\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'station_id': site_row['station_id'].iloc[0],\n",
    "                'latitude': site_row['lat'].iloc[0],\n",
    "                'longitude': site_row['lon'].iloc[0],\n",
    "                'elevation': site_row['elevation'].iloc[0],\n",
    "                'data_period': f\"{extracted_data[list(extracted_data.keys())[0]].index.min()} to {extracted_data[list(extracted_data.keys())[0]].index.max()}\",\n",
    "                'data_points': len(extracted_data[list(extracted_data.keys())[0]]),\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            # Add time series data\n",
    "            result.update(extracted_data)\n",
    "            \n",
    "            # Add statistics\n",
    "            result.update(snow_stats)\n",
    "            \n",
    "            snow_results.append(result)\n",
    "            processing_summary['domains_with_snow'] += 1\n",
    "            \n",
    "            swe_info = f\"{snow_stats.get('swe_mean', 0):.1f} mm (max: {snow_stats.get('swe_max', 0):.1f})\" if 'swe' in extracted_data else \"N/A\"\n",
    "            depth_info = f\"{snow_stats.get('depth_mean', 0):.1f} cm (max: {snow_stats.get('depth_max', 0):.1f})\" if 'depth' in extracted_data else \"N/A\"\n",
    "            \n",
    "            print(f\"     ✅ Snow extracted - SWE: {swe_info}, Depth: {depth_info}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\n❄️ Snow Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with results: {processing_summary['domains_with_results']}\")\n",
    "    print(f\"   Successful snow extractions: {processing_summary['domains_with_snow']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    return snow_results, processing_summary\n",
    "\n",
    "def load_norswe_observations(completed_domains):\n",
    "    \"\"\"\n",
    "    Load NorSWE observation data for snow validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n📥 Loading NorSWE Snow Observation Data...\")\n",
    "    \n",
    "    norswe_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_swe': 0,\n",
    "        'sites_with_depth': 0,\n",
    "        'total_swe_observations': 0,\n",
    "        'total_depth_observations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for extracted NorSWE observation data in domain directories\n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_observations']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        try:\n",
    "            print(f\"   📊 Loading {domain_name}...\")\n",
    "            \n",
    "            obs_summary['sites_found'] += 1\n",
    "            site_obs = {}\n",
    "            \n",
    "            # Load SWE observations\n",
    "            if domain['swe_obs_file']:\n",
    "                swe_df = pd.read_csv(domain['swe_obs_file'])\n",
    "                swe_df['time'] = pd.to_datetime(swe_df['time'])\n",
    "                swe_df.set_index('time', inplace=True)\n",
    "                \n",
    "                swe_obs = swe_df['SWE_kg_m2'].dropna()\n",
    "                \n",
    "                if len(swe_obs) > 0:\n",
    "                    site_obs['swe_timeseries'] = swe_obs\n",
    "                    site_obs['swe_mean'] = swe_obs.mean()\n",
    "                    site_obs['swe_max'] = swe_obs.max()\n",
    "                    site_obs['swe_std'] = swe_obs.std()\n",
    "                    \n",
    "                    # Seasonal statistics\n",
    "                    peak_idx = swe_obs.idxmax()\n",
    "                    site_obs['swe_peak_date'] = peak_idx\n",
    "                    site_obs['swe_peak_value'] = swe_obs[peak_idx]\n",
    "                    \n",
    "                    # Snow season length\n",
    "                    snow_days = (swe_obs > 10).sum()  # Days with >10mm SWE\n",
    "                    site_obs['swe_season_length'] = snow_days\n",
    "                    \n",
    "                    obs_summary['sites_with_swe'] += 1\n",
    "                    obs_summary['total_swe_observations'] += len(swe_obs)\n",
    "                    \n",
    "                    print(f\"     ❄️ SWE obs: {swe_obs.mean():.1f} ± {swe_obs.std():.1f} mm ({len(swe_obs)} points)\")\n",
    "            \n",
    "            # Load snow depth observations  \n",
    "            if domain['depth_obs_file']:\n",
    "                depth_df = pd.read_csv(domain['depth_obs_file'])\n",
    "                depth_df['time'] = pd.to_datetime(depth_df['time'])\n",
    "                depth_df.set_index('time', inplace=True)\n",
    "                \n",
    "                depth_obs = depth_df['Depth_m'].dropna() * 100  # Convert m to cm\n",
    "                \n",
    "                if len(depth_obs) > 0:\n",
    "                    site_obs['depth_timeseries'] = depth_obs\n",
    "                    site_obs['depth_mean'] = depth_obs.mean()\n",
    "                    site_obs['depth_max'] = depth_obs.max()\n",
    "                    site_obs['depth_std'] = depth_obs.std()\n",
    "                    \n",
    "                    # Seasonal statistics\n",
    "                    peak_idx = depth_obs.idxmax()\n",
    "                    site_obs['depth_peak_date'] = peak_idx\n",
    "                    site_obs['depth_peak_value'] = depth_obs[peak_idx]\n",
    "                    \n",
    "                    # Snow season length\n",
    "                    snow_days = (depth_obs > 5).sum()  # Days with >5cm depth\n",
    "                    site_obs['depth_season_length'] = snow_days\n",
    "                    \n",
    "                    obs_summary['sites_with_depth'] += 1\n",
    "                    obs_summary['total_depth_observations'] += len(depth_obs)\n",
    "                    \n",
    "                    print(f\"     📏 Depth obs: {depth_obs.mean():.1f} ± {depth_obs.std():.1f} cm ({len(depth_obs)} points)\")\n",
    "            \n",
    "            # Add site metadata\n",
    "            site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "            if not site_row.empty:\n",
    "                site_obs['latitude'] = site_row['lat'].iloc[0]\n",
    "                site_obs['longitude'] = site_row['lon'].iloc[0]\n",
    "                site_obs['elevation'] = site_row['elevation'].iloc[0]\n",
    "                site_obs['station_id'] = site_row['station_id'].iloc[0]\n",
    "                \n",
    "                norswe_obs[domain_name] = site_obs\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n❄️ NorSWE Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with SWE observations: {obs_summary['sites_with_swe']}\")\n",
    "    print(f\"   Sites with depth observations: {obs_summary['sites_with_depth']}\")\n",
    "    print(f\"   Total SWE observations: {obs_summary['total_swe_observations']}\")\n",
    "    print(f\"   Total depth observations: {obs_summary['total_depth_observations']}\")\n",
    "    \n",
    "    return norswe_obs, obs_summary\n",
    "\n",
    "def create_snow_comparison_analysis(snow_results, norswe_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive snow comparison analysis between simulated and observed snow\n",
    "    \"\"\"\n",
    "    print(f\"\\n❄️ Creating Snow Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in snow_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in norswe_obs:\n",
    "            # Align time periods for both SWE and snow depth\n",
    "            comparisons = {}\n",
    "            \n",
    "            # SWE comparison\n",
    "            if 'swe' in sim_result and 'swe_timeseries' in norswe_obs[domain_name]:\n",
    "                sim_swe = sim_result['swe']\n",
    "                obs_swe = norswe_obs[domain_name]['swe_timeseries']\n",
    "                \n",
    "                # Find common time period\n",
    "                common_start = max(sim_swe.index.min(), obs_swe.index.min())\n",
    "                common_end = min(sim_swe.index.max(), obs_swe.index.max())\n",
    "                \n",
    "                if common_start < common_end:\n",
    "                    # Resample to daily and align\n",
    "                    sim_daily = sim_swe.resample('D').mean().loc[common_start:common_end]\n",
    "                    obs_daily = obs_swe.resample('D').mean().loc[common_start:common_end]\n",
    "                    \n",
    "                    # Remove NaN values\n",
    "                    valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                    sim_valid = sim_daily[valid_mask]\n",
    "                    obs_valid = obs_daily[valid_mask]\n",
    "                    \n",
    "                    if len(sim_valid) > 30:  # Need minimum data for meaningful comparison\n",
    "                        \n",
    "                        # Calculate performance metrics\n",
    "                        rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                        bias = (sim_valid - obs_valid).mean()\n",
    "                        mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                        \n",
    "                        # Correlation\n",
    "                        try:\n",
    "                            correlation = obs_valid.corr(sim_valid)\n",
    "                            if pd.isna(correlation):\n",
    "                                correlation = 0.0\n",
    "                        except:\n",
    "                            correlation = 0.0\n",
    "                        \n",
    "                        # Nash-Sutcliffe Efficiency\n",
    "                        if obs_valid.var() > 0:\n",
    "                            nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                        else:\n",
    "                            nse = np.nan\n",
    "                        \n",
    "                        comparisons['swe'] = {\n",
    "                            'sim_data': sim_valid,\n",
    "                            'obs_data': obs_valid,\n",
    "                            'rmse': rmse,\n",
    "                            'bias': bias,\n",
    "                            'mae': mae,\n",
    "                            'correlation': correlation,\n",
    "                            'nse': nse,\n",
    "                            'n_points': len(sim_valid)\n",
    "                        }\n",
    "            \n",
    "            # Snow depth comparison\n",
    "            if 'depth' in sim_result and 'depth_timeseries' in norswe_obs[domain_name]:\n",
    "                sim_depth = sim_result['depth']\n",
    "                obs_depth = norswe_obs[domain_name]['depth_timeseries']\n",
    "                \n",
    "                # Find common time period\n",
    "                common_start = max(sim_depth.index.min(), obs_depth.index.min())\n",
    "                common_end = min(sim_depth.index.max(), obs_depth.index.max())\n",
    "                \n",
    "                if common_start < common_end:\n",
    "                    # Resample to daily and align\n",
    "                    sim_daily = sim_depth.resample('D').mean().loc[common_start:common_end]\n",
    "                    obs_daily = obs_depth.resample('D').mean().loc[common_start:common_end]\n",
    "                    \n",
    "                    # Remove NaN values\n",
    "                    valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                    sim_valid = sim_daily[valid_mask]\n",
    "                    obs_valid = obs_daily[valid_mask]\n",
    "                    \n",
    "                    if len(sim_valid) > 30:  # Need minimum data for meaningful comparison\n",
    "                        \n",
    "                        # Calculate performance metrics\n",
    "                        rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                        bias = (sim_valid - obs_valid).mean()\n",
    "                        mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                        \n",
    "                        # Correlation\n",
    "                        try:\n",
    "                            correlation = obs_valid.corr(sim_valid)\n",
    "                            if pd.isna(correlation):\n",
    "                                correlation = 0.0\n",
    "                        except:\n",
    "                            correlation = 0.0\n",
    "                        \n",
    "                        # Nash-Sutcliffe Efficiency\n",
    "                        if obs_valid.var() > 0:\n",
    "                            nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                        else:\n",
    "                            nse = np.nan\n",
    "                        \n",
    "                        comparisons['depth'] = {\n",
    "                            'sim_data': sim_valid,\n",
    "                            'obs_data': obs_valid,\n",
    "                            'rmse': rmse,\n",
    "                            'bias': bias,\n",
    "                            'mae': mae,\n",
    "                            'correlation': correlation,\n",
    "                            'nse': nse,\n",
    "                            'n_points': len(sim_valid)\n",
    "                        }\n",
    "            \n",
    "            if comparisons:\n",
    "                common_site = {\n",
    "                    'domain_name': domain_name,\n",
    "                    'latitude': sim_result['latitude'],\n",
    "                    'longitude': sim_result['longitude'],\n",
    "                    'elevation': sim_result['elevation'],\n",
    "                    'station_id': sim_result['station_id'],\n",
    "                    'comparisons': comparisons\n",
    "                }\n",
    "                \n",
    "                common_sites.append(common_site)\n",
    "                \n",
    "                # Print summary\n",
    "                comp_summary = []\n",
    "                for var_type, comp_data in comparisons.items():\n",
    "                    comp_summary.append(f\"{var_type}: r={comp_data['correlation']:.3f}, RMSE={comp_data['rmse']:.2f}\")\n",
    "                \n",
    "                print(f\"   ✅ {domain_name}: {', '.join(comp_summary)} ({comp_data['n_points']} points)\")\n",
    "    \n",
    "    print(f\"\\n❄️ Snow Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(\"   ⚠️  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive snow comparison visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # SWE scatter plot (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if swe_sites:\n",
    "        all_obs_swe = np.concatenate([site['comparisons']['swe']['obs_data'].values for site in swe_sites])\n",
    "        all_sim_swe = np.concatenate([site['comparisons']['swe']['sim_data'].values for site in swe_sites])\n",
    "        \n",
    "        ax1.scatter(all_obs_swe, all_sim_swe, alpha=0.5, s=15, c='blue')\n",
    "        \n",
    "        # 1:1 line\n",
    "        min_val = min(all_obs_swe.min(), all_sim_swe.min())\n",
    "        max_val = max(all_obs_swe.max(), all_sim_swe.max())\n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "        \n",
    "        ax1.set_xlabel('Observed SWE (mm)')\n",
    "        ax1.set_ylabel('Simulated SWE (mm)')\n",
    "        ax1.set_title('SWE: Simulated vs Observed')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall statistics\n",
    "        overall_corr = np.corrcoef(all_obs_swe, all_sim_swe)[0,1]\n",
    "        overall_rmse = np.sqrt(np.mean((all_obs_swe - all_sim_swe)**2))\n",
    "        overall_bias = np.mean(all_sim_swe - all_obs_swe)\n",
    "        \n",
    "        stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.1f}\\nBias = {overall_bias:+.1f}'\n",
    "        ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Snow depth scatter plot (top middle)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "    \n",
    "    if depth_sites:\n",
    "        all_obs_depth = np.concatenate([site['comparisons']['depth']['obs_data'].values for site in depth_sites])\n",
    "        all_sim_depth = np.concatenate([site['comparisons']['depth']['sim_data'].values for site in depth_sites])\n",
    "        \n",
    "        ax2.scatter(all_obs_depth, all_sim_depth, alpha=0.5, s=15, c='purple')\n",
    "        \n",
    "        # 1:1 line\n",
    "        min_val = min(all_obs_depth.min(), all_sim_depth.min())\n",
    "        max_val = max(all_obs_depth.max(), all_sim_depth.max())\n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "        \n",
    "        ax2.set_xlabel('Observed Snow Depth (cm)')\n",
    "        ax2.set_ylabel('Simulated Snow Depth (cm)')\n",
    "        ax2.set_title('Snow Depth: Simulated vs Observed')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall statistics\n",
    "        overall_corr = np.corrcoef(all_obs_depth, all_sim_depth)[0,1]\n",
    "        overall_rmse = np.sqrt(np.mean((all_obs_depth - all_sim_depth)**2))\n",
    "        overall_bias = np.mean(all_sim_depth - all_obs_depth)\n",
    "        \n",
    "        stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.1f}\\nBias = {overall_bias:+.1f}'\n",
    "        ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance vs elevation (top right)\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    elevations = [site['elevation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    swe_correlations = [site['comparisons']['swe']['correlation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if elevations and swe_correlations:\n",
    "        ax3.scatter(elevations, swe_correlations, alpha=0.7, s=40, c='green')\n",
    "        ax3.set_xlabel('Elevation (m)')\n",
    "        ax3.set_ylabel('SWE Correlation')\n",
    "        ax3.set_title('SWE Performance vs Elevation')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # SWE bias distribution (bottom left)\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    if swe_sites:\n",
    "        swe_biases = [site['comparisons']['swe']['bias'] for site in swe_sites]\n",
    "        ax4.hist(swe_biases, bins=15, color='lightblue', alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "        ax4.set_xlabel('SWE Bias (mm)')\n",
    "        ax4.set_ylabel('Number of Sites')\n",
    "        ax4.set_title('Distribution of SWE Bias')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Snow depth bias distribution (bottom middle)\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    if depth_sites:\n",
    "        depth_biases = [site['comparisons']['depth']['bias'] for site in depth_sites]\n",
    "        ax5.hist(depth_biases, bins=15, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "        ax5.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "        ax5.set_xlabel('Snow Depth Bias (cm)')\n",
    "        ax5.set_ylabel('Number of Sites')\n",
    "        ax5.set_title('Distribution of Snow Depth Bias')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Performance by latitude (bottom right)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    latitudes = [site['latitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    swe_rmses = [site['comparisons']['swe']['rmse'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if latitudes and swe_rmses:\n",
    "        ax6.scatter(latitudes, swe_rmses, alpha=0.7, s=40, c='orange')\n",
    "        ax6.set_xlabel('Latitude')\n",
    "        ax6.set_ylabel('SWE RMSE (mm)')\n",
    "        ax6.set_title('SWE Performance vs Latitude')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Comparison Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'snow_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Snow comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    # Create spatial performance map\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Map 1: SWE correlation map\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    lons = [site['longitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    corrs = [site['comparisons']['swe']['correlation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if lats and lons and corrs:\n",
    "        scatter1 = ax1.scatter(lons, lats, c=corrs, cmap='RdYlBu', s=80, \n",
    "                              vmin=0, vmax=1, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.set_title('Snow Model Performance: SWE Correlation')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim(-180, 180)\n",
    "        ax1.set_ylim(30, 85)  # Northern Hemisphere focus\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "        cbar1.set_label('SWE Correlation')\n",
    "    \n",
    "    # Map 2: SWE bias map\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    biases = [site['comparisons']['swe']['bias'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if lats and lons and biases:\n",
    "        max_abs_bias = max(abs(min(biases)), abs(max(biases)))\n",
    "        \n",
    "        scatter2 = ax2.scatter(lons, lats, c=biases, cmap='RdBu_r', s=80,\n",
    "                              vmin=-max_abs_bias, vmax=max_abs_bias, \n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax2.set_xlabel('Longitude')\n",
    "        ax2.set_ylabel('Latitude')\n",
    "        ax2.set_title('Snow Model Performance: SWE Bias (Sim - Obs)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(-180, 180)\n",
    "        ax2.set_ylim(30, 85)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "        cbar2.set_label('SWE Bias (mm)')\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Performance - Spatial Distribution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save spatial analysis\n",
    "    spatial_path = experiment_dir / 'plots' / 'snow_spatial_performance.png'\n",
    "    plt.savefig(spatial_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Snow spatial performance map saved: {spatial_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\n🔍 Step 3.1: Snow Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_snow_domains()\n",
    "\n",
    "# Create domain overview map\n",
    "total_selected, total_discovered, total_with_results, total_with_obs, total_complete = create_snow_domain_overview_map(completed_domains)\n",
    "\n",
    "print(f\"\\n❄️ Step 3.2: Snow Results Extraction\")\n",
    "\n",
    "# Extract snow results from simulations\n",
    "snow_results, snow_processing_summary = extract_snow_results_from_domains(completed_domains)\n",
    "\n",
    "# Load NorSWE observations\n",
    "norswe_obs, obs_summary = load_norswe_observations(completed_domains)\n",
    "\n",
    "print(f\"\\n❄️ Step 3.3: Snow Comparison Analysis\")\n",
    "\n",
    "# Create snow comparison analysis\n",
    "if snow_results and norswe_obs:\n",
    "    common_sites = create_snow_comparison_analysis(snow_results, norswe_obs)\n",
    "else:\n",
    "    print(\"   ⚠️  Insufficient data for snow comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final summary report\n",
    "print(f\"\\n📋 Creating Final NorSWE Snow Study Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'norswe_final_report.txt'\n",
    "summary_report_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"NorSWE Large Sample Snow Study - Final Analysis Report\\n\")\n",
    "    f.write(\"=\" * 58 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PROCESSING SUMMARY:\\n\")\n",
    "    f.write(f\"  Sites selected for analysis: {total_selected}\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\n\")\n",
    "    f.write(f\"  Observations extracted: {total_with_obs}\\n\")\n",
    "    f.write(f\"  Complete snow validation: {total_complete}\\n\")\n",
    "    f.write(f\"  Snow extractions successful: {snow_processing_summary['domains_with_snow']}\\n\")\n",
    "    f.write(f\"  NorSWE observations available: {obs_summary['sites_with_swe']}\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\n\\n\")\n",
    "        \n",
    "        # SWE performance summary\n",
    "        swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "        if swe_sites:\n",
    "            swe_correlations = [site['comparisons']['swe']['correlation'] for site in swe_sites]\n",
    "            swe_rmses = [site['comparisons']['swe']['rmse'] for site in swe_sites]\n",
    "            swe_biases = [site['comparisons']['swe']['bias'] for site in swe_sites]\n",
    "            \n",
    "            f.write(\"SWE PERFORMANCE SUMMARY:\\n\")\n",
    "            f.write(f\"  Mean correlation: {np.mean(swe_correlations):.3f} ± {np.std(swe_correlations):.3f}\\n\")\n",
    "            f.write(f\"  Mean RMSE: {np.mean(swe_rmses):.1f} ± {np.std(swe_rmses):.1f} mm\\n\")\n",
    "            f.write(f\"  Mean bias: {np.mean(swe_biases):+.1f} ± {np.std(swe_biases):.1f} mm\\n\\n\")\n",
    "        \n",
    "        # Snow depth performance summary\n",
    "        depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "        if depth_sites:\n",
    "            depth_correlations = [site['comparisons']['depth']['correlation'] for site in depth_sites]\n",
    "            depth_rmses = [site['comparisons']['depth']['rmse'] for site in depth_sites]\n",
    "            depth_biases = [site['comparisons']['depth']['bias'] for site in depth_sites]\n",
    "            \n",
    "            f.write(\"SNOW DEPTH PERFORMANCE SUMMARY:\\n\")\n",
    "            f.write(f\"  Mean correlation: {np.mean(depth_correlations):.3f} ± {np.std(depth_correlations):.3f}\\n\")\n",
    "            f.write(f\"  Mean RMSE: {np.mean(depth_rmses):.1f} ± {np.std(depth_rmses):.1f} cm\\n\")\n",
    "            f.write(f\"  Mean bias: {np.mean(depth_biases):+.1f} ± {np.std(depth_biases):.1f} cm\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING SITES (by SWE correlation):\\n\")\n",
    "        if swe_sites:\n",
    "            sorted_sites = sorted(swe_sites, key=lambda x: x['comparisons']['swe']['correlation'], reverse=True)\n",
    "            for i, site in enumerate(sorted_sites[:5]):\n",
    "                f.write(f\"  {i+1}. {site['domain_name']}: r={site['comparisons']['swe']['correlation']:.3f}, RMSE={site['comparisons']['swe']['rmse']:.1f} mm\\n\")\n",
    "\n",
    "print(f\"✅ Final summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\n🎉 Step 3 Complete: NorSWE Snow Validation Analysis\")\n",
    "print(f\"   📁 Results saved to: {experiment_dir}\")\n",
    "print(f\"   ❄️ Snow domain overview: {total_complete}/{total_selected} sites with complete validation\")\n",
    "\n",
    "if common_sites:\n",
    "    swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "    depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "    \n",
    "    if swe_sites:\n",
    "        swe_correlations = [site['comparisons']['swe']['correlation'] for site in swe_sites]\n",
    "        print(f\"   📊 SWE analysis: {len(swe_sites)} sites with sim/obs comparison\")\n",
    "        print(f\"   📈 SWE performance: Mean r = {np.mean(swe_correlations):.3f}\")\n",
    "    \n",
    "    if depth_sites:\n",
    "        depth_correlations = [site['comparisons']['depth']['correlation'] for site in depth_sites]\n",
    "        print(f\"   📏 Depth analysis: {len(depth_sites)} sites with sim/obs comparison\")\n",
    "        print(f\"   📈 Depth performance: Mean r = {np.mean(depth_correlations):.3f}\")\n",
    "else:\n",
    "    print(f\"   📈 Performance: Awaiting more simulation results\")\n",
    "\n",
    "print(f\"\\n✅ Large Sample NorSWE Snow Analysis Complete!\")\n",
    "print(f\"   ❄️ Multi-site snow hydrology validation achieved\")\n",
    "print(f\"   📊 Statistical patterns identified across elevation and climate gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ready to explore large sample basin simulations?** → **[Tutorial 04c: Large Sample Studies - CAMELS-Spat](./04c_large_sample_camelsspat.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
