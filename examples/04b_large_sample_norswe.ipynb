{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 9: NorSWE Large Sample Study (Snow Observation Network)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial extends our large sample studies approach to focus specifically on snow hydrology validation using the NorSWE (Northern Hemisphere Snow Water Equivalent) dataset. Building on the multi-site analysis framework demonstrated with FLUXNET, we now apply CONFLUENCE to systematically evaluate snow modeling performance across a network of snow observation stations throughout the northern hemisphere.\n",
    "\n",
    "### NorSWE: A Critical Snow Observation Network\n",
    "\n",
    "The NorSWE dataset represents one of the most comprehensive collections of snow observations available for hydrological model validation:\n",
    "\n",
    "**Spatial Coverage**:\n",
    "- **Northern Hemisphere focus**: Stations across snow-dominated regions\n",
    "- **Nordic emphasis**: Dense coverage in Scandinavia, Finland, and Norway\n",
    "- **Elevation gradients**: From coastal lowlands to high mountain regions\n",
    "- **Climate diversity**: Maritime, continental, and Arctic snow climates\n",
    "\n",
    "**Observational Richness**:\n",
    "- **Snow Water Equivalent (SWE)**: Direct measurements of snow mass\n",
    "- **Snow Depth**: Complementary snow pack structure information\n",
    "- **Long-term records**: Multi-decade observations at many sites\n",
    "- **Quality control**: Standardized measurement protocols and data processing\n",
    "\n",
    "### Scientific Importance of Snow Validation\n",
    "\n",
    "Snow processes represent some of the most challenging aspects of hydrological modeling:\n",
    "\n",
    "**Physical Complexity**:\n",
    "- **Phase transitions**: Freezing, melting, and sublimation processes\n",
    "- **Energy balance**: Complex interactions between radiation, temperature, and wind\n",
    "- **Layered structure**: Metamorphism and density changes within the snowpack\n",
    "- **Spatial variability**: Strong elevation and aspect dependencies\n",
    "\n",
    "**Hydrological Significance**:\n",
    "- **Seasonal storage**: Snow acts as a natural reservoir in many regions\n",
    "- **Timing control**: Snowmelt timing affects peak flows and water availability\n",
    "- **Climate sensitivity**: Snow processes are highly sensitive to temperature changes\n",
    "- **Extreme events**: Snow-rain transitions and rain-on-snow events\n",
    "\n",
    "### Why NorSWE for Large Sample Snow Studies?\n",
    "\n",
    "NorSWE provides unique advantages for systematic snow model evaluation:\n",
    "\n",
    "1. **Process Focus**: Dedicated snow observations rather than mixed-variable datasets\n",
    "2. **Measurement Quality**: Direct SWE measurements provide unambiguous validation targets\n",
    "3. **Environmental Gradients**: Sites span elevation, latitude, and climate gradients\n",
    "4. **Seasonal Dynamics**: Full seasonal cycle observations capture accumulation and ablation\n",
    "5. **Nordic Expertise**: Stations operated by countries with world-leading snow science\n",
    "\n",
    "### Research Questions for Snow Modeling\n",
    "\n",
    "Large sample studies with NorSWE enable investigation of critical snow science questions:\n",
    "\n",
    "1. **Model Physics**: How well do different snow process representations perform across environments?\n",
    "2. **Climate Controls**: Which meteorological variables most strongly control snow accumulation and melt?\n",
    "3. **Elevation Effects**: How do snow processes change with elevation and their representation in models?\n",
    "4. **Regional Patterns**: Are there systematic regional biases in snow modeling?\n",
    "5. **Seasonal Dynamics**: Can models capture both accumulation and ablation processes accurately?\n",
    "\n",
    "### Unique Challenges of Snow Modeling\n",
    "\n",
    "Snow modeling presents distinct challenges compared to other hydrological processes:\n",
    "\n",
    "**Meteorological Sensitivity**:\n",
    "- **Temperature thresholds**: Critical temperature for rain-snow transitions\n",
    "- **Radiation balance**: Complex interactions between shortwave and longwave radiation\n",
    "- **Wind effects**: Redistribution and sublimation processes\n",
    "- **Humidity control**: Sublimation rates and surface energy balance\n",
    "\n",
    "**Temporal Dynamics**:\n",
    "- **Seasonal cycle**: Distinct accumulation and ablation seasons\n",
    "- **Diurnal variation**: Strong daily cycles in energy balance\n",
    "- **Event-based processes**: Individual storm impacts on snowpack\n",
    "- **Intermittency**: Episodic accumulation and melt events\n",
    "\n",
    "### CONFLUENCE's Snow Modeling Capabilities\n",
    "\n",
    "CONFLUENCE's integration with SUMMA provides sophisticated snow modeling capabilities:\n",
    "\n",
    "**Advanced Snow Physics**:\n",
    "- **Multi-layer snowpack**: Explicit representation of snow stratigraphy\n",
    "- **Energy balance**: Detailed surface energy balance calculations\n",
    "- **Metamorphism**: Snow density and thermal property evolution\n",
    "- **Liquid water**: Representation of liquid water flow through snow\n",
    "\n",
    "**Flexible Parameterizations**:\n",
    "- **Multiple options**: Different approaches for key snow processes\n",
    "- **Sensitivity analysis**: Test different process representations\n",
    "- **Decision analysis**: Compare alternative model structures\n",
    "- **Uncertainty quantification**: Assess parameter and structural uncertainty\n",
    "\n",
    "### NorSWE vs. FLUXNET: Complementary Approaches\n",
    "\n",
    "While FLUXNET focused on energy balance validation, NorSWE provides complementary insights:\n",
    "\n",
    "| Aspect | FLUXNET | NorSWE |\n",
    "|--------|---------|--------|\n",
    "| **Focus** | Energy/carbon fluxes | Snow mass/depth |\n",
    "| **Process** | Continuous processes | Seasonal accumulation |\n",
    "| **Validation** | Flux measurements | State variables |\n",
    "| **Complexity** | Ecosystem interactions | Phase change physics |\n",
    "| **Temporal** | Year-round | Seasonal focus |\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "This tutorial demonstrates several key capabilities for snow-focused large sample studies:\n",
    "\n",
    "1. **Snow-Specific Configuration**: Adapt CONFLUENCE configurations for snow observation sites\n",
    "2. **Seasonal Analysis**: Focus on snow accumulation and ablation periods\n",
    "3. **Multi-Variable Validation**: Compare both SWE and snow depth simulations\n",
    "4. **Elevation Analysis**: Examine how model performance varies with elevation\n",
    "5. **Climate Sensitivity**: Assess model performance across different snow climates\n",
    "\n",
    "### Methodological Considerations\n",
    "\n",
    "Snow-focused large sample studies require specific methodological approaches:\n",
    "\n",
    "**Site Selection**:\n",
    "- **Elevation gradients**: Represent different snow accumulation zones\n",
    "- **Climate diversity**: Include maritime, continental, and Arctic sites\n",
    "- **Data quality**: Ensure reliable SWE and snow depth measurements\n",
    "- **Temporal coverage**: Adequate seasonal cycle representation\n",
    "\n",
    "**Analysis Approaches**:\n",
    "- **Seasonal statistics**: Focus on peak SWE, melt timing, and duration\n",
    "- **Process evaluation**: Assess accumulation vs. ablation performance\n",
    "- **Threshold analysis**: Evaluate temperature and precipitation thresholds\n",
    "- **Extreme events**: Analyze performance during unusual snow years\n",
    "\n",
    "### Tutorial Structure\n",
    "\n",
    "This tutorial follows the established large sample framework while emphasizing snow-specific aspects:\n",
    "\n",
    "1. **NorSWE Site Selection**: Choose representative sites across snow environments\n",
    "2. **Snow-Focused Configuration**: Adapt CONFLUENCE for snow observation validation\n",
    "3. **Seasonal Analysis Setup**: Configure for snow season evaluation\n",
    "4. **Batch Processing**: Execute CONFLUENCE across multiple snow sites\n",
    "5. **Snow-Specific Results**: Collect and analyze SWE and snow depth outputs\n",
    "6. **Elevation Analysis**: Examine performance across elevation gradients\n",
    "7. **Climate Comparison**: Compare results across different snow climates\n",
    "\n",
    "### Scientific Impact\n",
    "\n",
    "NorSWE large sample studies contribute to advancing snow science:\n",
    "\n",
    "- **Model Validation**: Systematic evaluation of snow process representations\n",
    "- **Process Understanding**: Identify key controls on snow accumulation and melt\n",
    "- **Climate Applications**: Improve projections of snow under changing climate\n",
    "- **Operational Applications**: Enhance seasonal forecasting and water management\n",
    "- **Uncertainty Assessment**: Quantify reliability of snow predictions\n",
    "\n",
    "### Building on Previous Tutorials\n",
    "\n",
    "This tutorial leverages all the skills developed throughout the CONFLUENCE series:\n",
    "\n",
    "- **Point-scale understanding**: Foundation in vertical snow processes\n",
    "- **Workflow automation**: Efficient multi-site processing\n",
    "- **Configuration management**: Template-based site setup\n",
    "- **Results analysis**: Statistical evaluation of multi-site results\n",
    "- **Visualization**: Clear presentation of spatial and temporal patterns\n",
    "\n",
    "By applying these skills to snow-focused validation, you'll gain expertise in one of the most challenging aspects of hydrological modeling while contributing to improved understanding of snow processes across diverse northern hemisphere environments.\n",
    "\n",
    "The combination of CONFLUENCE's sophisticated snow modeling capabilities with NorSWE's comprehensive observation network provides a powerful framework for advancing snow science through systematic, large sample analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"coolwarm\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the NorSWE large sample experiment\n",
    "experiment_config = {\n",
    "    'experiment_name': 'norswe_tutorial',\n",
    "    'norswe_path': '/work/comphyd_lab/data/geospatial-data/NorSWE/NorSWE-NorEEN_1979-2021_v2.nc',\n",
    "    'template_config': '../CONFLUENCE/0_config_files/config_norswe_template.yaml',\n",
    "    'config_dir': '../CONFLUENCE/0_config_files/norswe',\n",
    "    'output_dir': './norswe_output',\n",
    "    'base_path': '/work/comphyd_lab/data/CONFLUENCE_data/norswe',\n",
    "    'min_completeness': 0.0,  # Minimum % data completeness\n",
    "    'max_stations': 10,  # Number of stations to process\n",
    "    'start_year': 2010,  # Optional: filter data by year\n",
    "    'end_year': 2020,\n",
    "    'no_submit': False  # Set to True for dry run\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "experiment_dir = Path(f\"./experiments/{experiment_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "Path(experiment_config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "Path(experiment_config['config_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(experiment_dir / 'experiment_config.yaml', 'w') as f:\n",
    "    yaml.dump(experiment_config, f)\n",
    "\n",
    "print(f\"Experiment configured: {experiment_config['experiment_name']}\")\n",
    "print(f\"Processing up to {experiment_config['max_stations']} NorSWE stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore NorSWE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Open NorSWE dataset\n",
    "ds = xr.open_dataset(experiment_config['norswe_path'])\n",
    "\n",
    "print(\"NorSWE Dataset Information:\")\n",
    "print(f\"Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"Number of stations: {len(ds.station_id)}\")\n",
    "print(f\"Variables: {list(ds.data_vars)}\")\n",
    "print(f\"Coordinates: {list(ds.coords)}\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(ds)\n",
    "\n",
    "ds.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process NorSWE Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the processing function from the script\n",
    "sys.path.append(str(confluence_path / '9_scripts'))\n",
    "from run_sites_norswe import process_norswe_data\n",
    "\n",
    "# Process station data\n",
    "stations_csv = Path('norswe_stations.csv')\n",
    "stations_df = pd.read_csv(stations_csv)\n",
    "\n",
    "'''\n",
    "stations_df = process_norswe_data(\n",
    "    experiment_config['norswe_path'],\n",
    "    str(stations_csv),\n",
    "    start_year=experiment_config.get('start_year'),\n",
    "    end_year=experiment_config.get('end_year'),\n",
    "    use_existing_csv=True\n",
    ")\n",
    "'''\n",
    "\n",
    "print(f\"Processed {len(stations_df)} stations\")\n",
    "print(\"\\nStation data columns:\")\n",
    "for col in stations_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Display sample stations\n",
    "print(\"\\nSample stations:\")\n",
    "display(stations_df[['station_id', 'station_name', 'lat', 'lon', 'elevation', 'swq_completeness']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Station Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create station distribution map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Geographic distribution\n",
    "scatter = ax1.scatter(stations_df['lon'], stations_df['lat'], \n",
    "                     c=stations_df['elevation'], cmap='terrain',\n",
    "                     s=50, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_title('NorSWE Station Locations', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Longitude', fontsize=12)\n",
    "ax1.set_ylabel('Latitude', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar for elevation\n",
    "cbar = plt.colorbar(scatter, ax=ax1)\n",
    "cbar.set_label('Elevation (m)', fontsize=12)\n",
    "\n",
    "# Elevation distribution\n",
    "ax2.hist(stations_df['elevation'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Elevation (m)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Stations', fontsize=12)\n",
    "ax2.set_title('Elevation Distribution', fontsize=14)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data completeness distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(stations_df['swq_completeness'], bins=20, color='lightblue', \n",
    "        edgecolor='black', alpha=0.7, label='SWE')\n",
    "ax.hist(stations_df['snd_completeness'], bins=20, color='lightcoral', \n",
    "        edgecolor='black', alpha=0.5, label='Snow Depth')\n",
    "ax.set_xlabel('Data Completeness (%)', fontsize=12)\n",
    "ax.set_ylabel('Number of Stations', fontsize=12)\n",
    "ax.set_title('Data Completeness Distribution', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select Stations for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stations by data completeness\n",
    "complete_stations = stations_df[\n",
    "    (stations_df['swq_completeness'] >= experiment_config['min_completeness']) &\n",
    "    (stations_df['snd_completeness'] >= experiment_config['min_completeness'])\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(complete_stations)} stations with ≥{experiment_config['min_completeness']}% completeness\")\n",
    "\n",
    "# Select stations (prioritize by completeness and elevation diversity)\n",
    "if len(complete_stations) > experiment_config['max_stations']:\n",
    "    # Sort by completeness and select diverse elevations\n",
    "    complete_stations = complete_stations.sort_values(\n",
    "        by=['swq_completeness', 'snd_completeness'], \n",
    "        ascending=False\n",
    "    ).head(experiment_config['max_stations'])\n",
    "\n",
    "print(f\"\\nSelected {len(complete_stations)} stations for processing\")\n",
    "display(complete_stations[['station_id', 'station_name', 'elevation', 'swq_completeness']])\n",
    "\n",
    "# Save selected stations\n",
    "complete_stations.to_csv(experiment_dir / 'selected_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Snow Observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Example: Extract and plot snow data for one station\n",
    "from run_sites_norswe import extract_snow_data\n",
    "\n",
    "# Select first station as example\n",
    "example_station = complete_stations.iloc[0]\n",
    "station_id = example_station['station_id']\n",
    "station_name = example_station['Watershed_Name']\n",
    "\n",
    "# Create output directory for this station\n",
    "station_dir = Path(experiment_config['base_path']) / f\"domain_{station_name}\"\n",
    "station_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract snow data\n",
    "swe_file, snd_file = extract_snow_data(\n",
    "    experiment_config['norswe_path'],\n",
    "    station_id,\n",
    "    str(station_dir),\n",
    "    start_year=experiment_config.get('start_year'),\n",
    "    end_year=experiment_config.get('end_year')\n",
    ")\n",
    "\n",
    "# Load and visualize the extracted data\n",
    "swe_df = pd.read_csv(swe_file)\n",
    "snd_df = pd.read_csv(snd_file)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot SWE\n",
    "ax1.plot(pd.to_datetime(swe_df['time']), swe_df['SWE_kg_m2'], \n",
    "         color='blue', linewidth=1.5, alpha=0.7)\n",
    "ax1.set_ylabel('SWE (kg/m²)', fontsize=12)\n",
    "ax1.set_title(f'Snow Observations - {station_name} (ID: {station_id})', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot snow depth\n",
    "ax2.plot(pd.to_datetime(snd_df['time']), snd_df['Depth_m'] * 100, \n",
    "         color='purple', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_ylabel('Snow Depth (cm)', fontsize=12)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Extracted snow data for {station_name}:\")\n",
    "print(f\"  SWE file: {swe_file}\")\n",
    "print(f\"  Snow depth file: {snd_file}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate configs for selected stations\n",
    "from run_sites_norswe import generate_config_file\n",
    "\n",
    "config_dir = Path(experiment_config['config_dir'])\n",
    "generated_configs = []\n",
    "\n",
    "for _, station in complete_stations.iterrows():\n",
    "    station_name = station['Watershed_Name']\n",
    "    pour_point = station['POUR_POINT_COORDS']\n",
    "    bounding_box = station['BOUNDING_BOX_COORDS']\n",
    "    \n",
    "    # Generate config file\n",
    "    config_path = config_dir / f\"config_{station_name}.yaml\"\n",
    "    \n",
    "    generate_config_file(\n",
    "        experiment_config['template_config'],\n",
    "        str(config_path),\n",
    "        station_name,\n",
    "        pour_point,\n",
    "        bounding_box\n",
    "    )\n",
    "    \n",
    "    generated_configs.append(config_path)\n",
    "\n",
    "print(f\"Generated {len(generated_configs)} configuration files\")\n",
    "print(\"\\nExample config locations:\")\n",
    "for config in generated_configs[:3]:\n",
    "    print(f\"  - {config}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launch CONFLUENCE Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Prepare to launch CONFLUENCE runs\n",
    "from run_sites_norswe import run_confluence\n",
    "\n",
    "submitted_jobs = []\n",
    "skipped_jobs = []\n",
    "\n",
    "# Interactive decision (for notebook, we'll simulate 'y' or 'n')\n",
    "if experiment_config['no_submit']:\n",
    "    submit_jobs = 'n'\n",
    "    print(\"DRY RUN MODE - No jobs will be submitted\")\n",
    "else:\n",
    "    submit_jobs = 'y'  # In real notebook, you'd ask user\n",
    "    print(\"Preparing to submit CONFLUENCE jobs...\")\n",
    "\n",
    "if submit_jobs == 'y':\n",
    "    for _, station in complete_stations.iterrows():\n",
    "        station_name = station['Watershed_Name']\n",
    "        \n",
    "        # Check if simulation already exists\n",
    "        sim_path = Path(experiment_config['base_path']) / f\"domain_{station_name}\" / \"simulations\" / \"run_1\" / \"SUMMA\" / \"run_1_timestep.nc\"\n",
    "        \n",
    "        if sim_path.exists():\n",
    "            print(f\"Skipping {station_name} - simulation already exists\")\n",
    "            skipped_jobs.append(station_name)\n",
    "            continue\n",
    "        \n",
    "        # Submit job\n",
    "        config_path = config_dir / f\"config_{station_name}.yaml\"\n",
    "        job_id = run_confluence(str(config_path), station_name)\n",
    "        \n",
    "        if job_id:\n",
    "            submitted_jobs.append((station_name, job_id))\n",
    "            print(f\"Submitted job for {station_name}: {job_id}\")\n",
    "        \n",
    "        # Small delay between submissions\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nJob submission summary:\")\n",
    "print(f\"  Submitted: {len(submitted_jobs)}\")\n",
    "print(f\"  Skipped: {len(skipped_jobs)}\")\n",
    "\n",
    "if submitted_jobs:\n",
    "    print(\"\\nSubmitted jobs:\")\n",
    "    for station_name, job_id in submitted_jobs[:5]:\n",
    "        print(f\"  {station_name}: {job_id}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Check job status\n",
    "def check_job_status(user=None):\n",
    "    user = user or os.environ.get('USER')\n",
    "    cmd = ['squeue', '-u', user]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "print(\"Current job status:\")\n",
    "print(check_job_status())\n",
    "\n",
    "# Save job information\n",
    "if submitted_jobs:\n",
    "    job_df = pd.DataFrame(submitted_jobs, columns=['station_name', 'job_id'])\n",
    "    job_df.to_csv(experiment_dir / 'submitted_jobs.csv', index=False)\n",
    "    print(f\"\\nJob information saved to {experiment_dir / 'submitted_jobs.csv'}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Find Completed Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find completed simulations\n",
    "base_path = Path(experiment_config['base_path'])\n",
    "completed = []\n",
    "\n",
    "for _, station in complete_stations.iterrows():\n",
    "    station_name = station['Watershed_Name']\n",
    "    sim_path = base_path / f\"domain_{station_name}\" / \"simulations\" / \"run_1\" / \"SUMMA\"\n",
    "    \n",
    "    if sim_path.exists() and list(sim_path.glob(\"*timestep*.nc\")):\n",
    "        completed.append({\n",
    "            'station_name': station_name,\n",
    "            'station_id': station['station_id'],\n",
    "            'elevation': station['elevation'],\n",
    "            'sim_path': sim_path\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(completed)} completed simulations\")\n",
    "if completed:\n",
    "    completed_df = pd.DataFrame(completed)\n",
    "    display(completed_df[['station_name', 'station_id', 'elevation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load SUMMA snow output\n",
    "def load_summa_snow(sim_path):\n",
    "    output_files = list(Path(sim_path).glob(\"*day*.nc\"))\n",
    "    if output_files:\n",
    "        ds = xr.open_dataset(output_files[0])\n",
    "        \n",
    "        # Extract SWE and snow depth if available\n",
    "        data = {}\n",
    "        if 'scalarSWE' in ds.variables:\n",
    "            data['swe'] = ds.scalarSWE.values.flatten()\n",
    "        if 'scalarSnowDepth' in ds.variables:\n",
    "            data['depth'] = ds.scalarSnowDepth.values.flatten()\n",
    "        \n",
    "        data['time'] = pd.to_datetime(ds.time.values)\n",
    "        ds.close()\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    return None\n",
    "\n",
    "# Compare modeled and observed snow for completed simulations\n",
    "if completed:\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i, site in enumerate(completed[:30]):\n",
    "        # Load model output\n",
    "        model_data = load_summa_snow(site['sim_path'])\n",
    "        \n",
    "        # Load observations\n",
    "        #obs_path = base_path / f\"domain_{site['station_name']}\" / \"observations\" / \"snow\" / \"raw_data\"\n",
    "        #swe_obs = pd.read_csv(obs_path / \"swe\" / f\"{site['station_id']}_swe.csv\")\n",
    "        #snd_obs = pd.read_csv(obs_path / \"depth\" / f\"{site['station_id']}_depth.csv\")\n",
    "        \n",
    "        if model_data is not None:\n",
    "            if 'swe' in model_data:\n",
    "                ax.plot(model_data['time'], model_data['swe'], \n",
    "                        'r-', label=site, alpha=0.7)\n",
    "            ax.set_ylabel('SWE (kg/m²)')\n",
    "            ax.set_title(f\"{site['station_name']} - Elevation: {site['elevation']}m\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Analysis Across Elevations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
