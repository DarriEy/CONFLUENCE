{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NorSWE Large Sample Experiment Tutorial\n",
    "\n",
    "This notebook demonstrates how to run CONFLUENCE over multiple snow observation sites from the NorSWE dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "NorSWE provides snow observations (snow water equivalent and snow depth) from a network of stations across the northern hemisphere. Running CONFLUENCE at these sites allows us to:\n",
    "\n",
    "- Validate snow modeling in Nordic conditions\n",
    "- Compare model performance across different elevations and climates\n",
    "- Test model physics in snow-dominated environments\n",
    "- Analyze mountain snow processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "\n",
    "# Add CONFLUENCE to path\n",
    "confluence_path = Path('../').resolve()\n",
    "sys.path.append(str(confluence_path))\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"coolwarm\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the NorSWE large sample experiment\n",
    "experiment_config = {\n",
    "    'experiment_name': 'norswe_tutorial',\n",
    "    'norswe_path': '/work/comphyd_lab/data/geospatial-data/NorSWE/NorSWE-NorEEN_1979-2021_v2.nc',\n",
    "    'template_config': '../CONFLUENCE/0_config_files/config_norswe_template.yaml',\n",
    "    'config_dir': '../CONFLUENCE/0_config_files/norswe',\n",
    "    'output_dir': './norswe_output',\n",
    "    'base_path': '/work/comphyd_lab/data/CONFLUENCE_data/norswe',\n",
    "    'min_completeness': 0.0,  # Minimum % data completeness\n",
    "    'max_stations': 10,  # Number of stations to process\n",
    "    'start_year': 2010,  # Optional: filter data by year\n",
    "    'end_year': 2020,\n",
    "    'no_submit': False  # Set to True for dry run\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "experiment_dir = Path(f\"./experiments/{experiment_config['experiment_name']}\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "Path(experiment_config['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "Path(experiment_config['config_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(experiment_dir / 'experiment_config.yaml', 'w') as f:\n",
    "    yaml.dump(experiment_config, f)\n",
    "\n",
    "print(f\"Experiment configured: {experiment_config['experiment_name']}\")\n",
    "print(f\"Processing up to {experiment_config['max_stations']} NorSWE stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore NorSWE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Open NorSWE dataset\n",
    "ds = xr.open_dataset(experiment_config['norswe_path'])\n",
    "\n",
    "print(\"NorSWE Dataset Information:\")\n",
    "print(f\"Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"Number of stations: {len(ds.station_id)}\")\n",
    "print(f\"Variables: {list(ds.data_vars)}\")\n",
    "print(f\"Coordinates: {list(ds.coords)}\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(ds)\n",
    "\n",
    "ds.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process NorSWE Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the processing function from the script\n",
    "sys.path.append(str(confluence_path / '9_scripts'))\n",
    "from run_sites_norswe import process_norswe_data\n",
    "\n",
    "# Process station data\n",
    "stations_csv = Path('norswe_stations.csv')\n",
    "stations_df = pd.read_csv(stations_csv)\n",
    "\n",
    "'''\n",
    "stations_df = process_norswe_data(\n",
    "    experiment_config['norswe_path'],\n",
    "    str(stations_csv),\n",
    "    start_year=experiment_config.get('start_year'),\n",
    "    end_year=experiment_config.get('end_year'),\n",
    "    use_existing_csv=True\n",
    ")\n",
    "'''\n",
    "\n",
    "print(f\"Processed {len(stations_df)} stations\")\n",
    "print(\"\\nStation data columns:\")\n",
    "for col in stations_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Display sample stations\n",
    "print(\"\\nSample stations:\")\n",
    "display(stations_df[['station_id', 'station_name', 'lat', 'lon', 'elevation', 'swq_completeness']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Station Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create station distribution map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Geographic distribution\n",
    "scatter = ax1.scatter(stations_df['lon'], stations_df['lat'], \n",
    "                     c=stations_df['elevation'], cmap='terrain',\n",
    "                     s=50, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_title('NorSWE Station Locations', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Longitude', fontsize=12)\n",
    "ax1.set_ylabel('Latitude', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar for elevation\n",
    "cbar = plt.colorbar(scatter, ax=ax1)\n",
    "cbar.set_label('Elevation (m)', fontsize=12)\n",
    "\n",
    "# Elevation distribution\n",
    "ax2.hist(stations_df['elevation'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Elevation (m)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Stations', fontsize=12)\n",
    "ax2.set_title('Elevation Distribution', fontsize=14)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data completeness distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(stations_df['swq_completeness'], bins=20, color='lightblue', \n",
    "        edgecolor='black', alpha=0.7, label='SWE')\n",
    "ax.hist(stations_df['snd_completeness'], bins=20, color='lightcoral', \n",
    "        edgecolor='black', alpha=0.5, label='Snow Depth')\n",
    "ax.set_xlabel('Data Completeness (%)', fontsize=12)\n",
    "ax.set_ylabel('Number of Stations', fontsize=12)\n",
    "ax.set_title('Data Completeness Distribution', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select Stations for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stations by data completeness\n",
    "complete_stations = stations_df[\n",
    "    (stations_df['swq_completeness'] >= experiment_config['min_completeness']) &\n",
    "    (stations_df['snd_completeness'] >= experiment_config['min_completeness'])\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(complete_stations)} stations with ≥{experiment_config['min_completeness']}% completeness\")\n",
    "\n",
    "# Select stations (prioritize by completeness and elevation diversity)\n",
    "if len(complete_stations) > experiment_config['max_stations']:\n",
    "    # Sort by completeness and select diverse elevations\n",
    "    complete_stations = complete_stations.sort_values(\n",
    "        by=['swq_completeness', 'snd_completeness'], \n",
    "        ascending=False\n",
    "    ).head(experiment_config['max_stations'])\n",
    "\n",
    "print(f\"\\nSelected {len(complete_stations)} stations for processing\")\n",
    "display(complete_stations[['station_id', 'station_name', 'elevation', 'swq_completeness']])\n",
    "\n",
    "# Save selected stations\n",
    "complete_stations.to_csv(experiment_dir / 'selected_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Snow Observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Example: Extract and plot snow data for one station\n",
    "from run_sites_norswe import extract_snow_data\n",
    "\n",
    "# Select first station as example\n",
    "example_station = complete_stations.iloc[0]\n",
    "station_id = example_station['station_id']\n",
    "station_name = example_station['Watershed_Name']\n",
    "\n",
    "# Create output directory for this station\n",
    "station_dir = Path(experiment_config['base_path']) / f\"domain_{station_name}\"\n",
    "station_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract snow data\n",
    "swe_file, snd_file = extract_snow_data(\n",
    "    experiment_config['norswe_path'],\n",
    "    station_id,\n",
    "    str(station_dir),\n",
    "    start_year=experiment_config.get('start_year'),\n",
    "    end_year=experiment_config.get('end_year')\n",
    ")\n",
    "\n",
    "# Load and visualize the extracted data\n",
    "swe_df = pd.read_csv(swe_file)\n",
    "snd_df = pd.read_csv(snd_file)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot SWE\n",
    "ax1.plot(pd.to_datetime(swe_df['time']), swe_df['SWE_kg_m2'], \n",
    "         color='blue', linewidth=1.5, alpha=0.7)\n",
    "ax1.set_ylabel('SWE (kg/m²)', fontsize=12)\n",
    "ax1.set_title(f'Snow Observations - {station_name} (ID: {station_id})', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot snow depth\n",
    "ax2.plot(pd.to_datetime(snd_df['time']), snd_df['Depth_m'] * 100, \n",
    "         color='purple', linewidth=1.5, alpha=0.7)\n",
    "ax2.set_ylabel('Snow Depth (cm)', fontsize=12)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Extracted snow data for {station_name}:\")\n",
    "print(f\"  SWE file: {swe_file}\")\n",
    "print(f\"  Snow depth file: {snd_file}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate configs for selected stations\n",
    "from run_sites_norswe import generate_config_file\n",
    "\n",
    "config_dir = Path(experiment_config['config_dir'])\n",
    "generated_configs = []\n",
    "\n",
    "for _, station in complete_stations.iterrows():\n",
    "    station_name = station['Watershed_Name']\n",
    "    pour_point = station['POUR_POINT_COORDS']\n",
    "    bounding_box = station['BOUNDING_BOX_COORDS']\n",
    "    \n",
    "    # Generate config file\n",
    "    config_path = config_dir / f\"config_{station_name}.yaml\"\n",
    "    \n",
    "    generate_config_file(\n",
    "        experiment_config['template_config'],\n",
    "        str(config_path),\n",
    "        station_name,\n",
    "        pour_point,\n",
    "        bounding_box\n",
    "    )\n",
    "    \n",
    "    generated_configs.append(config_path)\n",
    "\n",
    "print(f\"Generated {len(generated_configs)} configuration files\")\n",
    "print(\"\\nExample config locations:\")\n",
    "for config in generated_configs[:3]:\n",
    "    print(f\"  - {config}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launch CONFLUENCE Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Prepare to launch CONFLUENCE runs\n",
    "from run_sites_norswe import run_confluence\n",
    "\n",
    "submitted_jobs = []\n",
    "skipped_jobs = []\n",
    "\n",
    "# Interactive decision (for notebook, we'll simulate 'y' or 'n')\n",
    "if experiment_config['no_submit']:\n",
    "    submit_jobs = 'n'\n",
    "    print(\"DRY RUN MODE - No jobs will be submitted\")\n",
    "else:\n",
    "    submit_jobs = 'y'  # In real notebook, you'd ask user\n",
    "    print(\"Preparing to submit CONFLUENCE jobs...\")\n",
    "\n",
    "if submit_jobs == 'y':\n",
    "    for _, station in complete_stations.iterrows():\n",
    "        station_name = station['Watershed_Name']\n",
    "        \n",
    "        # Check if simulation already exists\n",
    "        sim_path = Path(experiment_config['base_path']) / f\"domain_{station_name}\" / \"simulations\" / \"run_1\" / \"SUMMA\" / \"run_1_timestep.nc\"\n",
    "        \n",
    "        if sim_path.exists():\n",
    "            print(f\"Skipping {station_name} - simulation already exists\")\n",
    "            skipped_jobs.append(station_name)\n",
    "            continue\n",
    "        \n",
    "        # Submit job\n",
    "        config_path = config_dir / f\"config_{station_name}.yaml\"\n",
    "        job_id = run_confluence(str(config_path), station_name)\n",
    "        \n",
    "        if job_id:\n",
    "            submitted_jobs.append((station_name, job_id))\n",
    "            print(f\"Submitted job for {station_name}: {job_id}\")\n",
    "        \n",
    "        # Small delay between submissions\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nJob submission summary:\")\n",
    "print(f\"  Submitted: {len(submitted_jobs)}\")\n",
    "print(f\"  Skipped: {len(skipped_jobs)}\")\n",
    "\n",
    "if submitted_jobs:\n",
    "    print(\"\\nSubmitted jobs:\")\n",
    "    for station_name, job_id in submitted_jobs[:5]:\n",
    "        print(f\"  {station_name}: {job_id}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Check job status\n",
    "def check_job_status(user=None):\n",
    "    user = user or os.environ.get('USER')\n",
    "    cmd = ['squeue', '-u', user]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "print(\"Current job status:\")\n",
    "print(check_job_status())\n",
    "\n",
    "# Save job information\n",
    "if submitted_jobs:\n",
    "    job_df = pd.DataFrame(submitted_jobs, columns=['station_name', 'job_id'])\n",
    "    job_df.to_csv(experiment_dir / 'submitted_jobs.csv', index=False)\n",
    "    print(f\"\\nJob information saved to {experiment_dir / 'submitted_jobs.csv'}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Find Completed Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find completed simulations\n",
    "base_path = Path(experiment_config['base_path'])\n",
    "completed = []\n",
    "\n",
    "for _, station in complete_stations.iterrows():\n",
    "    station_name = station['Watershed_Name']\n",
    "    sim_path = base_path / f\"domain_{station_name}\" / \"simulations\" / \"run_1\" / \"SUMMA\"\n",
    "    \n",
    "    if sim_path.exists() and list(sim_path.glob(\"*timestep*.nc\")):\n",
    "        completed.append({\n",
    "            'station_name': station_name,\n",
    "            'station_id': station['station_id'],\n",
    "            'elevation': station['elevation'],\n",
    "            'sim_path': sim_path\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(completed)} completed simulations\")\n",
    "if completed:\n",
    "    completed_df = pd.DataFrame(completed)\n",
    "    display(completed_df[['station_name', 'station_id', 'elevation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load SUMMA snow output\n",
    "def load_summa_snow(sim_path):\n",
    "    output_files = list(Path(sim_path).glob(\"*timestep*.nc\"))\n",
    "    if output_files:\n",
    "        ds = xr.open_dataset(output_files[0])\n",
    "        \n",
    "        # Extract SWE and snow depth if available\n",
    "        data = {}\n",
    "        if 'scalarSWE' in ds.variables:\n",
    "            data['swe'] = ds.scalarSWE.values.flatten()\n",
    "        if 'scalarSnowDepth' in ds.variables:\n",
    "            data['depth'] = ds.scalarSnowDepth.values.flatten()\n",
    "        \n",
    "        data['time'] = pd.to_datetime(ds.time.values)\n",
    "        ds.close()\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    return None\n",
    "\n",
    "# Compare modeled and observed snow for completed simulations\n",
    "if completed:\n",
    "    fig, axes = plt.subplots(len(completed[:3]), 2, figsize=(15, 4*len(completed[:3])))\n",
    "    \n",
    "    for i, site in enumerate(completed[:3]):\n",
    "        # Load model output\n",
    "        model_data = load_summa_snow(site['sim_path'])\n",
    "        \n",
    "        # Load observations\n",
    "        obs_path = base_path / f\"domain_{site['station_name']}\" / \"observations\" / \"snow\" / \"raw_data\"\n",
    "        swe_obs = pd.read_csv(obs_path / \"swe\" / f\"{site['station_id']}_swe.csv\")\n",
    "        snd_obs = pd.read_csv(obs_path / \"depth\" / f\"{site['station_id']}_depth.csv\")\n",
    "        \n",
    "        if model_data is not None:\n",
    "            # Plot SWE\n",
    "            ax1 = axes[i, 0] if len(completed) > 1 else axes[0]\n",
    "            ax1.plot(pd.to_datetime(swe_obs['time']), swe_obs['SWE_kg_m2'], \n",
    "                    'k-', label='Observed', linewidth=2)\n",
    "            if 'swe' in model_data:\n",
    "                ax1.plot(model_data['time'], model_data['swe'], \n",
    "                        'r-', label='Modeled', alpha=0.7)\n",
    "            ax1.set_ylabel('SWE (kg/m²)')\n",
    "            ax1.set_title(f\"{site['station_name']} - Elevation: {site['elevation']}m\")\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot snow depth\n",
    "            ax2 = axes[i, 1] if len(completed) > 1 else axes[1]\n",
    "            ax2.plot(pd.to_datetime(snd_obs['time']), snd_obs['Depth_m'] * 100, \n",
    "                    'k-', label='Observed', linewidth=2)\n",
    "            if 'depth' in model_data:\n",
    "                ax2.plot(model_data['time'], model_data['depth'] * 100, \n",
    "                        'r-', label='Modeled', alpha=0.7)\n",
    "            ax2.set_ylabel('Snow Depth (cm)')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Analysis Across Elevations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
