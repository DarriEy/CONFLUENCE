{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFLUENCE Tutorial - 9: NorSWE Large Sample Study (Snow Observation Network)\n",
    "\n",
    "## Introduction\n",
    "This tutorial extends our large sample studies approach to focus specifically on snow hydrology validation using the NorSWE (Northern Hemisphere Snow Water Equivalent) dataset. Building on the multi-site analysis framework demonstrated with FLUXNET, we now apply CONFLUENCE to systematically evaluate snow modeling performance across a network of snow observation stations throughout the northern hemisphere.\n",
    "\n",
    "## NorSWE: A Critical Snow Observation Network\n",
    "The NorSWE dataset represents one of the most comprehensive collections of snow observations available for hydrological model validation. The network provides extensive spatial coverage across snow-dominated regions of the Northern Hemisphere, with particularly dense coverage throughout Scandinavia, Finland, and Norway. Stations span elevation gradients from coastal lowlands to high mountain regions, capturing the full spectrum of snow climates including maritime, continental, and Arctic environments.\n",
    "The observational richness of NorSWE includes direct measurements of Snow Water Equivalent (SWE) and complementary snow depth observations that provide critical information about snowpack structure. Many sites contain multi-decade records processed through standardized measurement protocols and quality control procedures, making them ideal for systematic model evaluation.\n",
    "\n",
    "## Scientific Importance of Snow Validation\n",
    "Snow processes represent some of the most challenging aspects of hydrological modeling due to their inherent physical complexity. The snowpack undergoes continuous phase transitions through freezing, melting, and sublimation processes, while complex energy balance interactions involving radiation, temperature, and wind drive temporal evolution. Internal layered structure develops through metamorphism and density changes, creating spatial variability that exhibits strong dependencies on elevation and topographic aspect.\n",
    "The hydrological significance of snow extends far beyond its physical complexity. Snow functions as a natural seasonal reservoir in many regions, with snowmelt timing exerting primary control over peak flows and water availability. These processes exhibit high sensitivity to temperature changes, making them critical indicators of climate impacts. Additionally, extreme events such as snow-rain transitions and rain-on-snow scenarios pose significant challenges for both modeling and water resource management.\n",
    "\n",
    "## Learning Outcomes\n",
    "This tutorial demonstrates key capabilities for conducting snow-focused large sample studies through systematic application of CONFLUENCE. We show how to adapt CONFLUENCE configurations specifically for snow observation sites and focus analysis on snow accumulation and ablation periods. The tutorial covers multi-variable validation approaches that compare both SWE and snow depth simulations, while examining how model performance varies with elevation and assessing performance across different snow climate regimes.\n",
    "The combination of CONFLUENCE's sophisticated snow modeling capabilities with NorSWE's comprehensive observation network provides a powerful framework for advancing snow science through systematic, large sample analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Large Sample Snow Study Experimental Design and Site Selection\n",
    "Transitioning from the FLUXNET energy balance focus to systematic snow hydrology validation, this step establishes the foundation for large sample snow modeling using the comprehensive NorSWE observation network. We demonstrate how CONFLUENCE's workflow efficiency enables systematic snow process evaluation across the full spectrum of northern hemisphere snow environments, from temperate mountain ranges to Arctic tundra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set up plotting style for snow visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"coolwarm\")\n",
    "%matplotlib inline\n",
    "confluence_path = Path('../').resolve()\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE SAMPLE SNOW EXPERIMENTAL DESIGN CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Set directory paths\n",
    "CONFLUENCE_CODE_DIR = confluence_path\n",
    "CONFLUENCE_DATA_DIR = Path('/Users/darrieythorsson/compHydro/data/CONFLUENCE_data')  # â† Update this path\n",
    "#CONFLUENCE_DATA_DIR = Path('/path/to/your/CONFLUENCE_data') \n",
    "\n",
    "# Load snow observation configuration template or create from base template\n",
    "snow_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_point_template.yaml'\n",
    "with open(snow_config_path, 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Update for snow tutorial-specific settings\n",
    "config_updates = {\n",
    "    'CONFLUENCE_CODE_DIR': str(CONFLUENCE_CODE_DIR),\n",
    "    'CONFLUENCE_DATA_DIR': str(CONFLUENCE_DATA_DIR),\n",
    "    'DOMAIN_NAME': 'norswe_template',\n",
    "    'EXPERIMENT_ID': 'run_1',\n",
    "    'EXPERIMENT_TIME_START': '2018-01-01 01:00',\n",
    "    'EXPERIMENT_TIME_END': '2018-03-31 23:00',  # Short for tutorial demonstration\n",
    "}\n",
    "\n",
    "config_dict.update(config_updates)\n",
    "\n",
    "# Save snow configuration template\n",
    "norswe_config_path = CONFLUENCE_CODE_DIR / '0_config_files' / 'config_norswe_template.yaml'\n",
    "with open(norswe_config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ… NorSWE template configuration saved: {norswe_config_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND EXAMINE NORSWE SNOW STATIONS DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nðŸŒ¨ï¸ Loading NorSWE Snow Station Database...\")\n",
    "\n",
    "# Load the NorSWE stations database\n",
    "try:\n",
    "    norswe_df = pd.read_csv('norswe_stations.csv')\n",
    "    print(f\"âœ… Successfully loaded NorSWE database: {len(norswe_df)} snow stations available\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸  NorSWE database not found, creating demonstration dataset...\")\n",
    "    \n",
    "    # Create demonstration NorSWE dataset for tutorial\n",
    "    np.random.seed(42)\n",
    "    n_stations = 150\n",
    "    \n",
    "    # Generate realistic northern hemisphere snow station locations\n",
    "    # Focus on snow-dominated regions: Scandinavia, Canada, Russia, Alps, etc.\n",
    "    regions = [\n",
    "        {'name': 'Scandinavia', 'lat_range': (55, 70), 'lon_range': (5, 30), 'n': 40},\n",
    "        {'name': 'Canada', 'lat_range': (45, 70), 'lon_range': (-140, -60), 'n': 35},\n",
    "        {'name': 'Russia/Siberia', 'lat_range': (50, 70), 'lon_range': (30, 150), 'n': 30},\n",
    "        {'name': 'Alps/Mountains', 'lat_range': (45, 55), 'lon_range': (-10, 15), 'n': 25},\n",
    "        {'name': 'Other Northern', 'lat_range': (40, 75), 'lon_range': (-180, 180), 'n': 20}\n",
    "    ]\n",
    "    \n",
    "    stations_data = []\n",
    "    station_id = 1\n",
    "    \n",
    "    for region in regions:\n",
    "        for i in range(region['n']):\n",
    "            lat = np.random.uniform(region['lat_range'][0], region['lat_range'][1])\n",
    "            lon = np.random.uniform(region['lon_range'][0], region['lon_range'][1])\n",
    "            \n",
    "            # Elevation based on latitude and region (higher at mountains and northern latitudes)\n",
    "            if region['name'] == 'Alps/Mountains':\n",
    "                elevation = np.random.uniform(500, 3000)\n",
    "            elif lat > 65:  # Arctic\n",
    "                elevation = np.random.uniform(0, 800)\n",
    "            else:\n",
    "                elevation = np.random.uniform(0, 1500)\n",
    "            \n",
    "            # Data completeness (higher quality for more accessible sites)\n",
    "            base_completeness = 90 - (lat - 40) * 0.8  # Lower completeness at higher latitudes\n",
    "            swe_completeness = max(20, np.random.normal(base_completeness, 15))\n",
    "            depth_completeness = max(15, np.random.normal(base_completeness + 5, 12))\n",
    "            \n",
    "            # Create station entry\n",
    "            station = {\n",
    "                'station_id': f\"NOR_{station_id:04d}\",\n",
    "                'station_name': f\"{region['name']}_Station_{i+1:03d}\",\n",
    "                'lat': round(lat, 4),\n",
    "                'lon': round(lon, 4),\n",
    "                'elevation': round(elevation, 0),\n",
    "                'source': np.random.choice(['NorSWE', 'SYNOP', 'National_Met', 'Research'], p=[0.6, 0.2, 0.15, 0.05]),\n",
    "                'swq_completeness': round(min(95, max(20, swe_completeness)), 1),\n",
    "                'snd_completeness': round(min(95, max(15, depth_completeness)), 1),\n",
    "            }\n",
    "            \n",
    "            # Add CONFLUENCE formatting\n",
    "            buffer = 0.1\n",
    "            station['BOUNDING_BOX_COORDS'] = f\"{lat + buffer}/{lon - buffer}/{lat - buffer}/{lon + buffer}\"\n",
    "            station['POUR_POINT_COORDS'] = f\"{lat}/{lon}\"\n",
    "            station['Watershed_Name'] = station['station_id'].replace(' ', '_')\n",
    "            \n",
    "            stations_data.append(station)\n",
    "            station_id += 1\n",
    "    \n",
    "    norswe_df = pd.DataFrame(stations_data)\n",
    "    \n",
    "    # Save demonstration dataset\n",
    "    norswe_df.to_csv('norswe_stations.csv', index=False)\n",
    "    print(f\"âœ… Created demonstration NorSWE dataset: {len(norswe_df)} stations\")\n",
    "\n",
    "# Display basic dataset information\n",
    "print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "print(f\"  Total snow stations: {len(norswe_df)}\")\n",
    "print(f\"  Columns: {len(norswe_df.columns)}\")\n",
    "print(f\"  Column names: {', '.join(norswe_df.columns[:8])}...\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT SPATIAL COORDINATES AND SNOW-SPECIFIC ATTRIBUTES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nðŸ—ºï¸  Extracting Snow Station Spatial Information...\")\n",
    "\n",
    "# Ensure coordinate columns exist\n",
    "if 'latitude' not in norswe_df.columns and 'lat' in norswe_df.columns:\n",
    "    norswe_df['latitude'] = norswe_df['lat']\n",
    "if 'longitude' not in norswe_df.columns and 'lon' in norswe_df.columns:\n",
    "    norswe_df['longitude'] = norswe_df['lon']\n",
    "\n",
    "print(f\"âœ… Coordinate extraction successful\")\n",
    "print(f\"  Latitude range: {norswe_df['latitude'].min():.1f}Â° to {norswe_df['latitude'].max():.1f}Â°N\")\n",
    "print(f\"  Longitude range: {norswe_df['longitude'].min():.1f}Â° to {norswe_df['longitude'].max():.1f}Â°E\")\n",
    "print(f\"  Elevation range: {norswe_df['elevation'].min():.0f}m to {norswe_df['elevation'].max():.0f}m\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOW-SPECIFIC DATASET CHARACTERISTICS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nâ„ï¸ Analyzing Snow Dataset Characteristics...\")\n",
    "\n",
    "# Elevation-based snow climate zones\n",
    "elevation_zones = [\n",
    "    (0, 200, 'Coastal/Lowland'),\n",
    "    (200, 500, 'Montane'),\n",
    "    (500, 1000, 'Subalpine'),\n",
    "    (1000, 1500, 'Alpine'),\n",
    "    (1500, 10000, 'High Alpine')\n",
    "]\n",
    "\n",
    "norswe_df['elevation_zone'] = 'Unknown'\n",
    "for min_elev, max_elev, zone_name in elevation_zones:\n",
    "    mask = (norswe_df['elevation'] >= min_elev) & (norswe_df['elevation'] < max_elev)\n",
    "    norswe_df.loc[mask, 'elevation_zone'] = zone_name\n",
    "\n",
    "elevation_counts = norswe_df['elevation_zone'].value_counts()\n",
    "print(f\"  Elevation zones: {len(elevation_counts)}\")\n",
    "print(f\"    Most common: {elevation_counts.index[0]} ({elevation_counts.iloc[0]} stations)\")\n",
    "\n",
    "# Latitude-based climate zones\n",
    "latitude_zones = [\n",
    "    (40, 50, 'Temperate'),\n",
    "    (50, 60, 'Boreal'),\n",
    "    (60, 70, 'Subarctic'),\n",
    "    (70, 80, 'Arctic')\n",
    "]\n",
    "\n",
    "norswe_df['climate_zone'] = 'Unknown'\n",
    "for min_lat, max_lat, zone_name in latitude_zones:\n",
    "    mask = (norswe_df['latitude'] >= min_lat) & (norswe_df['latitude'] < max_lat)\n",
    "    norswe_df.loc[mask, 'climate_zone'] = zone_name\n",
    "\n",
    "climate_counts = norswe_df['climate_zone'].value_counts()\n",
    "print(f\"  Climate zones: {len(climate_counts)}\")\n",
    "print(f\"    Most common: {climate_counts.index[0]} ({climate_counts.iloc[0]} stations)\")\n",
    "\n",
    "# Data source analysis\n",
    "if 'source' in norswe_df.columns:\n",
    "    source_counts = norswe_df['source'].value_counts()\n",
    "    print(f\"  Data sources: {len(source_counts)}\")\n",
    "    print(f\"    Primary source: {source_counts.index[0]} ({source_counts.iloc[0]} stations)\")\n",
    "\n",
    "# Data quality analysis\n",
    "if 'swq_completeness' in norswe_df.columns:\n",
    "    swe_stats = norswe_df['swq_completeness'].describe()\n",
    "    print(f\"  SWE data completeness: {swe_stats['mean']:.1f}% Â± {swe_stats['std']:.1f}%\")\n",
    "    \n",
    "if 'snd_completeness' in norswe_df.columns:\n",
    "    depth_stats = norswe_df['snd_completeness'].describe()\n",
    "    print(f\"  Snow depth completeness: {depth_stats['mean']:.1f}% Â± {depth_stats['std']:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# SNOW DATASET VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Creating Snow Dataset Overview Visualization...\")\n",
    "\n",
    "# Create comprehensive snow dataset overview\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Northern Hemisphere snow station distribution map\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(norswe_df['longitude'], norswe_df['latitude'], \n",
    "                     c=norswe_df['elevation'], cmap='terrain', \n",
    "                     alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.set_title(f'NorSWE Snow Station Distribution\\n({len(norswe_df)} stations)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-180, 180)\n",
    "ax1.set_ylim(35, 80)  # Focus on northern hemisphere snow regions\n",
    "\n",
    "# Add colorbar for elevation\n",
    "cbar = plt.colorbar(scatter, ax=ax1)\n",
    "cbar.set_label('Elevation (m)')\n",
    "\n",
    "# 2. Elevation zone distribution\n",
    "ax2 = axes[0, 1]\n",
    "elevation_counts = norswe_df['elevation_zone'].value_counts()\n",
    "bars = ax2.bar(range(len(elevation_counts)), elevation_counts.values, \n",
    "               color='lightblue', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(elevation_counts)))\n",
    "ax2.set_xticklabels(elevation_counts.index, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Stations')\n",
    "ax2.set_title('Snow Stations by Elevation Zone')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, elevation_counts.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Climate zone distribution\n",
    "ax3 = axes[0, 2]\n",
    "climate_counts = norswe_df['climate_zone'].value_counts()\n",
    "colors = ['gold', 'lightgreen', 'lightcoral', 'lightsteelblue']\n",
    "bars = ax3.bar(range(len(climate_counts)), climate_counts.values, \n",
    "               color=colors[:len(climate_counts)], alpha=0.7, edgecolor='black')\n",
    "ax3.set_xticks(range(len(climate_counts)))\n",
    "ax3.set_xticklabels(climate_counts.index, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Number of Stations')\n",
    "ax3.set_title('Snow Stations by Climate Zone')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, climate_counts.values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Elevation vs Latitude relationship\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(norswe_df['latitude'], norswe_df['elevation'], \n",
    "           alpha=0.6, s=30, c='purple', edgecolors='black', linewidth=0.3)\n",
    "ax4.set_xlabel('Latitude (Â°N)')\n",
    "ax4.set_ylabel('Elevation (m)')\n",
    "ax4.set_title('Snow Stations: Elevation vs Latitude')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Data quality assessment\n",
    "ax5 = axes[1, 1]\n",
    "if 'swq_completeness' in norswe_df.columns and 'snd_completeness' in norswe_df.columns:\n",
    "    ax5.scatter(norswe_df['swq_completeness'], norswe_df['snd_completeness'], \n",
    "               alpha=0.6, s=30, c='orange', edgecolors='black', linewidth=0.3)\n",
    "    ax5.set_xlabel('SWE Data Completeness (%)')\n",
    "    ax5.set_ylabel('Snow Depth Data Completeness (%)')\n",
    "    ax5.set_title('Snow Data Quality Assessment')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add quality threshold lines\n",
    "    ax5.axvline(x=70, color='red', linestyle='--', alpha=0.7, label='70% threshold')\n",
    "    ax5.axhline(y=70, color='red', linestyle='--', alpha=0.7)\n",
    "    ax5.legend()\n",
    "\n",
    "# 6. Data source distribution\n",
    "ax6 = axes[1, 2]\n",
    "if 'source' in norswe_df.columns:\n",
    "    source_counts = norswe_df['source'].value_counts()\n",
    "    wedges, texts, autotexts = ax6.pie(source_counts.values, labels=source_counts.index, \n",
    "                                      autopct='%1.1f%%', startangle=90,\n",
    "                                      colors=['lightblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "    ax6.set_title('Snow Stations by Data Source')\n",
    "    \n",
    "    # Make percentage text bold\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "plt.suptitle('NorSWE Snow Observation Network - Dataset Overview', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# SNOW-SPECIFIC SUMMARY STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nâ„ï¸ Snow Dataset Summary:\")\n",
    "print(f\"  â„ï¸  Geographic coverage: {len(norswe_df)} snow stations across Northern Hemisphere\")\n",
    "print(f\"  ðŸŒ Latitudinal span: {norswe_df['latitude'].max() - norswe_df['latitude'].min():.0f}Â° (Arctic to temperate)\")\n",
    "print(f\"  â›°ï¸  Elevation range: {norswe_df['elevation'].min():.0f}m to {norswe_df['elevation'].max():.0f}m\")\n",
    "print(f\"  ðŸŒ¨ï¸  Environmental diversity:\")\n",
    "print(f\"    Climate zones: {len(norswe_df['climate_zone'].unique())} (Arctic to temperate)\")\n",
    "print(f\"    Elevation zones: {len(norswe_df['elevation_zone'].unique())} (coastal to high alpine)\")\n",
    "if 'source' in norswe_df.columns:\n",
    "    print(f\"    Data sources: {len(norswe_df['source'].unique())}\")\n",
    "\n",
    "print(f\"  ðŸ“Š Data quality characteristics:\")\n",
    "if 'swq_completeness' in norswe_df.columns:\n",
    "    high_quality_swe = len(norswe_df[norswe_df['swq_completeness'] >= 80])\n",
    "    print(f\"    High-quality SWE data (â‰¥80%): {high_quality_swe} stations ({high_quality_swe/len(norswe_df)*100:.1f}%)\")\n",
    "if 'snd_completeness' in norswe_df.columns:\n",
    "    high_quality_depth = len(norswe_df[norswe_df['snd_completeness'] >= 80])\n",
    "    print(f\"    High-quality depth data (â‰¥80%): {high_quality_depth} stations ({high_quality_depth/len(norswe_df)*100:.1f}%)\")\n",
    "\n",
    "# Regional distribution summary\n",
    "print(f\"  ðŸ—ºï¸  Regional distribution:\")\n",
    "for zone, count in climate_counts.head(4).items():\n",
    "    print(f\"    {zone}: {count} stations ({count/len(norswe_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Large Sample Snow Modeling Execution\n",
    "\n",
    "Building on the NorSWE station selection from Step 1, we now execute systematic snow modeling across diverse northern hemisphere environments using the `run_watersheds_camelsspat.py`. This step demonstrates CONFLUENCE's capability for large sample snow process validation, scaling from individual snow physics to continental-scale comparative snow science.\n",
    "\n",
    "### Snow Modeling at Scale: From Single Sites to Northern Hemisphere Analysis\n",
    "\n",
    "**Traditional Snow Modeling Approach**: Manual site-by-site snow process studies typically involve individual location simulations with limited transferability and manual configuration for each elevation zone or climate region. This approach makes it difficult to distinguish universal snow physics from site-specific effects and provides limited ability to identify systematic snow model performance patterns.\n",
    "\n",
    "**Large Sample Snow Modeling Approach**: Systematic validation across environmental gradients enables **automated snow configuration** across elevation, latitude, and climate zones through **parallel snow simulations** that leverage CONFLUENCE's computational efficiency. This approach provides **standardized snow validation** for direct performance comparison across sites, **multi-variable snow assessment** integrating SWE, snow depth, and seasonal dynamics, leading to **continental-scale insights** into snow process model performance patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_norswe_script_from_notebook():\n",
    "    \"\"\"\n",
    "    Execute the run_norswe-2.py script from within the notebook\n",
    "    \"\"\"\n",
    "    print(f\"\\nâ„ï¸ Executing NorSWE Large Sample Snow Processing Script...\")\n",
    "    \n",
    "    script_path = \"./run_norswe-2.py\"\n",
    "    \n",
    "    if not Path(script_path).exists():\n",
    "        print(f\"âŒ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"   ðŸ“ Script location: {script_path}\")\n",
    "    print(f\"   ðŸŽ¯ Target sites: {len(complete_stations)} NorSWE stations\")\n",
    "    print(f\"   â° Processing started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare script arguments based on experiment configuration\n",
    "        script_args = [\n",
    "            'python', script_path,\n",
    "            '--norswe_path', experiment_config['norswe_path'],\n",
    "            '--template_config', experiment_config['template_config'],\n",
    "            '--output_dir', experiment_config['output_dir'],\n",
    "            '--config_dir', experiment_config['config_dir'],\n",
    "            '--min_completeness', str(experiment_config['min_completeness']),\n",
    "            '--max_stations', str(experiment_config['max_stations']),\n",
    "            '--base_path', experiment_config['base_path']\n",
    "        ]\n",
    "        \n",
    "        # Add optional year filtering\n",
    "        if experiment_config.get('start_year'):\n",
    "            script_args.extend(['--start_year', str(experiment_config['start_year'])])\n",
    "        if experiment_config.get('end_year'):\n",
    "            script_args.extend(['--end_year', str(experiment_config['end_year'])])\n",
    "        \n",
    "        # Add no_submit flag if specified\n",
    "        if experiment_config.get('no_submit', False):\n",
    "            script_args.append('--no_submit')\n",
    "        \n",
    "        print(f\"   ðŸ”§ Script arguments: {' '.join(script_args[2:])}\")\n",
    "        \n",
    "        # Create a process with input automation\n",
    "        process = subprocess.Popen(\n",
    "            script_args,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Send 'y' to confirm job submission when prompted (unless no_submit)\n",
    "        if not experiment_config.get('no_submit', False):\n",
    "            stdout, stderr = process.communicate(input='y\\n')\n",
    "        else:\n",
    "            stdout, stderr = process.communicate()\n",
    "        \n",
    "        # Print the output\n",
    "        if stdout:\n",
    "            print(\"ðŸ“‹ Script Output:\")\n",
    "            for line in stdout.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"âš ï¸  Script Warnings/Errors:\")\n",
    "            for line in stderr.split('\\n'):\n",
    "                if line.strip():\n",
    "                    print(f\"   {line}\")\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"âœ… NorSWE processing script completed successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Script failed with return code: {process.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running script: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute the NorSWE processing script\n",
    "script_success = run_norswe_script_from_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Site Snow Validation and Process Analysis\n",
    "Having executed large sample snow modeling, we now demonstrate the analytical power that emerges from systematic multi-site snow validation using NorSWE observations. This step showcases comprehensive snow process evaluation, seasonal dynamics analysis, and elevation-climate performance assessmentâ€”the scientific breakthrough enabled by large sample snow hydrology methodology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_completed_snow_domains():\n",
    "    \"\"\"\n",
    "    Discover all completed NorSWE domain directories and their snow outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” Discovering Completed NorSWE Snow Modeling Domains...\")\n",
    "    \n",
    "    # Base data directory pattern\n",
    "    base_path = Path(experiment_config['base_path'])\n",
    "    domain_pattern = str(base_path / \"domain_*\")\n",
    "    \n",
    "    # Find all domain directories\n",
    "    domain_dirs = glob.glob(domain_pattern)\n",
    "    \n",
    "    print(f\"   ðŸ“ Found {len(domain_dirs)} total domain directories\")\n",
    "    \n",
    "    completed_domains = []\n",
    "    \n",
    "    for domain_dir in domain_dirs:\n",
    "        domain_path = Path(domain_dir)\n",
    "        domain_name = domain_path.name.replace('domain_', '')\n",
    "        \n",
    "        # Check if this is a NorSWE domain (should match our selected stations)\n",
    "        if any(domain_name in site for site in complete_stations['Watershed_Name'].values):\n",
    "            \n",
    "            # Check for key output files\n",
    "            shapefile_path = domain_path / \"shapefiles\" / \"catchment\" / f\"{domain_name}_HRUs.shp\"\n",
    "            simulation_dir = domain_path / \"simulations\"\n",
    "            obs_dir = domain_path / \"observations\" / \"snow\" / \"raw_data\"\n",
    "            \n",
    "            domain_info = {\n",
    "                'domain_name': domain_name,\n",
    "                'domain_path': domain_path,\n",
    "                'has_shapefile': shapefile_path.exists(),\n",
    "                'shapefile_path': shapefile_path if shapefile_path.exists() else None,\n",
    "                'has_simulations': simulation_dir.exists(),\n",
    "                'simulation_path': simulation_dir if simulation_dir.exists() else None,\n",
    "                'has_observations': obs_dir.exists(),\n",
    "                'observation_path': obs_dir if obs_dir.exists() else None,\n",
    "                'simulation_files': [],\n",
    "                'swe_obs_file': None,\n",
    "                'depth_obs_file': None\n",
    "            }\n",
    "            \n",
    "            # Find simulation output files\n",
    "            if simulation_dir.exists():\n",
    "                nc_files = list(simulation_dir.glob(\"**/*.nc\"))\n",
    "                domain_info['simulation_files'] = nc_files\n",
    "                domain_info['has_results'] = len(nc_files) > 0\n",
    "            else:\n",
    "                domain_info['has_results'] = False\n",
    "            \n",
    "            # Find observation files\n",
    "            if obs_dir.exists():\n",
    "                swe_files = list((obs_dir / \"swe\").glob(\"*.csv\"))\n",
    "                depth_files = list((obs_dir / \"depth\").glob(\"*.csv\"))\n",
    "                \n",
    "                if swe_files:\n",
    "                    domain_info['swe_obs_file'] = swe_files[0]\n",
    "                if depth_files:\n",
    "                    domain_info['depth_obs_file'] = depth_files[0]\n",
    "            \n",
    "            completed_domains.append(domain_info)\n",
    "    \n",
    "    print(f\"   â„ï¸ NorSWE domains found: {len(completed_domains)}\")\n",
    "    print(f\"   ðŸ“Š Domains with shapefiles: {sum(1 for d in completed_domains if d['has_shapefile'])}\")\n",
    "    print(f\"   ðŸ“ˆ Domains with simulation results: {sum(1 for d in completed_domains if d['has_results'])}\")\n",
    "    print(f\"   ðŸ“‹ Domains with observations: {sum(1 for d in completed_domains if d['has_observations'])}\")\n",
    "    \n",
    "    return completed_domains\n",
    "\n",
    "def create_snow_domain_overview_map(completed_domains):\n",
    "    \"\"\"\n",
    "    Create an overview map showing all snow domain locations and their completion status\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ—ºï¸  Creating Snow Domain Overview Map...\")\n",
    "    \n",
    "    # Create figure for overview map\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Map 1: Global overview with completion status (focus on Northern Hemisphere)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Plot all selected sites\n",
    "    ax1.scatter(complete_stations['lon'], complete_stations['lat'], \n",
    "               c='lightgray', alpha=0.5, s=30, label='Selected stations', marker='o')\n",
    "    \n",
    "    # Plot completed domains with different colors for different completion levels\n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        # Find corresponding site in complete_stations\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            lat = site_row['lat'].iloc[0]\n",
    "            lon = site_row['lon'].iloc[0]\n",
    "            \n",
    "            # Color based on completion status\n",
    "            if domain['has_results'] and domain['has_observations']:\n",
    "                color = 'green'\n",
    "                label = 'Complete with snow validation'\n",
    "                marker = 's'\n",
    "                size = 60\n",
    "            elif domain['has_results']:\n",
    "                color = 'orange' \n",
    "                label = 'Simulation complete'\n",
    "                marker = '^'\n",
    "                size = 50\n",
    "            elif domain['has_observations']:\n",
    "                color = 'blue'\n",
    "                label = 'Observations only'\n",
    "                marker = 'D'\n",
    "                size = 40\n",
    "            else:\n",
    "                color = 'red'\n",
    "                label = 'Processing started'\n",
    "                marker = 'v'\n",
    "                size = 30\n",
    "            \n",
    "            ax1.scatter(lon, lat, c=color, s=size, marker=marker, alpha=0.8,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('NorSWE Snow Domain Processing Status Overview')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-180, 180)\n",
    "    ax1.set_ylim(30, 85)  # Focus on Northern Hemisphere snow regions\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='green', s=60, marker='s', label='Complete with validation'),\n",
    "        plt.scatter([], [], c='orange', s=50, marker='^', label='Simulation complete'),\n",
    "        plt.scatter([], [], c='blue', s=40, marker='D', label='Observations extracted'),\n",
    "        plt.scatter([], [], c='red', s=30, marker='v', label='Processing started'),\n",
    "        plt.scatter([], [], c='lightgray', s=30, marker='o', label='Selected stations')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    # Map 2: Completion statistics by elevation bands\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Create elevation bands\n",
    "    elevation_bands = [(0, 500), (500, 1000), (1000, 1500), (1500, 2000), (2000, 10000)]\n",
    "    band_labels = ['0-500m', '500-1000m', '1000-1500m', '1500-2000m', '>2000m']\n",
    "    \n",
    "    elevation_completion = {}\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            elevation = site_row['elevation'].iloc[0]\n",
    "            \n",
    "            # Find elevation band\n",
    "            for i, (min_elev, max_elev) in enumerate(elevation_bands):\n",
    "                if min_elev <= elevation < max_elev:\n",
    "                    band_label = band_labels[i]\n",
    "                    \n",
    "                    if band_label not in elevation_completion:\n",
    "                        elevation_completion[band_label] = {'total': 0, 'complete': 0, 'partial': 0}\n",
    "                    \n",
    "                    elevation_completion[band_label]['total'] += 1\n",
    "                    \n",
    "                    if domain['has_results'] and domain['has_observations']:\n",
    "                        elevation_completion[band_label]['complete'] += 1\n",
    "                    elif domain['has_results'] or domain['has_observations']:\n",
    "                        elevation_completion[band_label]['partial'] += 1\n",
    "                    break\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    if elevation_completion:\n",
    "        bands = list(elevation_completion.keys())\n",
    "        complete_counts = [elevation_completion[b]['complete'] for b in bands]\n",
    "        partial_counts = [elevation_completion[b]['partial'] for b in bands]\n",
    "        pending_counts = [elevation_completion[b]['total'] - \n",
    "                         elevation_completion[b]['complete'] - \n",
    "                         elevation_completion[b]['partial'] for b in bands]\n",
    "        \n",
    "        x_pos = range(len(bands))\n",
    "        \n",
    "        ax2.bar(x_pos, complete_counts, label='Complete', color='green', alpha=0.7)\n",
    "        ax2.bar(x_pos, partial_counts, bottom=complete_counts, \n",
    "               label='Partial', color='orange', alpha=0.7)\n",
    "        ax2.bar(x_pos, pending_counts, \n",
    "               bottom=[c+p for c,p in zip(complete_counts, partial_counts)], \n",
    "               label='Pending', color='red', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(bands, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Number of Sites')\n",
    "        ax2.set_title('Processing Status by Elevation Band')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Map 3: Station elevation vs latitude\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    domain_elevations = []\n",
    "    domain_latitudes = []\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        domain_name = domain['domain_name']\n",
    "        site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "        \n",
    "        if not site_row.empty:\n",
    "            elevation = site_row['elevation'].iloc[0]\n",
    "            latitude = site_row['lat'].iloc[0]\n",
    "            domain_elevations.append(elevation)\n",
    "            domain_latitudes.append(latitude)\n",
    "            \n",
    "            # Color code by completion status\n",
    "            if domain['has_results'] and domain['has_observations']:\n",
    "                color = 'green'\n",
    "            elif domain['has_results']:\n",
    "                color = 'orange'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            \n",
    "            ax3.scatter(latitude, elevation, c=color, alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax3.set_xlabel('Latitude')\n",
    "    ax3.set_ylabel('Elevation (m)')\n",
    "    ax3.set_title('Station Distribution: Elevation vs Latitude')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Map 4: Processing summary statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_selected = len(complete_stations)\n",
    "    total_discovered = len(completed_domains)\n",
    "    total_with_results = sum(1 for d in completed_domains if d['has_results'])\n",
    "    total_with_obs = sum(1 for d in completed_domains if d['has_observations'])\n",
    "    total_complete = sum(1 for d in completed_domains if d['has_results'] and d['has_observations'])\n",
    "    \n",
    "    categories = ['Selected', 'Processing\\nStarted', 'Simulation\\nComplete', 'Observations\\nExtracted', 'Ready for\\nValidation']\n",
    "    counts = [total_selected, total_discovered, total_with_results, total_with_obs, total_complete]\n",
    "    colors = ['lightblue', 'yellow', 'orange', 'cyan', 'green']\n",
    "    \n",
    "    bars = ax4.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylabel('Number of Sites')\n",
    "    ax4.set_title('Snow Modeling Processing Progress')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Study - Domain Overview', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the overview map\n",
    "    overview_path = experiment_dir / 'plots' / 'snow_domain_overview_map.png'\n",
    "    overview_path.parent.mkdir(exist_ok=True)\n",
    "    plt.savefig(overview_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Snow domain overview map saved: {overview_path}\")\n",
    "    \n",
    "    return total_selected, total_discovered, total_with_results, total_with_obs, total_complete\n",
    "\n",
    "def extract_snow_results_from_domains(completed_domains):\n",
    "    \"\"\"\n",
    "    Extract snow simulation results (SWE and snow depth) from all completed domains\n",
    "    \"\"\"\n",
    "    print(f\"\\nâ„ï¸ Extracting Snow Results from Completed Domains...\")\n",
    "    \n",
    "    snow_results = []\n",
    "    processing_summary = {\n",
    "        'total_domains': len(completed_domains),\n",
    "        'domains_with_results': 0,\n",
    "        'domains_with_snow': 0,\n",
    "        'failed_extractions': 0\n",
    "    }\n",
    "    \n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_results']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        processing_summary['domains_with_results'] += 1\n",
    "        \n",
    "        try:\n",
    "            print(f\"   ðŸ”„ Processing {domain_name}...\")\n",
    "            \n",
    "            # Find simulation output files\n",
    "            nc_files = domain['simulation_files']\n",
    "            \n",
    "            # Look for daily or monthly output files\n",
    "            daily_files = [f for f in nc_files if 'day' in f.name.lower()]\n",
    "            monthly_files = [f for f in nc_files if 'month' in f.name.lower()]\n",
    "            timestep_files = [f for f in nc_files if 'timestep' in f.name.lower()]\n",
    "            \n",
    "            output_file = None\n",
    "            if daily_files:\n",
    "                output_file = daily_files[0]\n",
    "            elif timestep_files:\n",
    "                output_file = timestep_files[0]\n",
    "            elif monthly_files:\n",
    "                output_file = monthly_files[0]\n",
    "            elif nc_files:\n",
    "                output_file = nc_files[0]  # Use any available file\n",
    "            \n",
    "            if output_file is None:\n",
    "                print(f\"     âŒ No suitable output files found\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Load the netCDF file\n",
    "            ds = xr.open_dataset(output_file)\n",
    "            \n",
    "            # Look for snow variables\n",
    "            snow_vars = {}\n",
    "            \n",
    "            # Common SUMMA snow variable names\n",
    "            if 'scalarSWE' in ds.data_vars:\n",
    "                snow_vars['swe'] = 'scalarSWE'\n",
    "            elif 'SWE' in ds.data_vars:\n",
    "                snow_vars['swe'] = 'SWE'\n",
    "            \n",
    "            if 'scalarSnowDepth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'scalarSnowDepth'\n",
    "            elif 'snowDepth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'snowDepth'\n",
    "            elif 'snow_depth' in ds.data_vars:\n",
    "                snow_vars['depth'] = 'snow_depth'\n",
    "            \n",
    "            if not snow_vars:\n",
    "                print(f\"     âš ï¸  No snow variables found in {output_file.name}\")\n",
    "                available_vars = list(ds.data_vars.keys())\n",
    "                print(f\"     Available variables: {available_vars[:10]}...\")\n",
    "                processing_summary['failed_extractions'] += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"     â„ï¸ Using snow variables: {snow_vars}\")\n",
    "            \n",
    "            # Extract snow data\n",
    "            extracted_data = {}\n",
    "            \n",
    "            for var_type, var_name in snow_vars.items():\n",
    "                snow_data = ds[var_name]\n",
    "                \n",
    "                # Handle multi-dimensional data (take spatial mean if needed)\n",
    "                if len(snow_data.dims) > 1:\n",
    "                    spatial_dims = [dim for dim in snow_data.dims if dim != 'time']\n",
    "                    if spatial_dims:\n",
    "                        snow_data = snow_data.mean(dim=spatial_dims)\n",
    "                \n",
    "                # Convert to pandas Series\n",
    "                snow_series = snow_data.to_pandas()\n",
    "                \n",
    "                # Handle negative values and unit conversion\n",
    "                if var_type == 'swe':\n",
    "                    # SWE should be positive\n",
    "                    snow_series = snow_series.abs()\n",
    "                    # Convert from kg/mÂ² to mm if needed (1 kg/mÂ² = 1 mm)\n",
    "                    # SUMMA typically outputs in kg/mÂ²\n",
    "                elif var_type == 'depth':\n",
    "                    # Snow depth should be positive\n",
    "                    snow_series = snow_series.abs()\n",
    "                    # Convert from m to cm if needed\n",
    "                    if snow_series.max() < 10:  # Assume meters\n",
    "                        snow_series = snow_series * 100  # Convert to cm\n",
    "                \n",
    "                extracted_data[var_type] = snow_series\n",
    "            \n",
    "            # Get site information\n",
    "            site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "            \n",
    "            if site_row.empty:\n",
    "                print(f\"     âš ï¸  Site information not found for {domain_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate snow season statistics\n",
    "            snow_stats = {}\n",
    "            \n",
    "            for var_type, series in extracted_data.items():\n",
    "                # Basic statistics\n",
    "                snow_stats[f'{var_type}_mean'] = series.mean()\n",
    "                snow_stats[f'{var_type}_max'] = series.max()\n",
    "                snow_stats[f'{var_type}_std'] = series.std()\n",
    "                \n",
    "                # Seasonal statistics\n",
    "                if len(series) > 0:\n",
    "                    # Find peak snow (maximum value)\n",
    "                    peak_idx = series.idxmax()\n",
    "                    snow_stats[f'{var_type}_peak_date'] = peak_idx\n",
    "                    snow_stats[f'{var_type}_peak_value'] = series[peak_idx]\n",
    "                    \n",
    "                    # Snow season length (days with snow > threshold)\n",
    "                    threshold = 10 if var_type == 'swe' else 5  # 10 mm SWE or 5 cm depth\n",
    "                    snow_days = (series > threshold).sum()\n",
    "                    snow_stats[f'{var_type}_season_length'] = snow_days\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'domain_name': domain_name,\n",
    "                'station_id': site_row['station_id'].iloc[0],\n",
    "                'latitude': site_row['lat'].iloc[0],\n",
    "                'longitude': site_row['lon'].iloc[0],\n",
    "                'elevation': site_row['elevation'].iloc[0],\n",
    "                'data_period': f\"{extracted_data[list(extracted_data.keys())[0]].index.min()} to {extracted_data[list(extracted_data.keys())[0]].index.max()}\",\n",
    "                'data_points': len(extracted_data[list(extracted_data.keys())[0]]),\n",
    "                'output_file': str(output_file)\n",
    "            }\n",
    "            \n",
    "            # Add time series data\n",
    "            result.update(extracted_data)\n",
    "            \n",
    "            # Add statistics\n",
    "            result.update(snow_stats)\n",
    "            \n",
    "            snow_results.append(result)\n",
    "            processing_summary['domains_with_snow'] += 1\n",
    "            \n",
    "            swe_info = f\"{snow_stats.get('swe_mean', 0):.1f} mm (max: {snow_stats.get('swe_max', 0):.1f})\" if 'swe' in extracted_data else \"N/A\"\n",
    "            depth_info = f\"{snow_stats.get('depth_mean', 0):.1f} cm (max: {snow_stats.get('depth_max', 0):.1f})\" if 'depth' in extracted_data else \"N/A\"\n",
    "            \n",
    "            print(f\"     âœ… Snow extracted - SWE: {swe_info}, Depth: {depth_info}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Error processing {domain_name}: {e}\")\n",
    "            processing_summary['failed_extractions'] += 1\n",
    "    \n",
    "    print(f\"\\nâ„ï¸ Snow Extraction Summary:\")\n",
    "    print(f\"   Total domains: {processing_summary['total_domains']}\")\n",
    "    print(f\"   Domains with results: {processing_summary['domains_with_results']}\")\n",
    "    print(f\"   Successful snow extractions: {processing_summary['domains_with_snow']}\")\n",
    "    print(f\"   Failed extractions: {processing_summary['failed_extractions']}\")\n",
    "    \n",
    "    return snow_results, processing_summary\n",
    "\n",
    "def load_norswe_observations(completed_domains):\n",
    "    \"\"\"\n",
    "    Load NorSWE observation data for snow validation\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“¥ Loading NorSWE Snow Observation Data...\")\n",
    "    \n",
    "    norswe_obs = {}\n",
    "    obs_summary = {\n",
    "        'sites_found': 0,\n",
    "        'sites_with_swe': 0,\n",
    "        'sites_with_depth': 0,\n",
    "        'total_swe_observations': 0,\n",
    "        'total_depth_observations': 0\n",
    "    }\n",
    "    \n",
    "    # Look for extracted NorSWE observation data in domain directories\n",
    "    for domain in completed_domains:\n",
    "        if not domain['has_observations']:\n",
    "            continue\n",
    "            \n",
    "        domain_name = domain['domain_name']\n",
    "        \n",
    "        try:\n",
    "            print(f\"   ðŸ“Š Loading {domain_name}...\")\n",
    "            \n",
    "            obs_summary['sites_found'] += 1\n",
    "            site_obs = {}\n",
    "            \n",
    "            # Load SWE observations\n",
    "            if domain['swe_obs_file']:\n",
    "                swe_df = pd.read_csv(domain['swe_obs_file'])\n",
    "                swe_df['time'] = pd.to_datetime(swe_df['time'])\n",
    "                swe_df.set_index('time', inplace=True)\n",
    "                \n",
    "                swe_obs = swe_df['SWE_kg_m2'].dropna()\n",
    "                \n",
    "                if len(swe_obs) > 0:\n",
    "                    site_obs['swe_timeseries'] = swe_obs\n",
    "                    site_obs['swe_mean'] = swe_obs.mean()\n",
    "                    site_obs['swe_max'] = swe_obs.max()\n",
    "                    site_obs['swe_std'] = swe_obs.std()\n",
    "                    \n",
    "                    # Seasonal statistics\n",
    "                    peak_idx = swe_obs.idxmax()\n",
    "                    site_obs['swe_peak_date'] = peak_idx\n",
    "                    site_obs['swe_peak_value'] = swe_obs[peak_idx]\n",
    "                    \n",
    "                    # Snow season length\n",
    "                    snow_days = (swe_obs > 10).sum()  # Days with >10mm SWE\n",
    "                    site_obs['swe_season_length'] = snow_days\n",
    "                    \n",
    "                    obs_summary['sites_with_swe'] += 1\n",
    "                    obs_summary['total_swe_observations'] += len(swe_obs)\n",
    "                    \n",
    "                    print(f\"     â„ï¸ SWE obs: {swe_obs.mean():.1f} Â± {swe_obs.std():.1f} mm ({len(swe_obs)} points)\")\n",
    "            \n",
    "            # Load snow depth observations  \n",
    "            if domain['depth_obs_file']:\n",
    "                depth_df = pd.read_csv(domain['depth_obs_file'])\n",
    "                depth_df['time'] = pd.to_datetime(depth_df['time'])\n",
    "                depth_df.set_index('time', inplace=True)\n",
    "                \n",
    "                depth_obs = depth_df['Depth_m'].dropna() * 100  # Convert m to cm\n",
    "                \n",
    "                if len(depth_obs) > 0:\n",
    "                    site_obs['depth_timeseries'] = depth_obs\n",
    "                    site_obs['depth_mean'] = depth_obs.mean()\n",
    "                    site_obs['depth_max'] = depth_obs.max()\n",
    "                    site_obs['depth_std'] = depth_obs.std()\n",
    "                    \n",
    "                    # Seasonal statistics\n",
    "                    peak_idx = depth_obs.idxmax()\n",
    "                    site_obs['depth_peak_date'] = peak_idx\n",
    "                    site_obs['depth_peak_value'] = depth_obs[peak_idx]\n",
    "                    \n",
    "                    # Snow season length\n",
    "                    snow_days = (depth_obs > 5).sum()  # Days with >5cm depth\n",
    "                    site_obs['depth_season_length'] = snow_days\n",
    "                    \n",
    "                    obs_summary['sites_with_depth'] += 1\n",
    "                    obs_summary['total_depth_observations'] += len(depth_obs)\n",
    "                    \n",
    "                    print(f\"     ðŸ“ Depth obs: {depth_obs.mean():.1f} Â± {depth_obs.std():.1f} cm ({len(depth_obs)} points)\")\n",
    "            \n",
    "            # Add site metadata\n",
    "            site_row = complete_stations[complete_stations['Watershed_Name'] == domain_name]\n",
    "            if not site_row.empty:\n",
    "                site_obs['latitude'] = site_row['lat'].iloc[0]\n",
    "                site_obs['longitude'] = site_row['lon'].iloc[0]\n",
    "                site_obs['elevation'] = site_row['elevation'].iloc[0]\n",
    "                site_obs['station_id'] = site_row['station_id'].iloc[0]\n",
    "                \n",
    "                norswe_obs[domain_name] = site_obs\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Error loading {domain_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâ„ï¸ NorSWE Observation Summary:\")\n",
    "    print(f\"   Sites with observation files: {obs_summary['sites_found']}\")\n",
    "    print(f\"   Sites with SWE observations: {obs_summary['sites_with_swe']}\")\n",
    "    print(f\"   Sites with depth observations: {obs_summary['sites_with_depth']}\")\n",
    "    print(f\"   Total SWE observations: {obs_summary['total_swe_observations']}\")\n",
    "    print(f\"   Total depth observations: {obs_summary['total_depth_observations']}\")\n",
    "    \n",
    "    return norswe_obs, obs_summary\n",
    "\n",
    "def create_snow_comparison_analysis(snow_results, norswe_obs):\n",
    "    \"\"\"\n",
    "    Create comprehensive snow comparison analysis between simulated and observed snow\n",
    "    \"\"\"\n",
    "    print(f\"\\nâ„ï¸ Creating Snow Comparison Analysis...\")\n",
    "    \n",
    "    # Find sites with both simulated and observed data\n",
    "    common_sites = []\n",
    "    \n",
    "    for sim_result in snow_results:\n",
    "        domain_name = sim_result['domain_name']\n",
    "        \n",
    "        if domain_name in norswe_obs:\n",
    "            # Align time periods for both SWE and snow depth\n",
    "            comparisons = {}\n",
    "            \n",
    "            # SWE comparison\n",
    "            if 'swe' in sim_result and 'swe_timeseries' in norswe_obs[domain_name]:\n",
    "                sim_swe = sim_result['swe']\n",
    "                obs_swe = norswe_obs[domain_name]['swe_timeseries']\n",
    "                \n",
    "                # Find common time period\n",
    "                common_start = max(sim_swe.index.min(), obs_swe.index.min())\n",
    "                common_end = min(sim_swe.index.max(), obs_swe.index.max())\n",
    "                \n",
    "                if common_start < common_end:\n",
    "                    # Resample to daily and align\n",
    "                    sim_daily = sim_swe.resample('D').mean().loc[common_start:common_end]\n",
    "                    obs_daily = obs_swe.resample('D').mean().loc[common_start:common_end]\n",
    "                    \n",
    "                    # Remove NaN values\n",
    "                    valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                    sim_valid = sim_daily[valid_mask]\n",
    "                    obs_valid = obs_daily[valid_mask]\n",
    "                    \n",
    "                    if len(sim_valid) > 30:  # Need minimum data for meaningful comparison\n",
    "                        \n",
    "                        # Calculate performance metrics\n",
    "                        rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                        bias = (sim_valid - obs_valid).mean()\n",
    "                        mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                        \n",
    "                        # Correlation\n",
    "                        try:\n",
    "                            correlation = obs_valid.corr(sim_valid)\n",
    "                            if pd.isna(correlation):\n",
    "                                correlation = 0.0\n",
    "                        except:\n",
    "                            correlation = 0.0\n",
    "                        \n",
    "                        # Nash-Sutcliffe Efficiency\n",
    "                        if obs_valid.var() > 0:\n",
    "                            nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                        else:\n",
    "                            nse = np.nan\n",
    "                        \n",
    "                        comparisons['swe'] = {\n",
    "                            'sim_data': sim_valid,\n",
    "                            'obs_data': obs_valid,\n",
    "                            'rmse': rmse,\n",
    "                            'bias': bias,\n",
    "                            'mae': mae,\n",
    "                            'correlation': correlation,\n",
    "                            'nse': nse,\n",
    "                            'n_points': len(sim_valid)\n",
    "                        }\n",
    "            \n",
    "            # Snow depth comparison\n",
    "            if 'depth' in sim_result and 'depth_timeseries' in norswe_obs[domain_name]:\n",
    "                sim_depth = sim_result['depth']\n",
    "                obs_depth = norswe_obs[domain_name]['depth_timeseries']\n",
    "                \n",
    "                # Find common time period\n",
    "                common_start = max(sim_depth.index.min(), obs_depth.index.min())\n",
    "                common_end = min(sim_depth.index.max(), obs_depth.index.max())\n",
    "                \n",
    "                if common_start < common_end:\n",
    "                    # Resample to daily and align\n",
    "                    sim_daily = sim_depth.resample('D').mean().loc[common_start:common_end]\n",
    "                    obs_daily = obs_depth.resample('D').mean().loc[common_start:common_end]\n",
    "                    \n",
    "                    # Remove NaN values\n",
    "                    valid_mask = ~(sim_daily.isna() | obs_daily.isna())\n",
    "                    sim_valid = sim_daily[valid_mask]\n",
    "                    obs_valid = obs_daily[valid_mask]\n",
    "                    \n",
    "                    if len(sim_valid) > 30:  # Need minimum data for meaningful comparison\n",
    "                        \n",
    "                        # Calculate performance metrics\n",
    "                        rmse = np.sqrt(((obs_valid - sim_valid) ** 2).mean())\n",
    "                        bias = (sim_valid - obs_valid).mean()\n",
    "                        mae = np.abs(obs_valid - sim_valid).mean()\n",
    "                        \n",
    "                        # Correlation\n",
    "                        try:\n",
    "                            correlation = obs_valid.corr(sim_valid)\n",
    "                            if pd.isna(correlation):\n",
    "                                correlation = 0.0\n",
    "                        except:\n",
    "                            correlation = 0.0\n",
    "                        \n",
    "                        # Nash-Sutcliffe Efficiency\n",
    "                        if obs_valid.var() > 0:\n",
    "                            nse = 1 - ((obs_valid - sim_valid) ** 2).sum() / ((obs_valid - obs_valid.mean()) ** 2).sum()\n",
    "                        else:\n",
    "                            nse = np.nan\n",
    "                        \n",
    "                        comparisons['depth'] = {\n",
    "                            'sim_data': sim_valid,\n",
    "                            'obs_data': obs_valid,\n",
    "                            'rmse': rmse,\n",
    "                            'bias': bias,\n",
    "                            'mae': mae,\n",
    "                            'correlation': correlation,\n",
    "                            'nse': nse,\n",
    "                            'n_points': len(sim_valid)\n",
    "                        }\n",
    "            \n",
    "            if comparisons:\n",
    "                common_site = {\n",
    "                    'domain_name': domain_name,\n",
    "                    'latitude': sim_result['latitude'],\n",
    "                    'longitude': sim_result['longitude'],\n",
    "                    'elevation': sim_result['elevation'],\n",
    "                    'station_id': sim_result['station_id'],\n",
    "                    'comparisons': comparisons\n",
    "                }\n",
    "                \n",
    "                common_sites.append(common_site)\n",
    "                \n",
    "                # Print summary\n",
    "                comp_summary = []\n",
    "                for var_type, comp_data in comparisons.items():\n",
    "                    comp_summary.append(f\"{var_type}: r={comp_data['correlation']:.3f}, RMSE={comp_data['rmse']:.2f}\")\n",
    "                \n",
    "                print(f\"   âœ… {domain_name}: {', '.join(comp_summary)} ({comp_data['n_points']} points)\")\n",
    "    \n",
    "    print(f\"\\nâ„ï¸ Snow Comparison Summary:\")\n",
    "    print(f\"   Sites with both sim and obs: {len(common_sites)}\")\n",
    "    \n",
    "    if len(common_sites) == 0:\n",
    "        print(\"   âš ï¸  No sites with overlapping sim/obs data for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comprehensive snow comparison visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # SWE scatter plot (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if swe_sites:\n",
    "        all_obs_swe = np.concatenate([site['comparisons']['swe']['obs_data'].values for site in swe_sites])\n",
    "        all_sim_swe = np.concatenate([site['comparisons']['swe']['sim_data'].values for site in swe_sites])\n",
    "        \n",
    "        ax1.scatter(all_obs_swe, all_sim_swe, alpha=0.5, s=15, c='blue')\n",
    "        \n",
    "        # 1:1 line\n",
    "        min_val = min(all_obs_swe.min(), all_sim_swe.min())\n",
    "        max_val = max(all_obs_swe.max(), all_sim_swe.max())\n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "        \n",
    "        ax1.set_xlabel('Observed SWE (mm)')\n",
    "        ax1.set_ylabel('Simulated SWE (mm)')\n",
    "        ax1.set_title('SWE: Simulated vs Observed')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall statistics\n",
    "        overall_corr = np.corrcoef(all_obs_swe, all_sim_swe)[0,1]\n",
    "        overall_rmse = np.sqrt(np.mean((all_obs_swe - all_sim_swe)**2))\n",
    "        overall_bias = np.mean(all_sim_swe - all_obs_swe)\n",
    "        \n",
    "        stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.1f}\\nBias = {overall_bias:+.1f}'\n",
    "        ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Snow depth scatter plot (top middle)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "    \n",
    "    if depth_sites:\n",
    "        all_obs_depth = np.concatenate([site['comparisons']['depth']['obs_data'].values for site in depth_sites])\n",
    "        all_sim_depth = np.concatenate([site['comparisons']['depth']['sim_data'].values for site in depth_sites])\n",
    "        \n",
    "        ax2.scatter(all_obs_depth, all_sim_depth, alpha=0.5, s=15, c='purple')\n",
    "        \n",
    "        # 1:1 line\n",
    "        min_val = min(all_obs_depth.min(), all_sim_depth.min())\n",
    "        max_val = max(all_obs_depth.max(), all_sim_depth.max())\n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 line')\n",
    "        \n",
    "        ax2.set_xlabel('Observed Snow Depth (cm)')\n",
    "        ax2.set_ylabel('Simulated Snow Depth (cm)')\n",
    "        ax2.set_title('Snow Depth: Simulated vs Observed')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add overall statistics\n",
    "        overall_corr = np.corrcoef(all_obs_depth, all_sim_depth)[0,1]\n",
    "        overall_rmse = np.sqrt(np.mean((all_obs_depth - all_sim_depth)**2))\n",
    "        overall_bias = np.mean(all_sim_depth - all_obs_depth)\n",
    "        \n",
    "        stats_text = f'r = {overall_corr:.3f}\\nRMSE = {overall_rmse:.1f}\\nBias = {overall_bias:+.1f}'\n",
    "        ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8), fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    # Performance vs elevation (top right)\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    elevations = [site['elevation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    swe_correlations = [site['comparisons']['swe']['correlation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if elevations and swe_correlations:\n",
    "        ax3.scatter(elevations, swe_correlations, alpha=0.7, s=40, c='green')\n",
    "        ax3.set_xlabel('Elevation (m)')\n",
    "        ax3.set_ylabel('SWE Correlation')\n",
    "        ax3.set_title('SWE Performance vs Elevation')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # SWE bias distribution (bottom left)\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    if swe_sites:\n",
    "        swe_biases = [site['comparisons']['swe']['bias'] for site in swe_sites]\n",
    "        ax4.hist(swe_biases, bins=15, color='lightblue', alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "        ax4.set_xlabel('SWE Bias (mm)')\n",
    "        ax4.set_ylabel('Number of Sites')\n",
    "        ax4.set_title('Distribution of SWE Bias')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Snow depth bias distribution (bottom middle)\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    if depth_sites:\n",
    "        depth_biases = [site['comparisons']['depth']['bias'] for site in depth_sites]\n",
    "        ax5.hist(depth_biases, bins=15, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "        ax5.axvline(x=0, color='red', linestyle='--', label='Zero bias')\n",
    "        ax5.set_xlabel('Snow Depth Bias (cm)')\n",
    "        ax5.set_ylabel('Number of Sites')\n",
    "        ax5.set_title('Distribution of Snow Depth Bias')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Performance by latitude (bottom right)\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    latitudes = [site['latitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    swe_rmses = [site['comparisons']['swe']['rmse'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if latitudes and swe_rmses:\n",
    "        ax6.scatter(latitudes, swe_rmses, alpha=0.7, s=40, c='orange')\n",
    "        ax6.set_xlabel('Latitude')\n",
    "        ax6.set_ylabel('SWE RMSE (mm)')\n",
    "        ax6.set_title('SWE Performance vs Latitude')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Comparison Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison plot\n",
    "    comparison_path = experiment_dir / 'plots' / 'snow_comparison_analysis.png'\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Snow comparison analysis saved: {comparison_path}\")\n",
    "    \n",
    "    # Create spatial performance map\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Map 1: SWE correlation map\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lats = [site['latitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    lons = [site['longitude'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    corrs = [site['comparisons']['swe']['correlation'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if lats and lons and corrs:\n",
    "        scatter1 = ax1.scatter(lons, lats, c=corrs, cmap='RdYlBu', s=80, \n",
    "                              vmin=0, vmax=1, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.set_title('Snow Model Performance: SWE Correlation')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim(-180, 180)\n",
    "        ax1.set_ylim(30, 85)  # Northern Hemisphere focus\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "        cbar1.set_label('SWE Correlation')\n",
    "    \n",
    "    # Map 2: SWE bias map\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    biases = [site['comparisons']['swe']['bias'] for site in common_sites if 'swe' in site['comparisons']]\n",
    "    \n",
    "    if lats and lons and biases:\n",
    "        max_abs_bias = max(abs(min(biases)), abs(max(biases)))\n",
    "        \n",
    "        scatter2 = ax2.scatter(lons, lats, c=biases, cmap='RdBu_r', s=80,\n",
    "                              vmin=-max_abs_bias, vmax=max_abs_bias, \n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax2.set_xlabel('Longitude')\n",
    "        ax2.set_ylabel('Latitude')\n",
    "        ax2.set_title('Snow Model Performance: SWE Bias (Sim - Obs)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(-180, 180)\n",
    "        ax2.set_ylim(30, 85)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "        cbar2.set_label('SWE Bias (mm)')\n",
    "    \n",
    "    plt.suptitle('NorSWE Large Sample Snow Performance - Spatial Distribution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save spatial analysis\n",
    "    spatial_path = experiment_dir / 'plots' / 'snow_spatial_performance.png'\n",
    "    plt.savefig(spatial_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Snow spatial performance map saved: {spatial_path}\")\n",
    "    \n",
    "    return common_sites\n",
    "\n",
    "# Execute Step 3 Analysis\n",
    "print(f\"\\nðŸ” Step 3.1: Snow Domain Discovery and Overview\")\n",
    "\n",
    "# Discover completed domains\n",
    "completed_domains = discover_completed_snow_domains()\n",
    "\n",
    "# Create domain overview map\n",
    "total_selected, total_discovered, total_with_results, total_with_obs, total_complete = create_snow_domain_overview_map(completed_domains)\n",
    "\n",
    "print(f\"\\nâ„ï¸ Step 3.2: Snow Results Extraction\")\n",
    "\n",
    "# Extract snow results from simulations\n",
    "snow_results, snow_processing_summary = extract_snow_results_from_domains(completed_domains)\n",
    "\n",
    "# Load NorSWE observations\n",
    "norswe_obs, obs_summary = load_norswe_observations(completed_domains)\n",
    "\n",
    "print(f\"\\nâ„ï¸ Step 3.3: Snow Comparison Analysis\")\n",
    "\n",
    "# Create snow comparison analysis\n",
    "if snow_results and norswe_obs:\n",
    "    common_sites = create_snow_comparison_analysis(snow_results, norswe_obs)\n",
    "else:\n",
    "    print(\"   âš ï¸  Insufficient data for snow comparison analysis\")\n",
    "    common_sites = None\n",
    "\n",
    "# Create final summary report\n",
    "print(f\"\\nðŸ“‹ Creating Final NorSWE Snow Study Summary Report...\")\n",
    "\n",
    "summary_report_path = experiment_dir / 'reports' / 'norswe_final_report.txt'\n",
    "summary_report_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(summary_report_path, 'w') as f:\n",
    "    f.write(\"NorSWE Large Sample Snow Study - Final Analysis Report\\n\")\n",
    "    f.write(\"=\" * 58 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"PROCESSING SUMMARY:\\n\")\n",
    "    f.write(f\"  Sites selected for analysis: {total_selected}\\n\")\n",
    "    f.write(f\"  Processing initiated: {total_discovered}\\n\")\n",
    "    f.write(f\"  Simulation results available: {total_with_results}\\n\")\n",
    "    f.write(f\"  Observations extracted: {total_with_obs}\\n\")\n",
    "    f.write(f\"  Complete snow validation: {total_complete}\\n\")\n",
    "    f.write(f\"  Snow extractions successful: {snow_processing_summary['domains_with_snow']}\\n\")\n",
    "    f.write(f\"  NorSWE observations available: {obs_summary['sites_with_swe']}\\n\")\n",
    "    \n",
    "    if common_sites:\n",
    "        f.write(f\"  Sites with sim/obs comparison: {len(common_sites)}\\n\\n\")\n",
    "        \n",
    "        # SWE performance summary\n",
    "        swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "        if swe_sites:\n",
    "            swe_correlations = [site['comparisons']['swe']['correlation'] for site in swe_sites]\n",
    "            swe_rmses = [site['comparisons']['swe']['rmse'] for site in swe_sites]\n",
    "            swe_biases = [site['comparisons']['swe']['bias'] for site in swe_sites]\n",
    "            \n",
    "            f.write(\"SWE PERFORMANCE SUMMARY:\\n\")\n",
    "            f.write(f\"  Mean correlation: {np.mean(swe_correlations):.3f} Â± {np.std(swe_correlations):.3f}\\n\")\n",
    "            f.write(f\"  Mean RMSE: {np.mean(swe_rmses):.1f} Â± {np.std(swe_rmses):.1f} mm\\n\")\n",
    "            f.write(f\"  Mean bias: {np.mean(swe_biases):+.1f} Â± {np.std(swe_biases):.1f} mm\\n\\n\")\n",
    "        \n",
    "        # Snow depth performance summary\n",
    "        depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "        if depth_sites:\n",
    "            depth_correlations = [site['comparisons']['depth']['correlation'] for site in depth_sites]\n",
    "            depth_rmses = [site['comparisons']['depth']['rmse'] for site in depth_sites]\n",
    "            depth_biases = [site['comparisons']['depth']['bias'] for site in depth_sites]\n",
    "            \n",
    "            f.write(\"SNOW DEPTH PERFORMANCE SUMMARY:\\n\")\n",
    "            f.write(f\"  Mean correlation: {np.mean(depth_correlations):.3f} Â± {np.std(depth_correlations):.3f}\\n\")\n",
    "            f.write(f\"  Mean RMSE: {np.mean(depth_rmses):.1f} Â± {np.std(depth_rmses):.1f} cm\\n\")\n",
    "            f.write(f\"  Mean bias: {np.mean(depth_biases):+.1f} Â± {np.std(depth_biases):.1f} cm\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEST PERFORMING SITES (by SWE correlation):\\n\")\n",
    "        if swe_sites:\n",
    "            sorted_sites = sorted(swe_sites, key=lambda x: x['comparisons']['swe']['correlation'], reverse=True)\n",
    "            for i, site in enumerate(sorted_sites[:5]):\n",
    "                f.write(f\"  {i+1}. {site['domain_name']}: r={site['comparisons']['swe']['correlation']:.3f}, RMSE={site['comparisons']['swe']['rmse']:.1f} mm\\n\")\n",
    "\n",
    "print(f\"âœ… Final summary report saved: {summary_report_path}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Step 3 Complete: NorSWE Snow Validation Analysis\")\n",
    "print(f\"   ðŸ“ Results saved to: {experiment_dir}\")\n",
    "print(f\"   â„ï¸ Snow domain overview: {total_complete}/{total_selected} sites with complete validation\")\n",
    "\n",
    "if common_sites:\n",
    "    swe_sites = [site for site in common_sites if 'swe' in site['comparisons']]\n",
    "    depth_sites = [site for site in common_sites if 'depth' in site['comparisons']]\n",
    "    \n",
    "    if swe_sites:\n",
    "        swe_correlations = [site['comparisons']['swe']['correlation'] for site in swe_sites]\n",
    "        print(f\"   ðŸ“Š SWE analysis: {len(swe_sites)} sites with sim/obs comparison\")\n",
    "        print(f\"   ðŸ“ˆ SWE performance: Mean r = {np.mean(swe_correlations):.3f}\")\n",
    "    \n",
    "    if depth_sites:\n",
    "        depth_correlations = [site['comparisons']['depth']['correlation'] for site in depth_sites]\n",
    "        print(f\"   ðŸ“ Depth analysis: {len(depth_sites)} sites with sim/obs comparison\")\n",
    "        print(f\"   ðŸ“ˆ Depth performance: Mean r = {np.mean(depth_correlations):.3f}\")\n",
    "else:\n",
    "    print(f\"   ðŸ“ˆ Performance: Awaiting more simulation results\")\n",
    "\n",
    "print(f\"\\nâœ… Large Sample NorSWE Snow Analysis Complete!\")\n",
    "print(f\"   â„ï¸ Multi-site snow hydrology validation achieved\")\n",
    "print(f\"   ðŸ“Š Statistical patterns identified across elevation and climate gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Summary: Large Sample Snow Hydrology with NorSWE\n",
    "\n",
    "This tutorial demonstrated the power of large sample snow modeling through systematic validation across northern hemisphere environmental gradients. Using the comprehensive NorSWE observation network, we successfully scaled from individual snow physics simulations to continental-scale comparative snow science.\n",
    "\n",
    "**Key Accomplishments:**\n",
    "- **Multi-site snow evaluation** across elevation zones from coastal lowlands to high alpine environments\n",
    "- **Climate gradient analysis** spanning temperate, boreal, and Arctic snow regimes  \n",
    "- **Systematic snow process evaluation** comparing SWE and snow depth simulations against observations\n",
    "- **Performance pattern identification** revealing how snow model accuracy varies with elevation, latitude, and climate\n",
    "\n",
    "**Scientific Insights Gained:**\n",
    "The large sample approach revealed systematic patterns in snow model performance that would be impossible to identify through individual site studies. We quantified how snow simulation accuracy varies across environmental gradients and identified the elevation and climate conditions where current snow physics representations excel or struggle.\n",
    "\n",
    "**Methodological Advancement:**\n",
    "This workflow demonstrates CONFLUENCE's capacity for **automated snow-specific configuration**, **parallel multi-site processing**, and **standardized validation protocols** that enable robust comparative snow science at unprecedented scales.\n",
    "\n",
    "**Connection to Broader Large Sample Studies:**\n",
    "Having explored energy balance validation with FLUXNET (04a) and snow processes with NorSWE (04b), we've established the foundation for comprehensive large sample hydrological analysis. The systematic validation approaches developed here extend naturally to basin-scale discharge validation and multi-variable integrated assessments.\n",
    "\n",
    "The large sample methodology transforms snow hydrology from site-specific case studies to systematic, generalizable scienceâ€”essential for understanding snow processes in a changing climate.\n",
    "\n",
    "### Next Focus: Large Sample Experiments - CAMELS-Spat\n",
    "**Ready to explore large sample basin simulations?** â†’ **[Tutorial 04c: Large Sample Studies - CAMELS-Spat](./04c_large_sample_camelsspat.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
