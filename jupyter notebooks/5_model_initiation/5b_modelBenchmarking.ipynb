{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking initial model performance\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook guides you through the process of benchmarking hydrological models within the CONFLUENCE framework using several simple literature benchmark. Model benchmarking is a critical evaluates the quality of the model simulations by comparing the results to various performance alternatives.\n",
    "\n",
    "Key steps covered in this notebook include:\n",
    "\n",
    "1. Pre-processing the benchmarking data\n",
    "2. Calculating the benchmark datasets for the simulation period\n",
    "3. Vizualising the comparison of the model simulations to the benchmark and summarizing the results\n",
    "\n",
    "In this notebook we focus on benchmarking the primary model chosen for your project (e.g., SUMMA) and the HydroBM benchmarking library, but the principles can be applied to other models and benchmarking paradigms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import the libraries and functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "import yaml # type: ignore\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from utils.evaluation_util.evaluation_utils import Benchmarker # type: ignore\n",
    "from utils.dataHandling_utils.data_utils import BenchmarkPreprocessor # type: ignore  \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check configurations\n",
    "\n",
    "Now we should print our configuration settings and make sure that we have defined all the settings we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING_DATASET: ERA5\n",
      "EASYMORE_CLIENT: easymore cli\n",
      "FORCING_VARIABLES: longitude,latitude,time,LWRadAtm,SWRadAtm,pptrate,airpres,airtemp,spechum,windspd\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n"
     ]
    }
   ],
   "source": [
    "config_path = Path('../../0_config_files/config_active.yaml')\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "    print(f\"FORCING_DATASET: {config['FORCING_DATASET']}\")\n",
    "    print(f\"EASYMORE_CLIENT: {config['EASYMORE_CLIENT']}\")\n",
    "    print(f\"FORCING_VARIABLES: {config['FORCING_VARIABLES']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define default paths\n",
    "\n",
    "Now let's define the paths to data directories before we run the pre processing scripts and create the containing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main project directory\n",
    "data_dir = config['CONFLUENCE_DATA_DIR']\n",
    "project_dir = Path(data_dir) / f\"domain_{config['DOMAIN_NAME']}\"\n",
    "\n",
    "# Data directoris\n",
    "evaluation_results = project_dir / 'evaluation' \n",
    "benchmarking_plots = project_dir / 'plots' / 'benchmarking'\n",
    "\n",
    "# Make sure the new directories exists\n",
    "evaluation_results.mkdir(parents = True, exist_ok = True)\n",
    "benchmarking_plots.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Process the benchmarking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 22:02:32,257 - INFO - Starting benchmark data preprocessing\n",
      "2024-10-21 22:02:32,425 - INFO - Loaded streamflow data with shape: (118099, 1)\n",
      "2024-10-21 22:05:47,610 - INFO - Loaded forcing data with variables: ['latitude', 'longitude', 'hruId', 'airpres', 'LWRadAtm', 'SWRadAtm', 'precipitation', 'temperature', 'spechum', 'windspd']\n",
      "2024-10-21 22:05:47,632 - INFO - Merged data shape: (70135, 3)\n",
      "2024-10-21 22:05:47,638 - INFO - Filtered data shape: (70135, 3)\n",
      "2024-10-21 22:05:47,653 - INFO - Data statistics:\n",
      "         streamflow   temperature  precipitation\n",
      "count  70135.000000  70135.000000   70135.000000\n",
      "mean      40.081286    270.366730      94.530307\n",
      "std       46.004491     10.502746     222.341854\n",
      "min        5.214286    234.621429       0.000000\n",
      "25%        9.612819    262.730499       0.014337\n",
      "50%       18.461614    270.453491      10.557440\n",
      "75%       54.671425    278.303268      81.689040\n",
      "max      503.056587    298.729370    7382.431626\n",
      "2024-10-21 22:05:48,095 - INFO - Benchmark input data saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data for benchmarking\n",
    "preprocessor = BenchmarkPreprocessor(config, logger)\n",
    "benchmark_data = preprocessor.preprocess_benchmark_data(f\"{config['FORCING_START_YEAR']}-01-01\", f\"{config['FORCING_END_YEAR']}-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run benchmarking scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 22:15:27,825 - INFO - Starting hydrobm benchmarking\n",
      "2024-10-21 22:15:27,837 - INFO - input data: <xarray.Dataset> Size: 2MB\n",
      "Dimensions:        (index: 70135)\n",
      "Coordinates:\n",
      "  * index          (index) datetime64[ns] 561kB 2010-12-31T17:00:00 ... 2018-...\n",
      "Data variables:\n",
      "    streamflow     (index) float64 561kB 8.1 8.139 8.188 ... 9.158 9.162 9.165\n",
      "    temperature    (index) float32 281kB 245.4 246.7 249.1 ... 256.2 255.3 254.3\n",
      "    precipitation  (index) float64 561kB 0.0 0.0 3.071 12.14 ... 0.0 0.0 0.0 0.0\n",
      "2024-10-21 22:15:27,838 - INFO - Running benchmarks ['mean_flow', 'median_flow', 'annual_mean_flow', 'annual_median_flow', 'monthly_mean_flow', 'monthly_median_flow', 'daily_mean_flow', 'daily_median_flow', 'rainfall_runoff_ratio_to_all', 'rainfall_runoff_ratio_to_annual', 'rainfall_runoff_ratio_to_monthly', 'rainfall_runoff_ratio_to_daily', 'rainfall_runoff_ratio_to_timestep', 'monthly_rainfall_runoff_ratio_to_monthly', 'monthly_rainfall_runoff_ratio_to_daily', 'monthly_rainfall_runoff_ratio_to_timestep', 'scaled_precipitation_benchmark', 'adjusted_precipitation_benchmark', 'adjusted_smoothed_precipitation_benchmark']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 22:16:01,713 - INFO - Finished running benchmarks\n",
      "2024-10-21 22:16:03,500 - INFO - Benchmark flows saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/evaluation/benchmark_flows.csv\n",
      "2024-10-21 22:16:03,504 - INFO - Benchmark scores saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarking\n",
    "benchmarker = Benchmarker(config, logger)\n",
    "benchmark_results = benchmarker.run_benchmarking(benchmark_data, f\"{config['FORCING_END_YEAR']}-12-31\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualise and summarise the benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmarkingVisualiser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the benchmarking vizualiser\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bmv \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmarkingVisualiser\u001b[49m(config,logger)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the visualisation \u001b[39;00m\n\u001b[1;32m      5\u001b[0m bmv\u001b[38;5;241m.\u001b[39mvizualise_streamflow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'benchmarkingVisualiser' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the benchmarking vizualiser\n",
    "bmv = benchmarkingVisualiser(config,logger)\n",
    "\n",
    "# Run the visualisation \n",
    "bmv.vizualise_streamflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confluenec_env",
   "language": "python",
   "name": "confluence_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
