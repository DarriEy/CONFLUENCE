{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Specific Input Data Preprocessing in CONFLUENCE\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers the model-specific preprocessing steps for input data in CONFLUENCE. After completing the model-agnostic preprocessing, we now focus on tailoring our data to the specific requirements of the chosen hydrological model (e.g., SUMMA, HYPE, or MESH).\n",
    "\n",
    "Key aspects covered in this notebook include:\n",
    "\n",
    "1. Formatting data according to the chosen model's input specifications\n",
    "2. Generating model-specific configuration files\n",
    "3. Preparing initial conditions and parameter files\n",
    "4. Creating forcing data in the required format and resolution\n",
    "\n",
    "In this notebook we ensure that our preprocessed data is compatible with the chosen hydrological model. By the end of this process, you will have a complete set of input files ready for model initialization and simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import the libraries and functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "import yaml # type: ignore\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from utils.dataHandling_utils.specificPreProcessor_util import summaPreProcessor, flashPreProcessor # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check configurations\n",
    "\n",
    "Now we should print our configuration settings and make sure that we have defined all the settings we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING_DATASET: RDRS\n",
      "EASYMORE_CLIENT: easymore cli\n",
      "FORCING_VARIABLES: ['RDRS_v2.1_P_P0_SFC', 'RDRS_v2.1_P_HU_09944', 'RDRS_v2.1_P_TT_09944', 'RDRS_v2.1_P_UVC_09944', 'RDRS_v2.1_A_PR0_SFC', 'RDRS_v2.1_P_FB_SFC', 'RDRS_v2.1_P_FI_SFC']\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n"
     ]
    }
   ],
   "source": [
    "config_path = Path('../../0_config_files/config_active.yaml')\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "    print(f\"FORCING_DATASET: {config['FORCING_DATASET']}\")\n",
    "    print(f\"EASYMORE_CLIENT: {config['EASYMORE_CLIENT']}\")\n",
    "    print(f\"FORCING_VARIABLES: {config['FORCING_VARIABLES']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define default paths\n",
    "\n",
    "Now let's define the paths to data directories before we run the pre processing scripts and create the containing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main project directory\n",
    "data_dir = config['CONFLUENCE_DATA_DIR']\n",
    "project_dir = Path(data_dir) / f\"domain_{config['DOMAIN_NAME']}\"\n",
    "\n",
    "# Data directoris\n",
    "model_input_dir = project_dir / f\"{config['HYDROLOGICAL_MODEL']}_input\"\n",
    "\n",
    "# Make sure the new directories exists\n",
    "model_input_dir.mkdir(parents = True, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize model specific preprocessors\n",
    "\n",
    "if config['HYDROLOGICAL_MODEL'] == 'SUMMA':\n",
    "    ssp = summaPreProcessor(config, logger)\n",
    "    ssp.copyBaseSettings()\n",
    "    ssp.process_forcings()\n",
    "    ssp.process_geospatial()\n",
    "    ssp.create_file_manager()\n",
    "    ssp.create_forcing_file_list()\n",
    "    ssp.create_initial_conditions()\n",
    "    ssp.create_trial_parameters()\n",
    "    ssp.create_attributes_file()\n",
    "\n",
    "\n",
    "\n",
    "elif config['HYDROLOGICAL_MODEL'] == 'FLASH':\n",
    "    ssp = flashPreProcessor(config, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
