{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Agnostic Input Data Preprocessing in CONFLUENCE\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook focuses on the model-agnostic preprocessing steps for input data in CONFLUENCE. Model-agnostic preprocessing involves tasks that are common across different hydrological models, such as data acquisition, and initial formatting.\n",
    "\n",
    "Key steps covered in this notebook include:\n",
    "\n",
    "1. Running the model agnostic orchestrator\n",
    "2. Optional lapse rate adjustment on forcing variables\n",
    "\n",
    "In this preprocessing stage we ensure that our input data is consistent, complete, and properly formatted before we move on to model-specific preprocessing steps. By the end of this notebook, you will have clean, standardized datasets ready for further model-specific processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import the libraries and functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "import yaml # type: ignore\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from utils.dataHandling_utils.data_utils import DataAcquisitionProcessor # type: ignore\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check configurations\n",
    "\n",
    "Now we should print our configuration settings and make sure that we have defined all the settings we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING_DATASET: ERA5\n",
      "EASYMORE_CLIENT: easymore cli\n",
      "FORCING_VARIABLES: longitude,latitude,time,LWRadAtm,SWRadAtm,pptrate,airpres,airtemp,spechum,windspd\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n",
      "EXPERIMENT_TIME_START: 2010-01-01 01:00\n"
     ]
    }
   ],
   "source": [
    "config_path = Path('../../0_config_files/config_active.yaml')\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "    print(f\"FORCING_DATASET: {config['FORCING_DATASET']}\")\n",
    "    print(f\"EASYMORE_CLIENT: {config['EASYMORE_CLIENT']}\")\n",
    "    print(f\"FORCING_VARIABLES: {config['FORCING_VARIABLES']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")\n",
    "    print(f\"EXPERIMENT_TIME_START: {config['EXPERIMENT_TIME_START']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define default paths\n",
    "\n",
    "Now let's define the paths to data directories before we run the pre processing scripts and create the containing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main project directory\n",
    "data_dir = config['CONFLUENCE_DATA_DIR']\n",
    "project_dir = Path(data_dir) / f\"domain_{config['DOMAIN_NAME']}\"\n",
    "\n",
    "# Data directoris\n",
    "raw_data_dir = project_dir / 'forcing' / 'raw_data'\n",
    "basin_averaged_data = project_dir / 'forcing' / 'basin_averaged_data'\n",
    "catchment_intersection_dir = project_dir / 'shapefiles' / 'catchment_intersection'\n",
    "\n",
    "# Make sure the new directories exists\n",
    "basin_averaged_data.mkdir(parents = True, exist_ok = True)\n",
    "catchment_intersection_dir.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Model Agnostic Orchestrator\n",
    "\n",
    "Now let's run the model agnostic orchestrator to acquire and pre-process our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize forcingReampler class\n",
    "fr = forcingResampler(config, logger)\n",
    "\n",
    "# Run resampling\n",
    "fr.run_resampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre process geospatial data\n",
    "\n",
    "Now let's calculate the zonal statistics of the geospatial attributes we need for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 13:47:17,311 - INFO - Calculating soil statistics\n",
      "2024-10-20 13:47:18,239 - INFO - Created 136 records\n",
      "2024-10-20 13:47:18,245 - INFO - Soil statistics saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/shapefiles/catchment_intersection/with_soilgrids/catchment_with_soilclass.shp\n",
      "2024-10-20 13:47:18,246 - INFO - Calculating land statistics\n",
      "2024-10-20 13:47:19,034 - INFO - Created 136 records\n",
      "2024-10-20 13:47:19,045 - INFO - Land statistics saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/shapefiles/catchment_intersection/with_landclass/catchment_with_landclass.shp\n",
      "2024-10-20 13:47:19,047 - INFO - Calculating elevation statistics\n",
      "2024-10-20 13:47:19,911 - INFO - Updating existing 'elev_mean' column\n",
      "2024-10-20 13:47:19,966 - INFO - Created 136 records\n",
      "2024-10-20 13:47:19,973 - INFO - Elevation statistics saved to /home/darri/data/CONFLUENCE_data/domain_Bow_at_Banff/shapefiles/catchment_intersection/with_dem/catchment_with_dem.shp\n",
      "2024-10-20 13:47:19,976 - INFO - All geospatial statistics calculated successfully\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "# Initialize geospatialStatistics class\n",
    "gs = geospatialStatistics(config, logger)\n",
    "\n",
    "# Run resampling\n",
    "gs.run_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confluenec_env",
   "language": "python",
   "name": "confluence_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
